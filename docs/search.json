[
  {
    "objectID": "assignments/data_quality_assurance_processing_project_proposal_2026.html",
    "href": "assignments/data_quality_assurance_processing_project_proposal_2026.html",
    "title": "Data Quality Assurance and Processing Project Proposal",
    "section": "",
    "text": "PSYC 859 Spring 2026\n\nDue dates\n\nProposal: 1/29/2026\nProject submission: 2/12/2026\n\n\n\nProposal guidelines\nPlease submit a brief proposal (6-8 pages) describing the data import, processing, and quality assurance steps to prepare datasets under a conceptual umbrella (e.g., a research project) for data visualization and analysis. Ideally, please use a dataset that will help you in current and/or future projects. Use multiple files from the same project that can be merged or aligned (shared IDs, time points, or measures) to support a coherent visualization/analysis goal.\nFor the proposal document, feel free to be concise, make use of lists, etc. For example, under section 6 (processing scripts proposal), this may look more like a numbered outline than prose. Note that when you submit the project, data (if provided) should be deidentified.\n\n\nKey ingredients of the proposal\n\nBackground and rationale for project (1-2 paragraphs)\nDescription of data structure:\n\nTypes of data/data source. Please provide enough detail about the measures and collection procedures that a scientific reader understands the basics. For example: ECG data collected at 1000Hz using BIOPAC equipment over the course of a 5-minute Treer social stress test.\nNames and brief descriptions of key variables\nOriginal data storage format: file type, number of files, storage on file system (i.e., subdirectory structure)\n\nProposed file system structure for project\n\nPlan for maintaining unadulterated original data source\nPlan for organizing code, output, figures, and data on file system\nFor processed data, how will they be stored? Why is this optimal?\nRationale for how this plan will support understanding and accessibility for collaborators (or other researchers in the future)\nHow will you document decisions about the data processing and analysis for the project? How will you document the basic file system structure of the project?\n\nDescription of any data preprocessing steps that need to be performed outside of R (e.g., large existing codebase for denoising EEG data etc.)\nData quality assurance plan\n\nWhat are the potential sources of artifacts or errors in the data?\nWhat checks can be conducted to mitigate these problems using algorithmic methods (i.e., automated code that walks through a dataset diagnosing and reporting problems)?\nHow will missing data be identified and resolved? Resolution of missing values includes\n\nWhen possible, finding the missing data and entering it.\nDocumenting the missingness and, when possible, having a mechanism for quieting your QA script so that it does not perpetually yell at you.\n\nAre there QA steps that can only be conducted by a trained human (e.g., ECG artifact correction)? If so, briefly document why these cannot be automated or codified.\nRelatedly, can simple algorithms be included to enhance error detection for human follow-up? For example, in ECG data, consecutive changes of &gt; 300ms in the inter-beat interval may be indicative of an artifact to be checked by a human. Or in fMRI, a shift in the global mean of the signal 3SD+ above/below the temporal mean may be indicative of an artifact.\n\nData QA and processing scripts proposal: Building on the QA plan above, write out a simple algorithm (cookbook-style, roughly like a simple outline) for the order of steps in your data processing pipeline. Note that the number and organization of scripts will depend on your project. In general, try to write reasonably modular (functionally specialized) scripts. Core steps include:\n\nSetting the working directory (or how you will use an Rstudio project)\nDefining any variables that will be used throughout the script (e.g., absolute paths to output data)\nImport of original data\nBasic tidying of data, if necessary\nBasic QA checks on each dataset. What are the expected outputs that will document data quality?\nDataset merging, including fidelity checks on merge\nData manipulation/wrangling of merged data for analysis and visualization\n\nBriefly document functions that you anticipate writing to automate repetitive steps in the QA process (e.g., a function that checks for invalid data by looking for values outside of a given range).\nBriefly document functions that exist in publicly available R packages (e.g., dplyr, tidyr, purrr, editrules, validate) that you anticipate using to accomplish your project."
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html",
    "href": "materials/w04_advanced/reproducibility_good_programming.html",
    "title": "How good programming practices support scientific reproducibility",
    "section": "",
    "text": "Warning: untar(compressed=) is deprecated\nVariable definitions that control algorithmic decisions below. These are placed here to draw our attention to them – if we wish to change our decisions throughout the pipeline, these can be adjusted, rather than having to hunt them down below.\nCode\n#parameters that control our algorithmic decisions about what constitutes bad head motion\n#we put these here to draw our attention to \nfd_spike_mm &lt;- 0.3 #movement greater than this threshold (in mm) is considered a movement spike\np_spikes_cutoff &lt;- 0.2 #if there are more than this proportion of spikes (based on fd_spike_mm), exclude the subject\nfd_max_mm &lt;- 10 #any movement greater than this amount should lead the subject to be excluded"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#the-value-of-functional-programming",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#the-value-of-functional-programming",
    "title": "How good programming practices support scientific reproducibility",
    "section": "The value of functional programming",
    "text": "The value of functional programming\nOne of the greatest assets to reproducible (data) science is the use of programming paradigms that enforce good practices. In particular, functional programming, in which we view data analysis as a set of predictable transformation functions, supports a clearheaded approach to data problems that blocks many programming crutches.\nWhen we write functions, they demand that we provide specific inputs (arguments) and we demand that the function produce a predictable set of outputs. This should be an unbreakable covenant that leads us to trust functions – and in a way, to forget how they work – after we have validated that they work as expected.\nFunctions should validate their inputs. For example, if we are expecting a set of time codes for when events happened in the experiment, times should not be negative! Or, if the function is expecting a 2D matrix and we pass in a vector, what should happen?"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#scope-what-datafiles-do-we-need",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#scope-what-datafiles-do-we-need",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Scope: what data/files do we need?",
    "text": "Scope: what data/files do we need?\nWe want all of these fd.txt files to be read into R so that we can diagnose FD problems for each subject. This means we need to loop over these in some way, read each of them, and compute motion metrics for each subject."
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#scope-what-computations-are-needed",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#scope-what-computations-are-needed",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Scope: what computations are needed?",
    "text": "Scope: what computations are needed?\nOur motion metrics:\n\nAverage FD (mm) – how much were they typically moving?\nMax FD (mm) – what was the worst jump?\nProportion of FD values &gt; 0.3mm – spikes generate problems"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#tracking-down-files",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#tracking-down-files",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Tracking down files",
    "text": "Tracking down files\nHow do we find all of these files? There are many ways, but list.files() is pretty handy:\n\n\nCode\nfd_root &lt;- file.path(working_dir, \"mot_example\")\nshort_path &lt;- function(p) sub(paste0(fd_root, .Platform$file.sep), \"\", p, fixed = TRUE)\n\nfd_files &lt;- list.files(\n  path = fd_root,\n  pattern = \"fd\\\\.txt$\",\n  recursive = TRUE,\n  full.names = TRUE\n)\nhead(short_path(fd_files))\n\n\n[1] \"MMClock/MR_Proc/10637_20140304/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n[2] \"MMClock/MR_Proc/10638_20140507/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n[3] \"MMClock/MR_Proc/10662_20140415/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n[4] \"MMClock/MR_Proc/10711_20140826/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n[5] \"MMClock/MR_Proc/10717_20140813/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n[6] \"MMClock/MR_Proc/10767_20140814/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n\n\nCode\nlength(fd_files)\n\n\n[1] 130"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#get-the-mean",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#get-the-mean",
    "title": "How good programming practices support scientific reproducibility",
    "section": "get the mean",
    "text": "get the mean\n\n\nCode\n(mfd &lt;- mean(fdvec))\n\n\n[1] 0.467"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#get-the-max",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#get-the-max",
    "title": "How good programming practices support scientific reproducibility",
    "section": "get the max",
    "text": "get the max\n\n\nCode\n(maxfd &lt;- max(fdvec))\n\n\n[1] 5.42"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#how-many-fd-values-are-0.3mm",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#how-many-fd-values-are-0.3mm",
    "title": "How good programming practices support scientific reproducibility",
    "section": "how many FD values are > 0.3mm?",
    "text": "how many FD values are &gt; 0.3mm?\n\n\nCode\n(nspikes &lt;- sum(fdvec &gt; fd_spike_mm))\n\n\n[1] 117\n\n\nCode\n(pspikes &lt;- nspikes/length(fdvec))\n\n\n[1] 0.39"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#is-the-subject-irreparable-by-my-criteria",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#is-the-subject-irreparable-by-my-criteria",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Is the subject ‘irreparable’ by my criteria?",
    "text": "Is the subject ‘irreparable’ by my criteria?\n\n\nCode\n(bad_subj &lt;- pspikes &gt; p_spikes_cutoff || maxfd &gt; fd_max_mm)\n\n\n[1] TRUE\n\n\nThis subject is bad on the basis of too many small head movements – 39% are above the threshold I set.\nHow do we put this together in a single result? A list or data.frame is the easiest.\n\n\nCode\n(metrics &lt;- data.frame(mfd=mfd, maxfd=maxfd, pspikes=pspikes, bad_subj=bad_subj))\n\n\n\n\n\n\nmfd\nmaxfd\npspikes\nbad_subj\n\n\n\n\n0.467\n5.42\n0.39\nTRUE"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#using-the-participant-info-to-guide-us",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#using-the-participant-info-to-guide-us",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Using the participant info to guide us",
    "text": "Using the participant info to guide us\nThat is, one problem with looping over files is that we may catch subjects we don’t want (for other exclusion criteria) or we may be missing a file for someone whose data we expect to be present. For this reason, it’s usually best in these kinds of batch operations to have a basic records file that keeps track of expected participants. Furthermore, this file should include information about any other exclusion criteria that should be accounted for at this step. For example, we may exclude someone after the fMRI scan if they fell asleep during the experimental task, or if their performance was remarkably poor.\nHere, let’s read in a participant info file to guide us on whose fd.txt files are relevant to our group analysis. Note the FMRI_Exclude column. Furthermore, this is a study where the data were collected in two parallel substudies, which are organized into different top-level folders: MMClock and SPECC. So, we need to do a bit of dplyr-based data wrangling to get the expected structure setup.\n\n\nCode\ngroupdir &lt;- file.path(working_dir, \"mot_example\")\nmr_subdir &lt;- file.path(\"mni_aroma_minimal_fsl\", \"rest1\") #this is where we expect the processed fMRI data for each subject\nexpect_mr_file &lt;- \"rnawuktm_rest1.nii.gz\" #this is the expected file name of the fMRI data\n\n#read in and process data\nspecc_info &lt;- read.csv(file.path(\"..\", \"files\", \"SPECC_Participant_Info.csv\"), stringsAsFactors=FALSE) %&gt;%\n  filter(HasRest==1 & FMRI_Exclude==0) %&gt;%\n  mutate(ScanDate = as.Date(ScanDate, format=\"%m/%d/%y\"), Luna_ID=as.character(Luna_ID)) %&gt;%\n  #If LunaMRI is 1, then data are in the MMClock substudy\n  #  Convert ScanDate Date, then reformat YYYYMMDD to match the folder naming structure of MMClock\n  #If LunaMRI is 0, use the SPECC folder and reformat data as DDMONYR to match folder naming structure.\n  mutate(mr_dir=if_else(LunaMRI==1,\n                        file.path(groupdir, \"MMClock\", \"MR_Proc\",\n                                  paste0(Luna_ID, \"_\", format((as.Date(ScanDate, format=\"%Y-%m-%d\")), \"%Y%m%d\"))),\n                        file.path(groupdir, \"SPECC\", \"MR_Proc\",\n                                  paste0(tolower(SPECC_ID), \"_\", tolower(format((as.Date(ScanDate, format=\"%Y-%m-%d\")), \"%d%b%Y\"))))),\n         mr_file=file.path(mr_dir, mr_subdir, expect_mr_file), \n         fd_file=file.path(mr_dir, mr_subdir, \"motion_info\", \"fd.txt\"), \n         mr_exists=file.exists(mr_file), \n         file_id=if_else(LunaMRI==1, Luna_ID, SPECC_ID))\n\nstr(specc_info)\n\n\n'data.frame':   90 obs. of  17 variables:\n $ NUM_ID         : int  1 2 3 5 8 10 12 13 15 16 ...\n $ SPECC_ID       : chr  \"001RA\" \"002HS\" \"003BU\" \"005AI\" ...\n $ Luna_ID        : chr  \"11131\" \"10997\" \"10895\" \"10644\" ...\n $ BPD            : int  0 0 0 0 1 0 0 1 1 0 ...\n $ ScanDate       : Date, format: \"2013-12-07\" \"2014-03-08\" ...\n $ AgeAtScan      : num  20.5 16.2 21.6 15.2 22.8 ...\n $ Female         : int  0 1 1 1 1 1 1 1 1 0 ...\n $ HasRest        : int  1 1 1 1 1 1 1 1 1 1 ...\n $ LunaMRI        : int  0 1 1 0 0 1 1 0 0 1 ...\n $ HasClock       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ FMRI_Exclude   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Wrong_fmap_dims: int  1 1 1 1 1 1 1 0 0 0 ...\n $ mr_dir         : chr  \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/SPECC/MR_Proc/001ra_07dec2013\" \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/10997_20140308\" \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/10895_20131204\" \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/SPECC/MR_Proc/005ai_06nov2013\" ...\n $ mr_file        : chr  \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/SPECC/MR_Proc/001ra_07dec2013/mni_arom\"| __truncated__ \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/10997_20140308/mni_aro\"| __truncated__ \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/10895_20131204/mni_aro\"| __truncated__ \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/SPECC/MR_Proc/005ai_06nov2013/mni_arom\"| __truncated__ ...\n $ fd_file        : chr  \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/SPECC/MR_Proc/001ra_07dec2013/mni_arom\"| __truncated__ \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/10997_20140308/mni_aro\"| __truncated__ \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/10895_20131204/mni_aro\"| __truncated__ \"/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/SPECC/MR_Proc/005ai_06nov2013/mni_arom\"| __truncated__ ...\n $ mr_exists      : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ file_id        : chr  \"001RA\" \"10997\" \"10895\" \"005AI\" ...\n\n\nHere are the first 10 expected FD files based on the subject information file:\n\n\nCode\n#just using sub here to trim off the working_dir from fd file paths to make it easier to see in the output\nhead(sub(working_dir, \"\", specc_info$fd_file, fixed=TRUE), n=10)\n\n\n [1] \"/mot_example/SPECC/MR_Proc/001ra_07dec2013/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\" \n [2] \"/mot_example/MMClock/MR_Proc/10997_20140308/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n [3] \"/mot_example/MMClock/MR_Proc/10895_20131204/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n [4] \"/mot_example/SPECC/MR_Proc/005ai_06nov2013/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\" \n [5] \"/mot_example/SPECC/MR_Proc/008jh_13jan2014/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\" \n [6] \"/mot_example/MMClock/MR_Proc/11250_20140228/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n [7] \"/mot_example/MMClock/MR_Proc/11252_20140213/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n [8] \"/mot_example/SPECC/MR_Proc/013jk_30apr2014/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\" \n [9] \"/mot_example/SPECC/MR_Proc/015cw_03may2014/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\" \n[10] \"/mot_example/MMClock/MR_Proc/11277_20140410/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt\"\n\n\nNote that I trimmed off the first part of the path to make it easier to see on the screen."
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#writing-a-worker-function",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#writing-a-worker-function",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Writing a worker function",
    "text": "Writing a worker function\nNow that we have the FD files we expect to read from relevant subjects, it would be useful to write a short function to compute relevant FD statistics. This essentially does what we were doing in the loop above, but lets us have a predictable piece of code that takes a file, the relevant decision thresholds (e.g., how much movement is ‘too much’) and gives predictable outputs.\nImportantly, as we will see in step 8 (self-healing code), functions also give us much finer control over how to handle unexpected or missing inputs (e.g., FD files that don’t match the typical format).\nLet’s write a very simple function, then test it on our example file. This essentially goes back to step 6 for a moment (develop a working prototype), but with the goal of having a portable function that will help with scaling.\n\n\nCode\n#simplest worker function to get statistics for one file\n#note that the default arguments here may not match the thresholds in the overall pipeline\n\nfd_stats_naive &lt;- function(fd_file, fd_spike=0.5, max_prop_spikes=.1, max_spike=5) {\n  fd &lt;- read.table(fd_file, col.names = \"fd\")$fd #vector of FD values\n  n_spikes=sum(fd &gt; fd_spike) #number of spikes above the threshold\n  p_spikes &lt;- n_spikes/length(fd) #spikes as a proportion of total volumes\n  bad_fd &lt;- p_spikes &gt; max_prop_spikes || any(fd &gt; max_spike) #decisions above subject exclusion\n  \n  ret &lt;- data.frame(mean_fd=mean(fd), max_fd=max(fd), nvol=length(fd),\n                    prop_spikes=p_spikes, bad_fd=bad_fd) #note that map_dfr from purrr (below) would accept a list, too\n  \n  return(ret)\n}\n\n\nOkay, here’s the output of our function for the test file:\n\n\nCode\n#test this on a single case\nfd_stats_naive(single_file)\n\n\n\n\n\n\nmean_fd\nmax_fd\nnvol\nprop_spikes\nbad_fd\n\n\n\n\n0.467\n5.42\n300\n0.23\nTRUE\n\n\n\n\n\n\nAck! Why doesn’t this match our hand calculations above for spike proportion? Note that in the function call above, we only provided single_file as the argument to fd_stats_naive. But the default arguments for the function are:\nfd_stats_naive &lt;- function(fd_file, fd_spike=0.5, max_prop_spikes=.1, max_spike=5) {\nThus, the default is to treat FD &gt; 0.5mm as a spike, whereas we chose 0.3. There are similar mismatches between the defaults for max FD and proportion of spikes. This highlights that it is important to know the default arguments, if any, for a function. And if we should always require the user to specify every input to a function explicitly, it may be better not to use default arguments at all.\nHere’s how we correct this small oversight here. We pass forward the thresholds set at the top of this document so that the decisions/calculations match our choices above.\n\n\nCode\nfd_stats_naive(single_file, fd_spike = fd_spike_mm, max_prop_spikes = p_spikes_cutoff, max_spike=fd_max_mm)\n\n\n\n\n\n\nmean_fd\nmax_fd\nnvol\nprop_spikes\nbad_fd\n\n\n\n\n0.467\n5.42\n300\n0.39\nTRUE\n\n\n\n\n\n\nNow that we have a working prototype, let’s write a more robust worker function that we will use throughout the rest of this pipeline. In particular, it will handle missing FD files gracefully by returning NAs (with a warning) rather than crashing.\n\n\nCode\n#worker function to get statistics for one file (robust to missing files)\nfd_stats &lt;- function(fd_file, fd_spike=0.5, max_prop_spikes=.1, max_spike=5) {\n  if (!file.exists(fd_file)) {\n    warning(\"Could not read FD file: \", fd_file, \". Returning NAs.\")\n    return(data.frame(mean_fd=NA, max_fd=NA, nvol=NA, prop_spikes=NA, bad_fd=NA))\n  }\n  \n  fd &lt;- read.table(fd_file, col.names = \"fd\")$fd #vector of FDs\n  n_spikes=sum(fd &gt; fd_spike)\n  p_spikes &lt;- n_spikes/length(fd)\n  bad_fd &lt;- p_spikes &gt; max_prop_spikes || any(fd &gt; max_spike)\n  \n  ret &lt;- data.frame(mean_fd=mean(fd), max_fd=max(fd), nvol=length(fd),\n                    prop_spikes=p_spikes, bad_fd=bad_fd)\n  \n  return(ret)\n}\n\n\nFinally, we can use the map functions from the purrr package to scale up this sort of calculation rather easily. In particular, look at the map_dfr function, which maps over a vector/list and then row-binds (like dplyr::bind_rows) the returned data frames/tibbles. Here, we map over .$fd_file (a vector of file paths), compute one-row motion summaries for each file, and bind them into one data.frame.\nRemember that . in a dplyr pipeline refers to the current dataset.\nLet’s add motion information as additional columns to our participant info data.frame using purrr::map_dfr approach\n\n\nCode\nspecc_info &lt;- specc_info %&gt;%\n  bind_cols(map_dfr(.$fd_file, fd_stats, fd_spike=fd_spike_mm, max_prop_spikes=p_spikes_cutoff, max_spike=fd_max_mm))\n\n#If you wanted the group fd data.frame alone\n#vv &lt;- map_dfr(specc_info$fd_file, fd_stats, fd_spike=0.5, max_prop_spikes=.20, max_spike=fd_max_mm)\n\n#just print the motion-relevant parts\nhead(specc_info %&gt;% select(NUM_ID, mean_fd, max_fd, nvol, prop_spikes, bad_fd))\n\n\n\n\n\n\nNUM_ID\nmean_fd\nmax_fd\nnvol\nprop_spikes\nbad_fd\n\n\n\n\n1\n0.192\n0.535\n300\n0.187\nFALSE\n\n\n2\n0.085\n0.266\n300\n0.000\nFALSE\n\n\n3\n0.127\n0.284\n350\n0.000\nFALSE\n\n\n5\n0.133\n0.378\n300\n0.007\nFALSE\n\n\n8\n0.166\n1.063\n350\n0.069\nFALSE\n\n\n10\n0.218\n5.772\n300\n0.087\nFALSE"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#defining-expected-inputs-and-what-to-do-about-failures",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#defining-expected-inputs-and-what-to-do-about-failures",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Defining expected inputs and what to do about failures",
    "text": "Defining expected inputs and what to do about failures\nPart of making code self-healing is to ask yourself:\n\nWhat does the code need to do its work? (i.e., what are the inputs?)\nWhat should the code do if an input is not what is expected?\nShould an unexpected condition stop the overall execution of a pipeline, or be handled gracefully with a warning?\n\nThe third question is not a leading one. Rather, there are occasions when an entire pipeline should fail if one input is wrong because proceeding would invalidate other steps in the pipeline or give misleading results. In other cases, the failure of one step or iteration in a pipeline should not halt the broader process.\nHere, if an FD file does not exist for a subject, I would lean toward telling the user about it, but not halting the broader pipeline. To make that work, the function should:\n\nCheck whether the file exists.\nIf it doesn’t, tell the user about the problem.\nReturn NA values for motion metrics so that the function behaves predictably.\n\nWith respect to the third item, one principle of functional programming is that the output structure should always be the same so that the user of a function knows what to expect. If we are supposed to return a one-row data.frame with motion metrics, we should still do so in the case of handling problems with warnings – that is non-fatal failure conditions. If the function does not behave predictably in its outputs (e.g., returning NULL instead of a data.frame), then it will be prone to failures in a broader pipeline.\nOur pipeline uses fd_stats(), which checks that the file exists and returns a predictable one-row data.frame of NAs (with a warning) if it does not.\nHere is how it handles the case of the missing file in the context of the broader pipeline:\n\n\nCode\nvv &lt;- map_dfr(specc_info$fd_file, fd_stats, fd_spike=fd_spike_mm, max_prop_spikes=p_spikes_cutoff, max_spike=fd_max_mm)\n\n\nWarning in .f(.x[[i]], ...): Could not read FD file:\n/var/folders/s3/fxpldcp575nfjjw2t4vp8z_40000gn/T//RtmpogmHz4/mot_example/MMClock/MR_Proc/11366_20150425/mni_aroma_minimal_fsl/rest1/motion_info/fd.txt.\nReturning NAs.\n\n\nCode\nprint(vv)\n\n\n   mean_fd max_fd nvol prop_spikes bad_fd\n1   0.1924  0.535  300     0.18667  FALSE\n2   0.0848  0.266  300     0.00000  FALSE\n3   0.1273  0.284  350     0.00000  FALSE\n4   0.1331  0.378  300     0.00667  FALSE\n5   0.1664  1.063  350     0.06857  FALSE\n6   0.2179  5.772  300     0.08667  FALSE\n7   0.0867  0.566  300     0.01000  FALSE\n8   0.2505  1.748  300     0.26333   TRUE\n9   0.1741  1.196  300     0.08667  FALSE\n10  0.1799  0.669  300     0.15333  FALSE\n11  0.0899  0.234  300     0.00000  FALSE\n12  0.1364  0.721  300     0.04000  FALSE\n13  0.2135  1.622  300     0.15333  FALSE\n14  0.2187  2.160  300     0.14000  FALSE\n15  0.0996  0.235  300     0.00000  FALSE\n16  0.1294  0.446  300     0.02000  FALSE\n17  0.1191  0.421  300     0.01333  FALSE\n18  0.1699  2.590  300     0.12667  FALSE\n19  0.1507  0.421  300     0.05000  FALSE\n20  0.4055  9.938  300     0.14667  FALSE\n21  0.2418  2.720  300     0.13000  FALSE\n22  0.1995  1.375  300     0.14000  FALSE\n23  0.1174  0.838  300     0.03333  FALSE\n24  0.1488  0.379  300     0.00333  FALSE\n25  0.1584  0.790  300     0.06333  FALSE\n26  0.1527  0.832  300     0.04000  FALSE\n27  0.2976  1.416  300     0.36333   TRUE\n28  0.6360 13.915  300     0.45333   TRUE\n29  0.1090  0.528  300     0.02000  FALSE\n30  0.4719  6.919  300     0.34333   TRUE\n31  0.1642  1.023  300     0.08667  FALSE\n32  0.1029  0.778  300     0.01667  FALSE\n33  0.1256  0.344  300     0.01667  FALSE\n34  0.1710  2.364  300     0.08667  FALSE\n35  0.1414  0.299  300     0.00000  FALSE\n36  0.1018  0.309  300     0.00333  FALSE\n37  0.1735  1.984  300     0.06000  FALSE\n38  0.1524  2.227  300     0.04667  FALSE\n39  0.5838  4.684  300     0.46000   TRUE\n40  0.2280  0.662  300     0.25333   TRUE\n41  0.1135  0.728  300     0.01333  FALSE\n42  0.1472  0.352  300     0.01667  FALSE\n43  0.3695  3.708  300     0.30333   TRUE\n44  0.0830  0.196  300     0.00000  FALSE\n45  0.1189  1.743  300     0.03000  FALSE\n46  0.1965  0.779  300     0.14667  FALSE\n47  0.1149  0.610  300     0.02000  FALSE\n48  0.1016  0.694  300     0.08000  FALSE\n49  0.4840  8.800  300     0.37000   TRUE\n50  0.1966  0.601  300     0.17333  FALSE\n51  0.1437  3.184  300     0.01667  FALSE\n52  0.1502  1.659  300     0.06333  FALSE\n53  0.1525  0.455  300     0.05333  FALSE\n54  0.1651  0.747  300     0.10667  FALSE\n55  0.2939  2.436  300     0.31000   TRUE\n56  0.1113  0.337  300     0.00667  FALSE\n57  0.1213  0.476  300     0.01000  FALSE\n58  0.4562  3.814  300     0.48000   TRUE\n59  0.3325  1.184  300     0.50333   TRUE\n60  0.1679  2.180  300     0.08667  FALSE\n61  0.2845  4.832  300     0.26000   TRUE\n62  0.1039  0.295  300     0.00000  FALSE\n63  0.2015  2.564  300     0.13333  FALSE\n64  0.1641  1.412  300     0.15667  FALSE\n65  0.3127  3.622  300     0.29333   TRUE\n66  0.2677  1.663  300     0.40333   TRUE\n67  0.0910  0.317  300     0.00333  FALSE\n68  0.2199  1.613  300     0.22667   TRUE\n69  0.2918  1.039  300     0.44333   TRUE\n70  0.1595  0.546  300     0.04000  FALSE\n71  0.2050  0.548  300     0.17667  FALSE\n72  0.1584  0.708  300     0.10333  FALSE\n73  0.1136  0.294  300     0.00000  FALSE\n74  0.2304  1.232  300     0.22333   TRUE\n75  0.0949  0.975  300     0.00667  FALSE\n76  1.1024 19.032  300     0.71667   TRUE\n77  0.1420  0.354  300     0.01000  FALSE\n78  0.1040  0.485  300     0.02333  FALSE\n79  0.0889  0.266  300     0.00000  FALSE\n80  0.3496  2.738  300     0.36000   TRUE\n81  0.1748  0.484  300     0.10000  FALSE\n82  0.1298  0.969  300     0.05667  FALSE\n83  0.1448  0.332  300     0.01333  FALSE\n84  0.1013  0.193  300     0.00000  FALSE\n85  0.2062  1.278  300     0.17000  FALSE\n86  0.1510  0.527  300     0.04333  FALSE\n87      NA     NA   NA          NA     NA\n88  0.1549  1.970  300     0.02667  FALSE\n89  0.1241  0.709  300     0.02000  FALSE\n90  0.1371  0.520  300     0.02333  FALSE\n\n\nNote the NAs on row 87. This is what we expect (and want) – a graceful failure, with NAs propagated for that subject."
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#more-flexible-error-trapping-using-trycatch",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#more-flexible-error-trapping-using-trycatch",
    "title": "How good programming practices support scientific reproducibility",
    "section": "More flexible error trapping using tryCatch",
    "text": "More flexible error trapping using tryCatch\nFor more general error handling in R, particularly in functions, check out the tryCatch function. This function tries to evaluate a given R expression, but allows you to catch and handle any arbitrary error. For example, let’s try to evaluate whether yy is greater than 2. But yy is not defined anywhere in this document – that is, it doesn’t exist. If you just try yy &gt; 2, you will get a fatal error. What if in this circumstance (undefined variable), we instead want to print the error, but return NA?\n\n\nCode\n#this will generate an error if uncommented because yy is not defined anywhere in this document\n#is_yy_big &lt;- yy &gt; 2\n\nis_yy_big &lt;- tryCatch(yy &gt; 2, error=function(err) { print(err); return(NA )})\n\n\n&lt;simpleError in eval(expr, envir): object 'yy' not found&gt;\n\n\nCode\nprint(is_yy_big)\n\n\n[1] NA\n\n\nNow that we’re done with the section on handling failures gracefully, I’m going to put the missing fd.txt file back into place for the final step of global metrics."
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#code-readability",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#code-readability",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Code readability",
    "text": "Code readability\nAt the end of solving a difficult data analysis challenge, you may have a set of scripts that work, but that are hard to understand. The scripts may have poor code readability (interesting read here: https://medium.com/@egonelbre/psychology-of-code-readability-d23b1ff1258a). Common examples include\n\nUnclear variable names: tmpa or var22.\nUse of numerical indices for subsetting data without documenting the choice: trials_a &lt;- my_matrix[1:50,]\nLots of vestigial code that appears similar, but has been commented out:\n\n#abc &lt;- read.table(\"dataset.txt.gz\")\n#abc &lt;- read.table(\"dataset_updated.txt.gz\")\nabc &lt;- read.table(\"dataset_updated_again.txt.gz\")\n\n#abc$trial &lt;- abc$trial - 1\nabc$trial &lt;- 1:50\n\nToo much inline code in a large pipeline – abstract to functions!!\n\n\n\nCode\nfor (i in 1:length(IDs_list)) {\n  print(i)\n  id = as.character(IDs_list[[i]])\n\n  runVolumeInfo &lt;- dplyr::filter(diminfo, ID == id)\n  forRunVolume &lt;- c()\n  forRunVolume &lt;- c(forRunVolume, as.numeric(runVolumeInfo$runVolumes))\n\n  if (length(forRunVolume) == 1) {\n    # Hidden dependencies: dozens of objects referenced in this block are assumed to exist.\n    events &lt;- rbind(\n      data.frame(\n        event = \"partnerchoice\",\n        run = 1,\n        trial = 1:144,\n        onset = pchoice_onsets_df[[i]]/1000 - itifixactions_list[[i]]/1000,\n        duration = pchoice_durations_df[[i]]/1000\n      )\n    )\n    # ... plus displaychoice and outcome events ...\n\n    trusteeDummyCodes = trusteeIDs_df[[i]]\n    d1 = dplyr::recode(trusteeDummyCodes, `0` = 0, `1` = 0, `-1` = 1)\n    d2 = dplyr::recode(trusteeDummyCodes, `0` = 0, `1` = 1, `-1` = 0)\n    d1xPE = d1*modelPEs_tshift_zscore_df[[i]]\n    d2xPE = d2*modelPEs_tshift_zscore_df[[i]]\n    signals &lt;- list(\n      partnerchoice = list(event = \"partnerchoice\", normalization = \"none\", value = data.frame(run = 1, trial = 1:144, value = 1)),\n      outcome = list(event = \"outcome\", normalization = \"none\", value = data.frame(run = 1, trial = 1:144, value = 1)),\n      # ... dozens more regressors ...\n    )\n\n    newsubdir &lt;- id\n\n    # Side effects: creates directories, changes working directory, and may write files.\n    dir.create(file.path(savedmlocation, newsubdir))\n    dir.create(file.path(savedmlocation, newsubdir, \"block_design\"))\n    setwd(file.path(savedmlocation, newsubdir, \"block_design\"))\n\n    simp_tmp_dm &lt;- build_design_matrix(events = events, signals = signals, tr= .6)\n    simp_dm&lt;- rbind(simp_dm, simp_tmp_dm)\n\n    # ... repeat similar blocks:\n    # - simp_w_interaction\n    # - parametric_reg\n    # - parametric_w_interaction\n  }\n\n}\n\n\nKey readability problems illustrated here:\n\nHidden dependencies: the loop relies on many external objects (data frames, lists, output containers) that are not passed in as inputs.\nSide effects: dir.create() and setwd() change the filesystem and global state, making runs order-dependent and harder to debug.\nRepetition: near-identical blocks differ only in small parameter tweaks; this is a sign you want a worker function + parameters.\nMagic numbers and unit conversions: values like 1:144, /1000, and tr = .6 are embedded without explanation.\n\n\nPoor documentation:\n\nrewardblue=[-1 0 1];\nrewardred=[-2 0 2];\nrewardred=rewardred';\nrewardblue=rewardblue';\nvisite=[];\nalphachapeau{1}=0.1*ones(6,1);\ncrochetcl{1}=zeros(6,3);\ncl{1}=zeros(6,3);\neta{1}=ones(6,1);\ncrochetcl{1}=zeros(6,3);\npcirconflexe{1}=ones(6,1)*[1/3 1/3 1/3];\nmu{1}=1*ones(6,1);\npzero=(1/3)*ones(6,3);\n%attention aux initialisations\nmuzero=1;\n%muzero=ones(6,1);\nmuante{1}=0*ones(6,1);\nentropy{1}=zeros(6,2);\nresponseconflict=[];\naleatory{1}=0.65*ones(6,1);\n\nPoor organization and documentation of script dependencies so that things have be run ‘just so’.\n\nscript1.R\nsource(\"import.R\")\n\ndata2 &lt;- process_dataset(data1)\nscript2.R\nsource(\"script1.R\")\n\ndata2 &lt;- convert_timecodes(data2)\nanalysis.R\nsource(\"script1.R\")\n#source(\"script2.R\")\nmodel_list &lt;- analyze_timecodes(data2) #But what if the time codes have to be converted in script2?!"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#documentation",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#documentation",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Documentation",
    "text": "Documentation\nCoding often reflects a series of multifactorial decisions that unfold over days or weeks. We often rely on short-term memory to keep track of code architecture in our heads from one day to the next. But when you get code to a reasonable state and move onto another part of the project, you will forget most, if not all, of your decisions and thinking. Simply put, once code is working, it’s easy to ‘dump it’ from your mind. This may be a matter of days or weeks, but you’re very unlikely to remember the context months later.\nIn this way, functional programming is great because it forces you to follow a paradigm that identifies sufficient inputs for a function as well as expected outputs.\nBut documentation is another key component. I would argue that more complex problems call for more verbose documentation, often in the form of inline comments. Example:\n\n\nCode\n#Example:\n#Average Latent Class Probabilities for Most Likely Latent Class Membership (Row)\n#by Latent Class (Column)\n#\n#           1        2\n#\n#  1   0.986    0.014\n#  2   0.030    0.970\n#\n#A bit of a wonky section. Some notes:\n# 1) Rows represent those hard classified into that class.\n# 2) Rows sum to 1.0 and represent the summed average posterior probabilities of all the class assignment possibilities.\n# 3) Columns represent average posterior probability of being in class 1 for those hard classified as 1 or 2.\n# 4) High diagonal indicates that hard classification matches posterior probability patterns.\n\ncountlist[[\"avgProbs.mostLikely\"]] &lt;- unlabeledMatrixExtract(mostLikelyProbs, filename)"
  },
  {
    "objectID": "materials/w04_advanced/reproducibility_good_programming.html#refactoring",
    "href": "materials/w04_advanced/reproducibility_good_programming.html#refactoring",
    "title": "How good programming practices support scientific reproducibility",
    "section": "Refactoring",
    "text": "Refactoring\nBy the time you finish a hard data/programming problem, you will know so much more than when you began. You’ll have opinions about why you approached the problems in a certain way. You’ll also probably realize that your solution is more complicated than it needs to be. For example, you may write several functions, only to realize that they each contain the same data manipulation step as part of the set of procedures.\nRefactoring can be time consuming, but it is least so when the code is relatively fresh and you still have a good short-term memory of how everything works. Again, evaluate how mission-critical this is, but do consider refactoring as an important final step of finishing a project."
  },
  {
    "objectID": "materials/w02_wrangling/strings_stringr.html",
    "href": "materials/w02_wrangling/strings_stringr.html",
    "title": "Strings in R with stringr",
    "section": "",
    "text": "The stringr package provides a consistent set of functions for working with strings. All functions start with str_ and are vectorized, so they work naturally with columns in a data.frame.\nWe will use a small example dataset to demonstrate the core verbs.\nCode\npeople &lt;- tibble::tibble(\n  id = 1:5,\n  name = c(\"Ada Lovelace\", \"Grace Hopper\", \"Margaret Hamilton\",\n           \"Katherine Johnson\", \"Mary Jackson\"),\n  email = c(\"ada@navy.mil\", \"grace@navy.mil\", \"margaret@mit.edu\",\n            \"katherine@nasa.gov\", NA),\n  dept = c(\"CompSci\", \"CompSci\", \"Engineering\", \"Research\", \"Research\")\n)\n\npeople %&gt;% kable_table()\n\n\n\n\n\nid\nname\nemail\ndept\n\n\n\n\n1\nAda Lovelace\nada@navy.mil\nCompSci\n\n\n2\nGrace Hopper\ngrace@navy.mil\nCompSci\n\n\n3\nMargaret Hamilton\nmargaret@mit.edu\nEngineering\n\n\n4\nKatherine Johnson\nkatherine@nasa.gov\nResearch\n\n\n5\nMary Jackson\nNA\nResearch"
  },
  {
    "objectID": "materials/w02_wrangling/strings_stringr.html#combine-strings",
    "href": "materials/w02_wrangling/strings_stringr.html#combine-strings",
    "title": "Strings in R with stringr",
    "section": "Combine strings",
    "text": "Combine strings\n\n\nCode\npeople %&gt;%\n  mutate(\n    label = str_c(name, \" (\", dept, \")\", sep = \"\")\n  ) %&gt;%\n  kable_table()\n\n\n\n\n\nid\nname\nemail\ndept\nlabel\n\n\n\n\n1\nAda Lovelace\nada@navy.mil\nCompSci\nAda Lovelace (CompSci)\n\n\n2\nGrace Hopper\ngrace@navy.mil\nCompSci\nGrace Hopper (CompSci)\n\n\n3\nMargaret Hamilton\nmargaret@mit.edu\nEngineering\nMargaret Hamilton (Engineering)\n\n\n4\nKatherine Johnson\nkatherine@nasa.gov\nResearch\nKatherine Johnson (Research)\n\n\n5\nMary Jackson\nNA\nResearch\nMary Jackson (Research)\n\n\n\n\n\nstr_glue() is convenient for inline formatting:\n\n\nCode\npeople %&gt;%\n  mutate(label = str_glue(\"{name} [{dept}]\")) %&gt;%\n  kable_table()\n\n\n\n\n\nid\nname\nemail\ndept\nlabel\n\n\n\n\n1\nAda Lovelace\nada@navy.mil\nCompSci\nAda Lovelace [CompSci]\n\n\n2\nGrace Hopper\ngrace@navy.mil\nCompSci\nGrace Hopper [CompSci]\n\n\n3\nMargaret Hamilton\nmargaret@mit.edu\nEngineering\nMargaret Hamilton [Engineering]\n\n\n4\nKatherine Johnson\nkatherine@nasa.gov\nResearch\nKatherine Johnson [Research]\n\n\n5\nMary Jackson\nNA\nResearch\nMary Jackson [Research]\n\n\n\n\n\nIf you want to collapse a vector into one string, use str_flatten():\n\n\nCode\nstr_flatten(people$dept, collapse = \", \")\n\n\n[1] \"CompSci, CompSci, Engineering, Research, Research\""
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html",
    "title": "Data wrangling in dplyr",
    "section": "",
    "text": "The goal of this document is to provide a basic introduction to data wrangling using functions from the so-called ‘tidyverse’ approach. The tidyverse (https://www.tidyverse.org) is a set of data science packages in R that are intended to provide a consistent paradigm for working with data. This approach unifies a previously inchoate landscape of different functions and packages in R that could be daunting to new users.\nAlthough I do not claim that the tidyverse approach is best according to all possible criteria, I believe that it is the best paradigm for working with data in R for social scientists, many of whom do not have a formal background in computer programming.\nHere, I will draw primarily from the tidyr and dplyr packages in R.\nFor an excellent book-length treatment of the tidyverse approach, see R for Data Science (2nd edition) by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund."
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#addressing-namespace-collisions",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#addressing-namespace-collisions",
    "title": "Data wrangling in dplyr",
    "section": "Addressing namespace collisions",
    "text": "Addressing namespace collisions\n\nWatch out for warnings about objects being ‘masked’ when packages are loaded.\nExplicitly specify the package where your desired function lives using the double colon operator. Example: dplyr::summarize.\nTry to load tidyverse packages using library(tidyverse). This handles collisions within the tidyverse!\n\nFor example, in this document, multilevel pulls in MASS, which masks dplyr::select. If we were to load dplyr first, then multilevel, we would see the following:\n&gt; library(dplyr)\n&gt; library(multilevel)\nLoading required package: nlme\n\nAttaching package: ‘nlme’\n\nThe following object is masked from ‘package:dplyr’:\n\n    collapse\n\nLoading required package: MASS\n\nAttaching package: ‘MASS’\n\nThe following object is masked from ‘package:dplyr’:\n\n    select\nRead in English, this says, “The multilevel package needs to load nlme (a mixed-effects package). Nlme has a function called ‘collapse’ that will now take precedence over ‘collapse’ in dplyr if you run collapse() in your session. Multilevel also needs the MASS package, so I’m loading it. The MASS package has a function called ‘select’ that now takes precedence over dplyr’s select function, so if you call select() in your session, you will be getting the MASS package function, not the dplyr one.”\nTo get around these problems, the easiest route is to load dplyr (and other tidyverse packages) last in the chain, giving them precedence over other packages. As you see above in the p_load call, we load dplyr last in the chain so that its select function is preferred over MASS. This is important because functions of the same name across packages may have completely different purposes and syntax!\nThe slightly harder, but more robust, route is always to use explicit namespace qualifiers with the :: operator. If you use dplyr::select as the function call, you are explicitly telling R that you want dplyr’s select function, not some other package’s.\nThe two most common collisions are with select (MASS) and summarize (Hmisc). If you’re not sure which version of a function has taken precedence you can type the function name without parentheses. This will print the function source (which you can ignore). But take a look at the last line, which reveals where R ‘found’ the function in the search across loaded packages (based on their load order/precedence).\n&gt; select\nfunction (obj) \nUseMethod(\"select\")\n&lt;bytecode: 0x93b18ebd8&gt;\n&lt;environment: namespace:MASS&gt;\nIf you were expecting that last line to read &lt;environment: namespace:dplyr&gt; (i.e. dplyr’s version), then you have a namespace problem to resolve!"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#use-of-this-reference-in-tidyverse",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#use-of-this-reference-in-tidyverse",
    "title": "Data wrangling in dplyr",
    "section": "Use of ‘this’ reference in tidyverse",
    "text": "Use of ‘this’ reference in tidyverse\nSometimes it is useful to refer to the current dataset or variable explicitly in tidyverse data wrangling syntax.\ndplyr/magrittr tends to hide this from us for convenience, but it’s there under the hood.\niris %&gt;% filter(Sepal.Length &gt; 7)\nis the same as\niris %&gt;% filter(., Sepal.Length &gt; 7)\nSo, '.' refers to the current dataset or variable (depending on context) in dplyr operations. And if you don’t specify where the '.' falls in your syntax, it will always be passed as the first argument to the downstream function."
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#overview-a-first-pass-through-an-nhanes-dataset",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#overview-a-first-pass-through-an-nhanes-dataset",
    "title": "Data wrangling in dplyr",
    "section": "Overview: a first pass through an NHANES dataset",
    "text": "Overview: a first pass through an NHANES dataset\nTo learn dplyr, let’s start with a survey from the National Health and Nutrition Examination Survey (NHANES) dataset. These data are provided in the nhanesA package. We’ll start by looking at a couple of basic demographic variables.\nNHANES variables used below (from DEMO_D and BMX_D):\n\nSEQN: respondent ID\nRIAGENDR: gender (coded numeric in raw NHANES)\nRIDAGEYR: age in years\nDMDHREDU: education level\nBMXWT: weight (kg)\nBMXARML: upper arm length (cm)\nBMXTHICR: thigh circumference (cm)\n\n\n\nCode\n# demographics: note that this code relies on functions in the nhanesA package, which\n#   is useful for working with NHANES data, but is not a focus of this workshop per se.\n\ndemo_d &lt;- nhanes('DEMO_D')\ndemo_d_vars  &lt;- nhanesTableVars('DEMOGRAPHICS', 'DEMO_D', namesonly=TRUE)\n\n# translate numeric codes into factors by using the nhanes lookup/codebook functions\ndemo_d &lt;- suppressWarnings(nhanesTranslate('DEMO_D', demo_d_vars, data=demo_d))\n\n\nTranslated columns: AIALANG DMDBORN DMDCITZN DMDEDUC2 DMDEDUC3 DMDHRBRN DMDHREDU DMDHRGND DMDHRMAR DMDHSEDU DMDMARTL DMDSCHOL DMDYRSUS DMQMILIT FIAINTRP FIALANG FIAPROXY INDFMINC INDHHINC MIAINTRP MIALANG MIAPROXY RIAGENDR RIDEXMON RIDEXPRG RIDRETH1 RIDSTATR SDDSRVYR SIAINTRP SIALANG SIAPROXY\n\n\nCode\n# dplyr pipeline\ndemo_d &lt;- demo_d %&gt;% \n  filter(!INDHHINC %in% c(\"Over $20,000\", \"Under $20,000\", \"Refused\", \"Don't know\")) %&gt;% \n  droplevels() %&gt;% # drop unused factor levels from the data.frame\n  mutate(\n    # case_when() is a vectorized if/else ladder for many conditions\n    income_num = case_when( # convert range-based factor labels to midpoint numbers\n    INDHHINC == \"$     0 to $ 4,999\" ~ 2500,\n    INDHHINC == \"$ 5,000 to $ 9,999\" ~ 7500,\n    INDHHINC == \"$10,000 to $14,999\" ~ 12500,\n    INDHHINC == \"$15,000 to $19,999\" ~ 17500,\n    INDHHINC == \"$20,000 to $24,999\" ~ 22500,\n    INDHHINC == \"$25,000 to $34,999\" ~ 30000,\n    INDHHINC == \"$35,000 to $44,999\" ~ 40000,\n    INDHHINC == \"$45,000 to $54,999\" ~ 50000,\n    INDHHINC == \"$55,000 to $64,999\" ~ 60000,\n    INDHHINC == \"$65,000 to $74,999\" ~ 70000,\n    INDHHINC == \"$75,000 and Over\" ~ 80000\n    )\n  )\n \n#load body (biometric) measures\nbmx_d &lt;- nhanes('BMX_D')\nbmx_d_vars  &lt;- nhanesTableVars('EXAM', 'BMX_D', namesonly=TRUE)\nbmx_d &lt;- suppressWarnings(nhanesTranslate('BMX_D', bmx_d_vars, data=bmx_d))\n\n\nTranslated columns: BMDSTATS BMIARMC BMIARML BMICALF BMIHT BMILEG BMIRECUM BMISUB BMITHICR BMITRI BMIWAIST BMIWT\n\n\nCode\n# merge the education demographics variable with the biometric data, joining on the SEQN column (basically an ID)\nbmx_d &lt;- demo_d %&gt;% \n  dplyr::select(SEQN, DMDHREDU) %&gt;% \n  inner_join(bmx_d, by=\"SEQN\")"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#group_by-summarize",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#group_by-summarize",
    "title": "Data wrangling in dplyr",
    "section": "group_by + summarize",
    "text": "group_by + summarize\nLet’s summarize the mean household income (income_num), converted from categories in INDHHINC) by highest level of education completed (DMDHREDU)\n\n\nCode\ndemo_d %&gt;% \n  group_by(DMDHREDU) %&gt;% # divide dataset into separate compartments by education level\n  dplyr::summarize(\n    n=n(), # number of observations\n    m_income=mean(income_num, na.rm=T), \n    sd_income=sd(income_num, na.rm=T)\n  ) %&gt;% \n  kable_table()\n\n\n\n\n\nDMDHREDU\nn\nm_income\nsd_income\n\n\n\n\nLess Than 9th Grade\n1173\n28024\n18489\n\n\n9-11th Grade (Includes 12th grade with no diploma)\n1626\n31318\n21584\n\n\nHigh School Grad/GED or equivalent\n2317\n39400\n23422\n\n\nSome College or AA degree\n2664\n48137\n25326\n\n\nCollege Graduate or above\n1768\n63785\n21816\n\n\nRefused\n6\n25000\n12624\n\n\nDon't know\n26\n20900\n15760\n\n\nNA\n276\n44623\n24849\n\n\n\n\n\nNote that summarize removes a single level of grouping in the group_by process. Here, we only have one grouping variable, DMDHREDU, so the output of summarize will be ‘ungrouped.’"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#grouped-summaries-of-several-variables",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#grouped-summaries-of-several-variables",
    "title": "Data wrangling in dplyr",
    "section": "Grouped summaries of several variables",
    "text": "Grouped summaries of several variables\nWhat if I want to have means and SDs for several continuous variables grouped by highest education? Let’s look specifically at the weight (BMXWT), upper arm length (BMXARML), and thigh circumference (BMXTHICR) measurements. The combination of summarize and across provide functionality to specify several variables using the .cols argument of across and potentially several summary functions by passing them in a named list.\n\n\nCode\nbmx_d %&gt;% \n  group_by(DMDHREDU) %&gt;% \n  dplyr::summarize(\n    across(\n      c(BMXWT, BMXARML, BMXTHICR), \n      list(m=~mean(.x, na.rm=T), sd=~sd(.x, na.rm=T))\n    )\n  ) %&gt;% \n  kable_table()\n\n\n\n\n\nDMDHREDU\nBMXWT_m\nBMXWT_sd\nBMXARML_m\nBMXARML_sd\nBMXTHICR_m\nBMXTHICR_sd\n\n\n\n\nLess Than 9th Grade\n58\n30\n32\n7.8\n50\n7.48\n\n\n9-11th Grade (Includes 12th grade with no diploma)\n56\n33\n32\n8.2\n51\n8.40\n\n\nHigh School Grad/GED or equivalent\n61\n33\n33\n7.9\n52\n8.10\n\n\nSome College or AA degree\n62\n32\n33\n7.6\n52\n8.65\n\n\nCollege Graduate or above\n61\n31\n33\n7.8\n52\n7.99\n\n\nRefused\n70\n25\n38\n2.5\n50\n0.57\n\n\nDon't know\n50\n36\n28\n11.5\n53\n7.74\n\n\nNA\n56\n29\n32\n7.6\n51\n7.73\n\n\n\n\n\nLet’s slow this down:\n\ngroup_by verb\nsurvey %&gt;% group_by(DMDHREDU)\nThis tells dplyr to divide the bmx_d (NHANES biometric measurements) data into a set of smaller data.frame objects, one per level of DMDHREDU. Internally, this looks something like the output below. After this division of the dataset into chunks, summarize will work on each chunk individually.\n\n\n$`Less Than 9th Grade`\n   SEQN            DMDHREDU                    BMDSTATS BMXWT\n1 31137 Less Than 9th Grade          Other partial exam    80\n2 31157 Less Than 9th Grade Complete data for age group    42\n3 31169 Less Than 9th Grade Complete data for age group    22\n4 31175 Less Than 9th Grade Complete data for age group    86\n\n$`9-11th Grade (Includes 12th grade with no diploma)`\n   SEQN                                           DMDHREDU\n1 31128 9-11th Grade (Includes 12th grade with no diploma)\n2 31133 9-11th Grade (Includes 12th grade with no diploma)\n3 31145 9-11th Grade (Includes 12th grade with no diploma)\n4 31148 9-11th Grade (Includes 12th grade with no diploma)\n                     BMDSTATS BMXWT\n1 Complete data for age group    40\n2 Complete data for age group    45\n3 Complete data for age group    40\n4 Complete data for age group    52\n\n$`High School Grad/GED or equivalent`\n   SEQN                           DMDHREDU                    BMDSTATS BMXWT\n1 31127 High School Grad/GED or equivalent Complete data for age group    10\n2 31139 High School Grad/GED or equivalent          Other partial exam    74\n3 31140 High School Grad/GED or equivalent Complete data for age group    42\n4 31143 High School Grad/GED or equivalent Complete data for age group    76\n\n$`Some College or AA degree`\n   SEQN                  DMDHREDU                    BMDSTATS BMXWT\n1 31129 Some College or AA degree Complete data for age group    75\n2 31130 Some College or AA degree  No body measures exam data    NA\n3 31138 Some College or AA degree Complete data for age group    14\n4 31142 Some College or AA degree          Other partial exam    80\n\n$`College Graduate or above`\n   SEQN                  DMDHREDU                    BMDSTATS BMXWT\n1 31131 College Graduate or above          Other partial exam    75\n2 31132 College Graduate or above Complete data for age group    70\n3 31135 College Graduate or above Complete data for age group    10\n4 31141 College Graduate or above Complete data for age group    60\n\n$Refused\n   SEQN DMDHREDU                             BMDSTATS BMXWT\n1 33385  Refused          Complete data for age group    78\n2 35296  Refused Partial:  Height and weight obtained    38\n3 37761  Refused          Complete data for age group    67\n4 40762  Refused Partial:  Height and weight obtained    99\n\n$`Don't know`\n   SEQN   DMDHREDU                    BMDSTATS BMXWT\n1 31546 Don't know          Other partial exam    78\n2 32106 Don't know Complete data for age group    12\n3 32626 Don't know Complete data for age group    64\n4 32648 Don't know Complete data for age group    85\n\n\n\n\nsummarize + across\nThe summarize function transforms a dataset that has many rows to a dataset that has a single row per grouping unit. If you do not use group_by, summarize will yield an overall summary statistic in the entire dataset. For example, to get the mean and SD of household income in NHANES, irrespective of education level or other categorical moderators, we could just use a simple summarize:\n\n\nCode\ndemo_d %&gt;%\n  dplyr::summarize(\n    m_income=mean(income_num, na.rm=T), \n    sd_income=sd(income_num, na.rm=T)\n  ) %&gt;% \n  kable_table()\n\n\n\n\n\nm_income\nsd_income\n\n\n\n\n43590\n25726\n\n\n\n\n\nBut because we used group_by(DMDHREDU) above, we got unique summaries of the variables at each level of education.\nThe across function accepts two primary arguments. First, we specify a set of variables (the .cols argument) that we wish to summarize in the same way (i.e., compute the same summary statistics). Second, we specify which statistics we wish to compute (the .fns argument). In our case, the syntax was:\ndplyr::summarize(\n    across(c(BMXWT, BMXARML, BMXTHICR), \n           list(m=~mean(.x, na.rm=T), sd=~sd(.x, na.rm=T)))\n)\nThe c() function specifies a vector of unquoted variable names in the dataset we wish to summarize, separated by commas. Note that any tidyselect operator for selecting columns will work. This includes starts_with, ends_with, matches, and others. For details, see ?dplyr_tidy_select.\nThe list() here asks dplyr to run each function in the list against each variables in the .cols specification. Here, this means that dplyr will compute the mean and SD of each variable in the .cols argument at each level of education completed (the group_by basis). The names of the list elements (left side) — here, m and sd — become the suffixes added for each variable. The value of the element (right side) — here, mean and sd — are the functions that should be used to compute a summary statistic (they should return one number per grouped variable).\nNote that the column names that result from an across operation can be modified using the .names argument. The default is to suffix the variable name with the names of the functions list, preceded by an underscore (e.g., income_m).\nPassing arguments to summary functions\nNotice how we passed na.rm=TRUE to the mean function within the list. This tells the mean to ignore missing (NA) values when computing the mean (i.e., mean of the non-missing numbers). In general, the dplyr syntax using what they call “lambdas” (starting with ~) is the clearest way to control the arguments passed to each function. Here is a simple definition of a lambda in R:\n~ mean(.x, na.rm=TRUE)\nThe .x refers to the current variable being used within a dplyr data wrangling operation. This is in contrast to ., which generally refers to the current dataset.\nIf you don’t need to pass arguments to the functions in an across() operation, you can just state the function name:\ndplyr::summarize(\n    across(c(BMXWT, BMXARML, BMXTHICR), \n           list(m=mean, sd=sd))\n)"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#making-a-summarize-pipeline-even-more-beautiful",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#making-a-summarize-pipeline-even-more-beautiful",
    "title": "Data wrangling in dplyr",
    "section": "Making a summarize pipeline even more beautiful",
    "text": "Making a summarize pipeline even more beautiful\nWe can also make the output more beautiful using tidying techniques we’ve already seen in the tidyr tutorial. Remember that R is all about programming for data science. In particular, notice that we have some columns that are means and others that are SDs.\nWe can just extend our data pipeline a bit. The extract function from tidyr here is like separate, but with a bit more oomph using regular expressions. This is a more intermediate topic, but there is a useful tutorial here: http://www.regular-expressions.info/tutorial.html.\n\n\nCode\nbmx_d %&gt;% \n  group_by(DMDHREDU) %&gt;% \n  dplyr::summarize(\n    across(\n      c(BMXWT, BMXARML, BMXTHICR), \n      list(m=~mean(.x, na.rm=T), sd=~sd(.x, na.rm=T))\n    )\n  ) %&gt;%\n  \n   # combine m and sd statistics (notice how you can add comments inline within a pipeline?)\n  pivot_longer(cols=-DMDHREDU, names_to = \"Measure\", values_to = \"value\") %&gt;%\n  \n  # divide income_m into income and m\n  #extract(col=Measure, into=c(\"bio_measure\", \"statistic\"), regex=(\"(.*)_(.*)$\")) %&gt;% \n  separate(col=Measure, into=c(\"bio_measure\", \"statistic\"), sep=\"_\") %&gt;% \n  pivot_wider(names_from=statistic, values_from = value) %&gt;% \n  arrange (bio_measure, DMDHREDU) %&gt;%\n  kable_table()\n\n\n\n\n\nDMDHREDU\nbio_measure\nm\nsd\n\n\n\n\nLess Than 9th Grade\nBMXARML\n32\n7.77\n\n\n9-11th Grade (Includes 12th grade with no diploma)\nBMXARML\n32\n8.22\n\n\nHigh School Grad/GED or equivalent\nBMXARML\n33\n7.86\n\n\nSome College or AA degree\nBMXARML\n33\n7.61\n\n\nCollege Graduate or above\nBMXARML\n33\n7.81\n\n\nRefused\nBMXARML\n38\n2.47\n\n\nDon't know\nBMXARML\n28\n11.49\n\n\nNA\nBMXARML\n32\n7.62\n\n\nLess Than 9th Grade\nBMXTHICR\n50\n7.48\n\n\n9-11th Grade (Includes 12th grade with no diploma)\nBMXTHICR\n51\n8.40\n\n\nHigh School Grad/GED or equivalent\nBMXTHICR\n52\n8.10\n\n\nSome College or AA degree\nBMXTHICR\n52\n8.65\n\n\nCollege Graduate or above\nBMXTHICR\n52\n7.99\n\n\nRefused\nBMXTHICR\n50\n0.57\n\n\nDon't know\nBMXTHICR\n53\n7.74\n\n\nNA\nBMXTHICR\n51\n7.73\n\n\nLess Than 9th Grade\nBMXWT\n58\n30.15\n\n\n9-11th Grade (Includes 12th grade with no diploma)\nBMXWT\n56\n33.44\n\n\nHigh School Grad/GED or equivalent\nBMXWT\n61\n32.70\n\n\nSome College or AA degree\nBMXWT\n62\n32.48\n\n\nCollege Graduate or above\nBMXWT\n61\n31.30\n\n\nRefused\nBMXWT\n70\n25.08\n\n\nDon't know\nBMXWT\n50\n36.00\n\n\nNA\nBMXWT\n56\n29.12\n\n\n\n\n\n\narrange: order observations\nToward the end of the pipeline above, we see:\narrange (bio_measure, DMDHREDU) %&gt;%\nThe arrange verb in dplyr requests that observations be sorted according to one or more variables. Here, we ask for the dataset to be sorted by bio_measure (biometric measure, such as weight) first, then by education level within that measure. The arrange verb sorts observations in ascending order (low to high) by default, but data can be sorted in descending order using the desc() function:\narrange(bio_measure, desc(DMDHREDU)) %&gt;%\nThis would sort by highest to lowest education level within each measure (bio_measure), where the bio_measures are still in ascending (alphabetical) order"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#filter-obtaining-observations-rows-based-on-some-criteria",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#filter-obtaining-observations-rows-based-on-some-criteria",
    "title": "Data wrangling in dplyr",
    "section": "filter: obtaining observations (rows) based on some criteria",
    "text": "filter: obtaining observations (rows) based on some criteria\nObjective: Retain only men in company A\n\n\nCode\nCOMPANY=\"A\"\nCOMPANY &lt;- \"A\"\n\ncompany_A_men &lt;- filter(univbct, COMPANY==\"A\" & GENDER==1)\n#print 10 observations at random to check the accuracy of the filter\n#p=11 just shows the first 11 columns to keep it on one page for formatting\ncompany_A_men %&gt;% sample_n(10) %&gt;% kable_table(p=11)\n\n\n\n\n\n\nBTN\nCOMPANY\nMARITAL\nGENDER\nHOWLONG\nRANK\nEDUCATE\nAGE\nJOBSAT1\nCOMMIT1\nREADY1\n\n\n\n\n232\n1022\nA\n2\n1\n2\n15\n2\n29\n3.7\n3.3\n3.2\n\n\n1373\n404\nA\n2\n1\n1\n16\n4\n36\n4.0\n4.0\n3.2\n\n\n1395\n3066\nA\n2\n1\n2\n15\n2\n26\n4.0\n4.0\n3.8\n\n\n1235\n3066\nA\n2\n1\n2\n16\n2\n34\n3.0\n3.7\n3.0\n\n\n1420\n3066\nA\n2\n1\n3\n13\n3\n21\n3.0\n3.7\n2.8\n\n\n1208\n4\nA\n2\n1\n3\n15\n2\n28\n2.3\n2.7\n1.8\n\n\n233\n1022\nA\n2\n1\n2\n15\n2\n29\n3.7\n3.3\n3.2\n\n\n1159\n299\nA\n2\n1\n3\n14\n2\n24\n2.7\n3.0\n2.5\n\n\n445\n1022\nA\n1\n1\n1\n14\n2\n23\n3.7\n3.3\n3.5\n\n\n616\n4042\nA\n1\n1\n2\n13\n2\n19\n1.3\n3.3\n2.5\n\n\n\n\n\nObjective: Count how many people are in companies A and B\n\n\nCode\nfilter(univbct, COMPANY %in% c(\"A\",\"B\")) %&gt;% nrow()\n\n\n[1] 750\n\n\nObjective: What about counts by company and battalion?\n\n\nCode\nunivbct %&gt;% \n  group_by(BTN, COMPANY) %&gt;% \n  tally() %&gt;%\n  kable_table(n=12)\n\n\n\n\n\nBTN\nCOMPANY\nn\n\n\n\n\n4\nA\n66\n\n\n4\nB\n15\n\n\n4\nC\n12\n\n\n4\nD\n30\n\n\n4\nHHC\n18\n\n\n104\nA\n12\n\n\n104\nHHC\n3\n\n\n124\nA\n42\n\n\n144\nA\n30\n\n\n299\nA\n39\n\n\n299\nB\n30\n\n\n299\nC\n27\n\n\n\n\n\nCode\n# N.B. The same result could be obtained with count(BTN, COMPANY) alone.\n#  This combines the group_by and tally functions"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#select-choose-variables-columns-based-on-some-criteria",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#select-choose-variables-columns-based-on-some-criteria",
    "title": "Data wrangling in dplyr",
    "section": "select: choose variables (columns) based on some criteria",
    "text": "select: choose variables (columns) based on some criteria\nLet’s start by keeping only the three core dependent variables over time: jobsat, commit, ready. Keep SUBNUM as well for unique identification.\n\n\nCode\ndvs_only &lt;- univbct %&gt;% \n  dplyr::select(SUBNUM, JOBSAT1, JOBSAT2, JOBSAT3, \n                COMMIT1, COMMIT2, COMMIT3, \n                READY1, READY2, READY3)\n\n\nIf you have many variables of a similar name, you might try starts_with(). Note in this case that it brings in “READY”, too. Note that you can mix different selection mechanisms within select. Look at the cheatsheet.\n\n\nCode\ndvs_only &lt;- univbct %&gt;% \n  dplyr::select(SUBNUM, starts_with(\"JOBSAT\"), starts_with(\"COMMIT\"), starts_with(\"READY\"))\n\n\nOther selection mechanisms:\n\ncontains: variable name contains a literal string\nstarts_with: variable names start with a string\nends_with: variable names end with a string\nnum_range: variables that have a common prefix (e.g., ‘reasoning’) and a numeric range (e.g., 1-20)\nmatches: variable name matches a regular expression\none_of: variable is one of the elements in a character vector. Example: select(one_of(c(“A”, “B”)))\n\nSee ?select_helpers for more details."
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#select-filter-zooming-in-on-specific-observations-and-variables",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#select-filter-zooming-in-on-specific-observations-and-variables",
    "title": "Data wrangling in dplyr",
    "section": "select + filter: zooming in on specific observations and variables",
    "text": "select + filter: zooming in on specific observations and variables\nNote that select and filter can be combined to subset both observations and variables of interest.\nFor example, look at readiness to deploy in battalion 299 only:\n\n\nCode\nunivbct %&gt;% \n  filter(BTN==299) %&gt;% \n  dplyr::select(SUBNUM, READY1, READY2, READY3) %&gt;% \n  kable_table(n=6)\n\n\n\n\n\n\nSUBNUM\nREADY1\nREADY2\nREADY3\n\n\n\n\n10\n4\n2.5\n3.2\n3.0\n\n\n11\n4\n2.5\n3.2\n3.0\n\n\n12\n4\n2.5\n3.2\n3.0\n\n\n19\n7\n2.0\n1.8\n1.2\n\n\n20\n7\n2.0\n1.8\n1.2\n\n\n21\n7\n2.0\n1.8\n1.2\n\n\n\n\n\nselect is also useful for dropping variables that are not of interest using a kind of subtraction syntax.\n\n\nCode\nnojobsat &lt;- univbct %&gt;% \n  dplyr::select(-starts_with(\"JOBSAT\"))\nnames(nojobsat)\n\n\n [1] \"BTN\"     \"COMPANY\" \"MARITAL\" \"GENDER\"  \"HOWLONG\" \"RANK\"    \"EDUCATE\"\n [8] \"AGE\"     \"COMMIT1\" \"READY1\"  \"COMMIT2\" \"READY2\"  \"COMMIT3\" \"READY3\" \n[15] \"TIME\"    \"JSAT\"    \"COMMIT\"  \"READY\"   \"SUBNUM\""
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#mutate-add-one-or-more-variables-that-are-a-function-of-other-variables",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#mutate-add-one-or-more-variables-that-are-a-function-of-other-variables",
    "title": "Data wrangling in dplyr",
    "section": "mutate: add one or more variables that are a function of other variables",
    "text": "mutate: add one or more variables that are a function of other variables\n(Row-wise) mean of commit scores over waves. Note how you can used select() within a mutate to run a function on a subset of the data.\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  mutate(commitmean=rowMeans(dplyr::select(., COMMIT1, COMMIT2, COMMIT3)))\n\n\nMutate can manipulate several variables in one call. Here, mean center any variable that starts with COMMIT and add the suffix _cm for clarity. Also compute the percentile rank for each of these columns, with _pct as suffix. Note the use of the starts_with function here within the across(). This operates identically to select, but in the context of a summary or mutation operation on specific variables. See ?select_helpers for details.\n\n\nCode\nmeancent &lt;- function(x) { x - mean(x, na.rm=TRUE) } #simple worker function to mean center a variable\n\nunivbct &lt;- univbct %&gt;% \n  mutate(across(starts_with(\"COMMIT\", ignore.case = FALSE), list(cm=meancent, pct=percent_rank)))\n\nunivbct %&gt;%\n  dplyr::select(starts_with(\"COMMIT\", ignore.case = FALSE)) %&gt;%\n  kable_table(n=8) %&gt;% kable_styling(font_size = 12)\n\n\n\n\n\nCOMMIT1\nCOMMIT2\nCOMMIT3\nCOMMIT\nCOMMIT1_cm\nCOMMIT1_pct\nCOMMIT2_cm\nCOMMIT2_pct\nCOMMIT3_cm\nCOMMIT3_pct\nCOMMIT_cm\nCOMMIT_pct\n\n\n\n\n1.7\n1.7\n3.0\n1.7\n-1.95\n0.01\n-1.80\n0.04\n-0.54\n0.12\n-1.87\n0.02\n\n\n1.7\n1.7\n3.0\n1.7\n-1.95\n0.01\n-1.80\n0.04\n-0.54\n0.12\n-1.87\n0.02\n\n\n1.7\n1.7\n3.0\n3.0\n-1.95\n0.01\n-1.80\n0.04\n-0.54\n0.12\n-0.54\n0.15\n\n\n1.7\n1.3\n1.3\n1.7\n-1.95\n0.01\n-2.13\n0.03\n-2.20\n0.01\n-1.87\n0.02\n\n\n1.7\n1.3\n1.3\n1.3\n-1.95\n0.01\n-2.13\n0.03\n-2.20\n0.01\n-2.21\n0.01\n\n\n1.7\n1.3\n1.3\n1.3\n-1.95\n0.01\n-2.13\n0.03\n-2.20\n0.01\n-2.21\n0.01\n\n\n3.3\n3.3\n3.7\n3.3\n-0.28\n0.27\n-0.13\n0.33\n0.13\n0.45\n-0.21\n0.30\n\n\n3.3\n3.3\n3.7\n3.3\n-0.28\n0.27\n-0.13\n0.33\n0.13\n0.45\n-0.21\n0.30"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#arrange-reorder-observations-in-specific-order",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#arrange-reorder-observations-in-specific-order",
    "title": "Data wrangling in dplyr",
    "section": "arrange: reorder observations in specific order",
    "text": "arrange: reorder observations in specific order\nOrder data by ascending battalion, company, then subnum\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  arrange(BTN, COMPANY, SUBNUM)\n\n\nDescending sort: descending battalion, ascending company, ascending subnum\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  arrange(desc(BTN), COMPANY, SUBNUM)"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#a-more-realistic-example-preparation-for-multilevel-analysis",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#a-more-realistic-example-preparation-for-multilevel-analysis",
    "title": "Data wrangling in dplyr",
    "section": "A more realistic example: preparation for multilevel analysis",
    "text": "A more realistic example: preparation for multilevel analysis\nIn MLM, one strategy for disentangling within- versus between-person effects is to include both within-person-centered variables and person means in the model (Curran & Bauer, 2011).\nWe can achieve this easily for our three DVs here using a single pipeline that combines tidying and mutation. Using -1 as the sep argument to separate splits the string at the second-to-last position (i.e., starting at the right).\nFor reshaping to work smoothly, we need a unique identifier for each row. Also, univbct is stored in a dangerously untidy format in which variables with suffix 1-3 indicate a ‘wide format’, but the data is also in long format under variables such as ‘JSAT’ and ‘COMMIT.’ In other words, there is a peculiar redundancy in the data that is altogether confusing.\nTake a look:\n\n\nCode\nunivbct %&gt;%\n  dplyr::select(SUBNUM, starts_with(\"JOBSAT\"), JSAT) %&gt;% \n  kable_table(n=12)\n\n\n\n\n\n\nSUBNUM\nJOBSAT1\nJOBSAT2\nJOBSAT3\nJSAT\n\n\n\n\n319\n103\n2.0\n2.3\n3.3\n2.0\n\n\n320\n103\n2.0\n2.3\n3.3\n2.3\n\n\n321\n103\n2.0\n2.3\n3.3\n3.3\n\n\n397\n129\n3.7\n4.3\n4.7\n3.7\n\n\n398\n129\n3.7\n4.3\n4.7\n4.3\n\n\n399\n129\n3.7\n4.3\n4.7\n4.7\n\n\n523\n171\n3.7\n4.0\nNA\n3.7\n\n\n524\n171\n3.7\n4.0\nNA\n4.0\n\n\n525\n171\n3.7\n4.0\nNA\nNA\n\n\n616\n202\n1.3\n2.0\n4.3\n1.3\n\n\n617\n202\n1.3\n2.0\n4.3\n2.0\n\n\n618\n202\n1.3\n2.0\n4.3\n4.3\n\n\n\n\n\nWe first need to eliminate this insanity. Group by subject number and retain only the first row (i.e., keep the wide version).\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  group_by(SUBNUM) %&gt;% # split into separate groups for each subject\n  filter(row_number() == 1) %&gt;% # only retain the first row of each subject\n  dplyr::select(-JSAT, -COMMIT, -READY) %&gt;% # drop redundant columns\n  ungroup() # remove grouping from data structure (we are done with group-based wrangling)\n\n\nFirst, let’s get the data into a conventional format (long) for MLM (e.g., using lmer)\n\n\nCode\nforMLM &lt;- univbct %&gt;% \n  dplyr::select(SUBNUM, JOBSAT1, JOBSAT2, JOBSAT3, \n                COMMIT1, COMMIT2, COMMIT3, \n                READY1, READY2, READY3) %&gt;% \n  \n  # pivot everything but SUBNUM\n  pivot_longer(names_to = \"key\", values_to = \"value\", cols=-SUBNUM) %&gt;%\n  \n  # -1 splits at the last character of the variable name\n  separate(col=\"key\", into=c(\"variable\", \"occasion\"), -1, convert=TRUE) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) #%&gt;% \n  #mutate(occasion=as.integer(occasion))\n\n\nNow, let’s perform the centering described above. You could do this in one pipeline – I just separated things here for conceptual clarity.\n\n\nCode\nforMLM &lt;- forMLM %&gt;% group_by(SUBNUM) %&gt;% \n  mutate(across(c(COMMIT, JOBSAT, READY), list(wic=meancent, pm=mean))) %&gt;%\n  ungroup()\n\nforMLM %&gt;% kable_table(n=10) %&gt;% kable_styling(font_size = 14)\n\n\n\n\n\nSUBNUM\noccasion\nJOBSAT\nCOMMIT\nREADY\nCOMMIT_wic\nCOMMIT_pm\nJOBSAT_wic\nJOBSAT_pm\nREADY_wic\nREADY_pm\n\n\n\n\n103\n1\n2.0\n3.7\n4.0\n0.00\n3.7\n-0.56\n2.6\n1.25\n2.8\n\n\n103\n2\n2.3\n3.7\n2.0\n0.00\n3.7\n-0.22\n2.6\n-0.75\n2.8\n\n\n103\n3\n3.3\n3.7\n2.2\n0.00\n3.7\n0.78\n2.6\n-0.50\n2.8\n\n\n129\n1\n3.7\n5.0\n2.5\n0.44\n4.6\n-0.56\n4.2\n-0.33\n2.8\n\n\n129\n2\n4.3\n4.3\n2.8\n-0.22\n4.6\n0.11\n4.2\n-0.08\n2.8\n\n\n129\n3\n4.7\n4.3\n3.2\n-0.22\n4.6\n0.44\n4.2\n0.42\n2.8\n\n\n171\n1\n3.7\n4.0\n3.2\n-0.17\nNA\n-0.17\nNA\n-0.12\nNA\n\n\n171\n2\n4.0\n4.3\n3.5\n0.17\nNA\n0.17\nNA\n0.12\nNA\n\n\n171\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n202\n1\n1.3\n3.3\n2.5\n0.44\n2.9\n-1.22\n2.6\n-0.75\n3.2"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "This page contains course materials used during the demonstration and practical exercises.\n\nWeek 1\n\nTidy data conceptual walkthrough (QMD)\n\n\n\nWeek 2\n\nData wrangling in dplyr (QMD)\nJoins tutorial (QMD)\nStrings in R with stringr (QMD)\n\n\n\nWeek 3\n\nCompact data validation with validate (Psychology Survey Example) (QMD)\nECG physio processing script (Data)\n\n\n\nWeek 4\n\nFunctional programming and reproducibility (QMD)"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html",
    "href": "assignments/week2_workflowr_setup.html",
    "title": "Week 2: Workflowr Project Setup",
    "section": "",
    "text": "This assignment is due by 2/4/2026 at 8am."
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#overview",
    "href": "assignments/week2_workflowr_setup.html#overview",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Overview",
    "text": "Overview\nThis assignment builds directly on the workflowr vignette: https://workflowr.github.io/workflowr/articles/wflow-01-getting-started.html. Your goal is to create a new workflowr project with a clean structure, publish at least one analysis file, and make it available as a GitHub Pages site.\nI don’t expect that you necessarily use workflowr in the class or more generally, but I think it is useful to see how the package takes a structured approach to:\n\nSetting up a predictable project structure\nUses Git to support version control and reproducibility\nAllows you to publish your reports easily at GitHub Pages accessible to others."
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#learning-goals",
    "href": "assignments/week2_workflowr_setup.html#learning-goals",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Learning goals",
    "text": "Learning goals\n\nInitialize a workflowr project with a reproducible structure\nKnit analysis files with workflowr and track the build status\nPublish a workflowr site to GitHub Pages"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#setup-requirements",
    "href": "assignments/week2_workflowr_setup.html#setup-requirements",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Setup requirements",
    "text": "Setup requirements\nBefore starting, make sure you have:\n\nR and RStudio installed\nGit installed and configured (For help, see: https://happygitwithr.com)\nA GitHub account\nThe workflowr package installed\n\n\n\nCode\n# Install once if needed\ninstall.packages(\"workflowr\")"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#tasks",
    "href": "assignments/week2_workflowr_setup.html#tasks",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Tasks",
    "text": "Tasks\n\n1. Start a new workflowr project\nCreate a new project named something like workflowr-&lt;your-name&gt; in a location you can find later. The wflow_start command will setup a new workflowr project in the speecified directory (by default, it does not overwrite existing projects).\n\n\nCode\nlibrary(workflowr) # load library\nwflow_start(\"~/Downloads/workflowr-michael_28jan2026\")\n\n\nOpen the project in RStudio (File &gt; Open Project) and review the default folders: analysis/, data/, code/, output/, and docs/.\n\n\n2. Create a new analysis file\nCreate a new analysis Rmarkdown file at analysis/01-getting-started.Rmd. You can use wflow_open if you wish, or just File &gt; New File &gt; R Markdown.\nIn the Rmd file, include the following:\n\nA short paragraph describing the project topic you plan to use this semester\nAdd a small public domain data file to the data/ folder (e.g., a CSV from data.gov or another public domain source).\nOne code chunk that reads your data file from data/ and prints a small summary (e.g., str() or summary())\n\nNext, add this new analysis page to your site index so it appears on the rendered site. Open analysis/index.Rmd and add a link under the Analysis section. Use a relative link to the rendered HTML file:\n- [Getting started](01-getting-started.html)\n\n\n3. Build and check the site locally\nUse workflowr to build the site and check its status.\n\n\nCode\n# wflow_open(\"analysis/01-getting-started.Rmd\")\nwflow_build(\"analysis/01-getting-started.Rmd\") # build only one script\nwflow_build() # build the whole site\nwflow_status() # check the build status\n\n\n\n\n4. Publish and commit with workflowr\nBefore publishing your own analysis file, publish the default site pages once so they are tracked by Git and appear on your site (these files are created by wflow_start()):\n\n\nCode\nwflow_publish(c(\"analysis/index.Rmd\", \"analysis/about.Rmd\", \"analysis/license.Rmd\"),\n              message = \"Publish initial site pages\")\n\n\nNow publish the new analysis page using workflowr so it is tracked by Git.\n\n\nCode\nwflow_publish(\"analysis/01-getting-started.Rmd\", message = \"update\") # committing your work to the repository\n\n\n\n\n5. Push to GitHub and enable GitHub Pages\nAfter publishing (basically concretizing a change), connect the project to GitHub with your username:\nSetup your Github repository using the wflow_use_github function. I chose the Oauth route when prompted, but either is fine.\n\n\nCode\nwflow_use_github(\"michaelhallquist\")\n\n\n\nCreate a Personal Access Token (PAT) for GitHub (required for pushing over HTTPS)\nGitHub no longer accepts your GitHub account password for pushing over HTTPS. Instead, you will create a Personal Access Token (PAT) and use it in place of a password when wflow_git_push() asks.\n\nLog into GitHub in your browser.\nGo to Settings → Developer settings → Personal access tokens.\nChoose Tokens (classic) (recommended). If you prefer Fine-grained tokens, choose that instead and follow the fine-grained settings below.\nClassic token settings: check repo and workflow, and set an Expiration (e.g., 90 days).\nFine-grained token settings: give it a name (e.g., workflowr-week2) and set an Expiration (e.g., 30–90 days). Under Repository access, choose Only select repositories and select the workflowr repo. Under Repository permissions, add Actions, Contents, and Pages; for each one, set Access to Read and write in the right-hand dropdown.\nClick Generate token and copy it somewhere safe. You will not be able to see it again.\n\nTreat the PAT like a password: don’t share it and don’t paste it into files that you will commit to GitHub.\n\n\nPush the code from your computer to GitHub\n\n\nCode\nwflow_git_push(set_upstream = TRUE)\n\n\nWhen prompted:\n\nUsername: your GitHub username\nPassword: paste your PAT (not your GitHub account password)\n\nIf wflow_git_push() fails or gives strange errors, see the Troubleshooting section below for a command-line fallback (you will still use your PAT when asked for a password).\nIn GitHub, enable Pages for the docs/ folder on the branch you pushed (often main, but sometimes master).\nGuidance here: https://docs.github.com/en/pages/quickstart\nAfter configured, this should look like the following: \nAfter a few minutes of setting this, the top part of the GitHub Pages for the repo (https://github.com/&lt;username&gt;/&lt;reponame&gt;/settings/pages) should look something like this:\n\n\n\n\n6. Verify the published site\nOpen your GitHub Pages site and confirm:\n\nThe home page loads\nYour analysis page appears in the navigation\nThe analysis page renders your text and code output\n\n\n\nDeliverables\n\nProvide the URL of your project site.\nShort reflection (2-3 sentences) on what was straightforward vs. confusing, what you think may be helpful about this approach and/or what aspects you may adopt in your own workflows"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#troubleshooting",
    "href": "assignments/week2_workflowr_setup.html#troubleshooting",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nAuthentication setup (default: PAT over HTTPS)\nGitHub no longer accepts account passwords for Git pushes over HTTPS. You must use either:\n\nA Personal Access Token (PAT) over HTTPS (default for this assignment)\nAn SSH key (alternative; great if you plan to use GitHub often)\n\nBoth options are secure. Unless you already use SSH for GitHub, use the PAT option below.\n\nOption A (default): Personal Access Token (PAT) over HTTPS\nStep 1: Create a PAT in GitHub\n\nLog into GitHub in your browser.\nGo to Settings → Developer settings → Personal access tokens.\nChoose Tokens (classic) (recommended). If you prefer Fine-grained tokens, choose that instead.\nClassic token settings: check repo and workflow, and set an Expiration (e.g., 90 days).\nFine-grained token settings: give it a name and set an Expiration (e.g., 30–90 days). Repository access: choose Only select repositories and pick your workflowr repo. Repository permissions: add Actions, Contents, and Pages; for each one, set Access to Read and write in the right-hand dropdown.\nClick Generate token and copy it. You will not see it again.\n\nStep 2: Push from R with workflowr (preferred)\nRun this from your R session:\n\n\nCode\nwflow_git_push(set_upstream = TRUE)\n\n\nWhen prompted: - Username: your GitHub username - Password: paste your PAT\nIf wflow_git_push() isn’t working (or gives confusing errors), try the command line instead:\n\n\nCode\ngit push -u origin main\n\n\nIf your default branch is master, replace main with master. If asked whether to save the credential, say Yes (recommended).\n\n\nOption B (alternative): SSH key\nStep 1: Generate an SSH key\n\n\nCode\nssh-keygen -t ed25519 -C \"youremail@unc.edu\"\n\n\nPress Enter to accept the default file location. Add a passphrase if you want extra security.\nStep 2: Add your key to the SSH agent\n\n\nCode\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n\nStep 3: Add the public key to GitHub\n\n\nCode\ncat ~/.ssh/id_ed25519.pub\n\n\nCopy the output. Then in GitHub: Settings → SSH and GPG keys → New SSH key → paste the key → Add.\nStep 4: Switch your repo remote to SSH\n\n\nCode\ngit remote set-url origin git@github.com:&lt;username&gt;/&lt;reponame&gt;.git\n\n\nStep 5: Test and push\n\n\nCode\nssh -T git@github.com\n\n\nThen push from R (preferred):\n\n\nCode\nwflow_git_push(set_upstream = TRUE)\n\n\nIf that fails, push from the command line:\n\n\nCode\ngit push -u origin main\n\n\nIf your default branch is master, replace main with master.\n\n\nUsing PAT or SSH without the command line (workflowr only)\nIf you want to work entirely in R with workflowr (no terminal commands), you can still authenticate:\nPAT (HTTPS): Run wflow_git_push() from R. When it asks for a password, paste your PAT.\nIf you want to avoid re-typing it each time, you can store the PAT in your .Renviron file as GITHUB_PAT, restart R, then push like this:\n\n\nCode\nwflow_git_push(username = \"&lt;github-username&gt;\",\n               password = Sys.getenv(\"GITHUB_PAT\"))\n\n\nSSH: Switch the remote to SSH using workflowr, then push:\n\n\nCode\nwflow_git_remote(remote = \"origin\", user = \"&lt;username&gt;\", repo = \"&lt;reponame&gt;\",\n                 protocol = \"ssh\", action = \"set_url\")\nwflow_git_push()\n\n\nIf SSH fails with a message about SSH support, use HTTPS + PAT or the command line (git push) instead.\n\n\n\nwflow_git_push() fails with a git2r::push() error\nSome students see errors like:\nError in 'git2r_push': unexpected http status code: 400\nThis is usually an authentication problem (HTTPS + no password support). Two fixes:\n\nUse GitHub’s Personal Access Token (PAT) when prompted for a password.\nOr, bypass git2r entirely and push with the Git CLI (still using a PAT if you are on HTTPS):\n\n\n\nCode\ngit push -u origin master\n\n\nIf your default branch is main, replace master with main. If you don’t have Git installed, install it first (https://happygitwithr.com). If you want advice on the git2r error, search the git2r GitHub issues for “unexpected http status code 400”.\n\n\ngit2r::push() fails with HTTP 403 (but git push works)\nIf you are using a PAT and see:\nError in 'git2r_push': unexpected http status code: 403\nIt usually means the token you provided doesn’t have permission to push or git2r is not using the same stored credential that your Git command line is using.\nTry these in order:\n\nDouble-check your PAT settings (it must have access to the repo and Contents: Read and write; if you used a fine-grained token, also include Actions and Pages).\nCreate a new PAT and try again (copy/paste carefully; no spaces).\nUse the Git CLI for the first push and set the upstream branch once:\n\n\n\nCode\ngit push -u origin HEAD\n\n\nAfter that, git push should work without needing -u again for that repo/branch.\n\n\nMy analysis file isn’t showing up on GitHub Pages\nIf the file exists in analysis/ but doesn’t appear on your GitHub Pages site, run this checklist:\n\nBuild it: Run wflow_build() (or wflow_publish() which builds + commits).\nPublish it: If you only built it, run wflow_publish(\"analysis/your-file.Rmd\").\nCheck status: Run wflow_status() and confirm the new HTML is tracked.\nPush it: Push the latest commit to GitHub.\nPages source: In GitHub → Settings → Pages, confirm main + /docs.\nWait + refresh: Give it a few minutes, then hard refresh your browser.\n\n\n\nRecommended workflow to avoid push problems\nThis workflow keeps you in R for the main steps (default: HTTPS + PAT):\n\nCreate your project with wflow_start().\nPublish changes with wflow_publish().\nPush with workflowr:\n\n\n\nCode\nwflow_git_push(set_upstream = TRUE)\n\n\nIf wflow_git_push() isn’t working, use the Git CLI instead (still using your PAT or SSH):\n\n\nCode\ngit status\ngit push -u origin master\n\n\nIf you prefer SSH, set up an SSH key on GitHub first, then ensure your remote is SSH:\n\n\nCode\ngit remote -v\ngit remote set-url origin git@github.com:&lt;username&gt;/&lt;reponame&gt;.git"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Week 1: Data Tidying (due 1/21 at 8am) — Download QMD\nWeek 2: Workflowr Project Setup (due 1/28 at 8am) — Download QMD\nData quality assurance and processing project proposal (due 1/29/2026)"
  },
  {
    "objectID": "assignments/week1_tidyData.html",
    "href": "assignments/week1_tidyData.html",
    "title": "Week 1: Data Tidying",
    "section": "",
    "text": "This assignment is due by 1/21/2026 at 8am."
  },
  {
    "objectID": "assignments/week1_tidyData.html#instructions",
    "href": "assignments/week1_tidyData.html#instructions",
    "title": "Week 1: Data Tidying",
    "section": "Instructions",
    "text": "Instructions\nFor each question either write the code you would use or copy and paste it from the RStudio syntax window. Additionally, paste any (reasonable) output generated by the code. If there is a lot of output, paste enough to see that you were able to get the correct answer."
  },
  {
    "objectID": "assignments/week1_tidyData.html#datasets-for-vignettes",
    "href": "assignments/week1_tidyData.html#datasets-for-vignettes",
    "title": "Week 1: Data Tidying",
    "section": "Datasets for Vignettes",
    "text": "Datasets for Vignettes\nLoad the who2 dataset from the tidyr package into your working environment.\n\n\nCode\ndata(\"who2\", package = \"tidyr\")\n\n\nThis dataset records counts of tuberculosis by country and year. The other values correspond to a method of diagnosis, sex, and age group. The method of diagnosis codes are: rel = relapse, sn = negative pulmonary smear, sp = positive pulmonary smear, ep = extrapulmonary. For example, sp_m_014 corresponds to positive pulmonary smear (sp), male sex (m), and ages 0-14 (014).\nYou will see many NA values in who2 because some country-year combinations do not report counts for certain diagnosis/sex/age groups. When you summarize totals, remember to use na.rm = TRUE so missing values do not turn your results into NA.\nAdditionally, load the Pew relig_income dataset from the tidyr package:\n\n\nCode\ndata(\"relig_income\", package = \"tidyr\")\n\n\nThis dataset describes the relationship between income and religion."
  },
  {
    "objectID": "assignments/week1_tidyData.html#basic-pew-dataset-structure",
    "href": "assignments/week1_tidyData.html#basic-pew-dataset-structure",
    "title": "Week 1: Data Tidying",
    "section": "Basic Pew Dataset Structure",
    "text": "Basic Pew Dataset Structure\nUsing the Pew dataset\n\nLook at the first and last five observations of the Pew dataset using head() and tail(). Is the dataset considered tidy? Why or why not?\n\n\nLook at the structure (str()) and class (class()) of the data. If this is not tidy, verbally describe how it would look if it were a tidy dataset. If it is tidy, is it in a format that you would store the data in long-term?"
  },
  {
    "objectID": "assignments/week1_tidyData.html#tidying-pew-data",
    "href": "assignments/week1_tidyData.html#tidying-pew-data",
    "title": "Week 1: Data Tidying",
    "section": "Tidying Pew Data",
    "text": "Tidying Pew Data\n\nUse tidyr verbs to tidy the Pew dataset. Your tidy dataset should have columns named religion, income, and count. Show the first five rows."
  },
  {
    "objectID": "assignments/week1_tidyData.html#tidying-tuberculosis-data",
    "href": "assignments/week1_tidyData.html#tidying-tuberculosis-data",
    "title": "Week 1: Data Tidying",
    "section": "Tidying Tuberculosis Data",
    "text": "Tidying Tuberculosis Data\nUsing the who2 dataset\n\nExplore the who2 dataset using the methods described in the section above.\n\n\nUsing functions from the tidyr package, tidy the who2 dataset so there is a case_group variable and a cases variable. Use those names throughout the rest of the assignment. Show the first five rows of your new dataframe by using head(). Remember, a tidy dataset consists of:\n\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\n\nThe values within the case_group variable are still considered not tidy because they represent three observations in one (case type, sex, and age). Use tidyr verbs to separate this variable into type, sex, and age (hint: the values are separated by _).\n\n\nCheck that each row is a unique observation by counting duplicates of country, year, type, sex, and age. Report whether any combinations appear more than once.\n\n\nRename the values within sex and age to be more descriptive of what they represent (male or female, 0-4, 5-14, etc.). You can use a combination of dplyr::mutate() and dplyr::recode() to recode the values. Use ?recode if you get stuck.\n\n\nTake your new tidy dataframe with the recoded values and variables and demonstrate tidyr::unite() by recombining sex and age into a single variable (e.g., sex_age). This is just to practice unite() before you mess it up again in the next step. Show the first five rows of your new dataframe.\n\n\nGo back to the dataframe you created in question two. Using that tidy dataframe, use tidyr verbs to recreate a ‘messy’ dataframe. It should look exactly the same (or similar) as when you first loaded the who2 dataset into your environment. Show the first five rows of your new dataframe. If the code throws a ‘duplicate identifiers error’ you should use dplyr::distinct() in your pipeline.\n\n\nArrange the dataset to be descending by most cases of tuberculosis to least using dplyr verbs (use your cases variable).\n\n\nSummarize how many cases of tuberculosis there are by sex and age (use your cases variable). Show your output. If you get NA values for everything, remember to remove those empty values somewhere in your pipeline! Hint Look at the default arguments for the functions you use to summarize your data.\n\n\nFinally, summarize the dataset by country and sex, then add a variable that is the relative prevalence by sex in each country."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 859",
    "section": "",
    "text": "Welcome to PSYC 859, Data Management and Visualization, taught at UNC by Michael Hallquist. This website provides access to lectures, labs, and other course materials for the Spring 2026 session.\nUse the Syllabus and Materials tabs to navigate the course schedule and resources.\n\nCourse description\nThis graduate course is intended to provide an applied introduction to data management and data visualization in the social sciences. In order to take full advantage of modern statistical methods (e.g., structural equation models), competency in data management, semi-automated processing, and data wrangling is prerequisite. Likewise, prior to employing inferential statistics, exploratory visualization and analysis is essential to facilitate data cleaning and to form an initial understanding of patterns in the data. This course will cover both the principles and practice of data management, visualization, and exploratory analysis for summarizing quantitative data. In addition, students will learn data science skills to manage and visualize “big data,” where the size or complexity of the dataset defies traditional techniques.\nApplications of data management, visualization, and analysis will use the R statistical programming language. R is quickly becoming the lingua franca in data science across disciplines and offers unparalleled tools for data analysis and visualization.\n\n\nSchedule overview\n\n1/8 (Week 1): Introduction to data management and tidy data\n1/15 (Week 2): Data aggregation, manipulation, joins\n1/22 (Week 3): Data processing and quality assurance, custom functions, basics of automation\n1/29 (Week 4): Advanced data manipulation and management, tracking work in R markdown\n2/5 (Week 5): Principles of data visualization and graphical grammar\n2/12 (Week 6): Visual and graphical perception\n2/19 (Week 7): Graphic design, layout, style, use of color\n2/26 (Week 8): A tour of quantitative visualization\n3/5 (Week 9): Visualizing continuous data (in ggplot2)\n3/12 (Week 10): Visualizing count and categorical data (in ggplot2)\n3/19: No class (Spring break)\n3/26 (Week 11): Maximizing clarity: preparing graphics for presentation and publication\n4/2: No class (Well-being day)\n4/9 (Week 12): Visualizing and understanding fit (and misfit) of statistical models\n4/16 (Week 13): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction\n4/23 (Week 14): Final presentations of data projects\n\n\n\nObtaining course materials\nTo obtain the full materials for this class, use git clone to download the course repository:\ngit clone https://github.com/michaelhallquist/dataviz_spr2026.git\nIf you already cloned a local copy of the repo, you can get the latest updates using git pull. If all of this git stuff is foreign, I would recommend a quick skim of this documentation: https://happygitwithr.com."
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html",
    "title": "Tidy data overview",
    "section": "",
    "text": "Most data wrangling can be accomplished using data.frame object (or tbl objects in dplyr). These objects consist of rows and columns, forming a rectangular structure.\n\n\nCode\ngapminder %&gt;% kable_table(n=6)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\nA variable contains all values measuring an attribute (e.g., neuroticism) across units (e.g., people).\nColumns in data.frames are typically labeled and represent variables.\nMoreover, in a data.frame, all values in a given column should have the same data type, such as character strings. But a data.frame is different from a matrix object because columns can differ in terms of data type. For example, in the gapminder, the country column is a factor, whereas lifeExp is a numeric column.\n“A data frame is a list of vectors that R displays as a table. When your data is tidy, the values of each variable fall in their own column vector.” -Garrett Grolemund Citation\n\n\n\nAn observation contains all values measured on the same unit across attributes.\nRows in a data.frame typically represent observations.\n\n\nCode\ngapminder %&gt;% kable_table(n=1)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#variables",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#variables",
    "title": "Tidy data overview",
    "section": "",
    "text": "A variable contains all values measuring an attribute (e.g., neuroticism) across units (e.g., people).\nColumns in data.frames are typically labeled and represent variables.\nMoreover, in a data.frame, all values in a given column should have the same data type, such as character strings. But a data.frame is different from a matrix object because columns can differ in terms of data type. For example, in the gapminder, the country column is a factor, whereas lifeExp is a numeric column.\n“A data frame is a list of vectors that R displays as a table. When your data is tidy, the values of each variable fall in their own column vector.” -Garrett Grolemund Citation"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#observations",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#observations",
    "title": "Tidy data overview",
    "section": "",
    "text": "An observation contains all values measured on the same unit across attributes.\nRows in a data.frame typically represent observations.\n\n\nCode\ngapminder %&gt;% kable_table(n=1)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#tidying-verbs",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#tidying-verbs",
    "title": "Tidy data overview",
    "section": "Tidying verbs",
    "text": "Tidying verbs\nThe tidyr package provides four core functions to aid in converting messy data into tidy form. We may also need functions from dplyr at times. Each of these verbs is also a function that transforms the dataset with the goal of making it more tidy.\n\nPivot_longer: combine multiple columns into a single column with a key-value pair format\nPivot_wider: divide key-value rows into multiple columns\nSeparate: split a single variable into multiple variables by pulling apart the values into pieces\nUnite: merge two variables (columns) into one, effectively pasting together the values\n\nNote: pivot_longer and pivot_wider are complements. And separate and unite are complements.\nDetails about the pivot functions can be found here: https://tidyr.tidyverse.org/articles/pivot.html.\nLet’s look at a series of datasets (from Wickham 2014) and consider how tidy or messy they are."
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_longer-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_longer-example",
    "title": "Tidy data overview",
    "section": "pivot_longer example",
    "text": "pivot_longer example\nHere is our first mess. Notice that the column headers are values, not variable names. This is untidy and hard to look at. We effectively have the data in a cross-tabulated format, but religion and income are not variables in the dataset.\n\nMessy version\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n\n\n\n\nAgnostic\n27\n34\n60\n81\n76\n137\n\n\nAtheist\n12\n27\n37\n52\n35\n70\n\n\nBuddhist\n27\n21\n30\n34\n33\n58\n\n\nCatholic\n418\n617\n732\n670\n638\n1116\n\n\nDon’t know/refused\n15\n14\n15\n11\n10\n35\n\n\nEvangelical Prot\n575\n869\n1064\n982\n881\n1486\n\n\nHindu\n1\n9\n7\n9\n11\n34\n\n\nHistorically Black Prot\n228\n244\n236\n238\n197\n223\n\n\nJehovah's Witness\n20\n27\n24\n24\n21\n30\n\n\nJewish\n19\n19\n25\n25\n30\n95\n\n\n\n\n\n\n\nTidy version\nIn the tidy version, religion and income become variables, and the number of observations in each religion x income combination is a frequency column. This is now tidy insofar as each value in the frequency column represents a unique combination of the religion and income factors, which are coded as variables.\n\n\n\n\n\nreligion\nincome\nfreq\n\n\n\n\nAgnostic\n$10-20k\n34\n\n\nAgnostic\n$100-150k\n109\n\n\nAgnostic\n$20-30k\n60\n\n\nAgnostic\n$30-40k\n81\n\n\nAgnostic\n$40-50k\n76\n\n\nAgnostic\n$50-75k\n137\n\n\nAgnostic\n$75-100k\n122\n\n\nAgnostic\n&lt;$10k\n27\n\n\nAgnostic\n&gt;150k\n84\n\n\nAgnostic\nDon't know/refused\n96\n\n\n\n\n\n\n\nTidying solution\nTo achieve the above transformation, we want to pivot_longer the many columns of income into a single income column.\ntidy1 &lt;- mess1 %&gt;% pivot_longer(cols=-religion, names_to=\"income\", values_to=\"freq\")\nHere, we tell tidyr that we wish to create a lookup (‘key’) column called income whose correponding values will be called freq (here, representing the frequency of this religion x income combination). Furthermore, as additional arguments to pivot_longer, we provide the columns (cols argument) that should be combined, representing levels of the key variable. By specifying -religion, we are saying ‘all columns except religion.’ The alternative would be to provide a comma-separate list of columns like this mess1 %&gt;% pivot_longer(names_to=\"income\", values_to=\"freq\", cols=c(\"&lt;$10k\", \"$10-20k\", etc.))"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_wider-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_wider-example",
    "title": "Tidy data overview",
    "section": "pivot_wider example",
    "text": "pivot_wider example\nIn our second mess, we have a weather dataset from the Global Historical Climatology Network for one weather station (MX17004) in Mexico. The data represent minimum and maximum temperatures measured across 31 days for five months. The days within each month are on the columns, the months are encoded as a variable month, and the min and max temperatures are separated by row, as identified by the element variable.\n\nMessy version\n\n\nCode\n#show a subset of columns that fit on the page\nmess2 %&gt;% dplyr::select(id:d13) %&gt;% kable_table(n=8)\n\n\n\n\n\nid\nyear\nmonth\nelement\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\nd11\nd12\nd13\n\n\n\n\nMX17004\n2010\n1\ntmax\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n1\ntmin\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n2\ntmax\nNA\n27.3\n24.1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n29.7\nNA\nNA\n\n\nMX17004\n2010\n2\ntmin\nNA\n14.4\n14.4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n13.4\nNA\nNA\n\n\nMX17004\n2010\n3\ntmax\nNA\nNA\nNA\nNA\n32.1\nNA\nNA\nNA\nNA\n34.5\nNA\nNA\nNA\n\n\nMX17004\n2010\n3\ntmin\nNA\nNA\nNA\nNA\n14.2\nNA\nNA\nNA\nNA\n16.8\nNA\nNA\nNA\n\n\nMX17004\n2010\n4\ntmax\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n4\ntmin\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nTidy version\n\n\n\n\n\nid\nyear\nmonth\nday\ntmax\ntmin\n\n\n\n\nMX17004\n2010\n1\n30\n27.8\n14.5\n\n\nMX17004\n2010\n2\n2\n27.3\n14.4\n\n\nMX17004\n2010\n2\n3\n24.1\n14.4\n\n\nMX17004\n2010\n2\n11\n29.7\n13.4\n\n\nMX17004\n2010\n2\n23\n29.9\n10.7\n\n\nMX17004\n2010\n3\n5\n32.1\n14.2\n\n\nMX17004\n2010\n3\n10\n34.5\n16.8\n\n\nMX17004\n2010\n3\n16\n31.1\n17.6\n\n\n\n\n\n\n\nTidying solution\nTo clean this up, we need to bring all of the day columns together using pivot_longer so that we can encode day as a variable and temperature as a variable.\nWe also may want to have max and min temperature as separate columns (i.e., variables), rather than keeping that as a key-value pair. That is, the tmin and tmax values denote the attributes of a single observation, which would usually be represented as separate variables in tidy format. To obtain min and max temperatures as separate columns, we use pivot_wider to move the element values onto separate columns.\nHere is the basic approach:\n#use num_range() to select variables called d1--d31\ntidy2 &lt;- mess2 %&gt;% \n  pivot_longer(\n    cols = num_range(\"d\", 1:31), \n    names_to = \"day\", \n    values_to = \"temperature\",\n    names_prefix = \"d\", #trim off the 'd'\n    names_transform = list(day = as.integer)\n  ) %&gt;%\n  pivot_wider(names_from = \"element\", values_from = \"temperature\") %&gt;%\n  na.omit()\nNotice that pivot_longer has a few built-in arguments for helping us trim off parts of the column names that are not data per se. Here, we have d1–d31, but only the numeric part of that is data. The names_prefix=\"d\" tells pivot_longer to trim the leading ‘d’ from every value in the day column. The names_transform=list(day=as.integer) argument then converts the resulting day values to integers so they behave like a numeric variable rather than text.\nHere, pivot_wider a key – element – that has values 'tmin' or 'tmax' and puts the values of these rows onto columns. This is a kind of ‘long-to-wide’ conversion and we would expect here for the number of rows in the dataset drop two-fold with the pivot_wider compared to the preceding step where we’ve gathered the day columns.\nIt is often useful to check the number of rows after each step in a data transformation pipeline. Here, I just break up the pipeline into the pivot_longer and pivot_wider steps and check the structure in between.\n\n\nCode\ntidy2.1 &lt;- mess2 %&gt;% \n  pivot_longer(\n    cols = num_range(\"d\", 1:31), names_to = \"day\", values_to = \"temperature\",\n    names_prefix = \"d\", names_transform = list(day = as.integer)\n  )\n\nnrow(tidy2.1)\n\n\n[1] 682\n\n\nCode\ntidy2.2 &lt;- tidy2.1 %&gt;%\n  pivot_wider(names_from = \"element\", values_from = \"temperature\")\n\n# with NAs included (since data are sparse), we get the expected 50% reduction in rows\nnrow(tidy2.2)\n\n\n[1] 341\n\n\nCode\n# there are only 33 useful/present observations\nnrow(tidy2.2 %&gt;% na.omit)\n\n\n[1] 33"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#missingness-implicit-vs-explicit",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#missingness-implicit-vs-explicit",
    "title": "Tidy data overview",
    "section": "Missingness: implicit vs explicit",
    "text": "Missingness: implicit vs explicit\nTidy data often needs missing values made explicit (or dropped) so analyses behave as intended.\n\n\nCode\ntidy_missing &lt;- tibble::tribble(\n  ~id, ~day, ~score,\n  1, \"Mon\", 10,\n  1, \"Wed\", 8,\n  2, \"Mon\", 9,\n  3, \"Mon\", NA_real_\n)\n\n# Drop rows where all pivoted values are NA\ntidy_missing %&gt;%\n  tidyr::pivot_wider(names_from = \"day\", values_from = \"score\") %&gt;%\n  dplyr::filter(!dplyr::if_all(-id, is.na))\n\n\n\n\n\n\nid\nMon\nWed\n\n\n\n\n1\n10\n8\n\n\n2\n9\nNA\n\n\n\n\n\n\nCode\n# Make implicit missing combinations explicit\ntidy_missing %&gt;%\n  tidyr::complete(id, day)\n\n\n\n\n\n\nid\nday\nscore\n\n\n\n\n1\nMon\n10\n\n\n1\nWed\n8\n\n\n2\nMon\n9\n\n\n2\nWed\nNA\n\n\n3\nMon\nNA\n\n\n3\nWed\nNA\n\n\n\n\n\n\nCode\n# Fill down within a group (e.g., carry forward metadata). Grouping ensures we\n# only fill within each id; ungroup to avoid carrying this into later steps.\ntidy_missing %&gt;%\n  tidyr::complete(id, day) %&gt;%\n  dplyr::group_by(id) %&gt;%\n  tidyr::fill(score, .direction = \"down\") %&gt;%\n  dplyr::ungroup()\n\n\n\n\n\n\nid\nday\nscore\n\n\n\n\n1\nMon\n10\n\n\n1\nWed\n8\n\n\n2\nMon\n9\n\n\n2\nWed\n9\n\n\n3\nMon\nNA\n\n\n3\nWed\nNA"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#separate-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#separate-example",
    "title": "Tidy data overview",
    "section": "separate example",
    "text": "separate example\nIn our third mess, we have multiple variables stored in one column. More specifically, in these data, the ‘m014’ etc. columns represent a combination of sex (m/f) and age range (e.g., 014 is 0–14). The country and year columns are ‘tidy’ because they represent variables, but the sex + age columns are not.\n\nMessy version\n\n\nCode\n#use select to select a few columns that can fit on the page\nmess3 %&gt;% dplyr::select(country:f1524) %&gt;% kable_table(n=5)\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\nf1524\n\n\n\n\nAD\n2000\n0\n0\n1\n0\n0\n0\n0\nNA\nNA\nNA\n\n\nAE\n2000\n2\n4\n4\n6\n5\n12\n10\nNA\n3\n16\n\n\nAF\n2000\n52\n228\n183\n149\n129\n94\n80\nNA\n93\n414\n\n\nAG\n2000\n0\n0\n0\n0\n0\n0\n1\nNA\n1\n1\n\n\nAL\n2000\n2\n19\n21\n14\n24\n19\n16\nNA\n3\n11\n\n\n\n\n\n\n\nTidy version\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n0-14\n0\n\n\nAE\n2000\nm\n0-14\n2\n\n\nAF\n2000\nm\n0-14\n52\n\n\nAG\n2000\nm\n0-14\n0\n\n\nAL\n2000\nm\n0-14\n2\n\n\nAM\n2000\nm\n0-14\n2\n\n\nAN\n2000\nm\n0-14\n0\n\n\nAO\n2000\nm\n0-14\n186\n\n\n\n\n\n\n\nTidying solution\nWe essentially need to parse apart the ‘m’ from the ‘014’ components of each value, which is a job for separate. Note that we also need to pivot_longer the wacky sex + age columns first to make this easier. Here I use cols=c(-country, -year) to say, ‘all columns except these.’\nThe sep argument of separate tells R how to split the values into multiple variables. Here, by using the number 1, we ask for the first character to become sex and the rest to become age_range.\n\n\nCode\ntidy3 &lt;- mess3 %&gt;% \n  pivot_longer(names_to=\"sex_age\", values_to=\"freq\", cols=c(-country, -year)) %&gt;%\n  separate(sex_age, into=c(\"sex\", \"age_range\"), sep=1)\n\n\nHere, we gathered all columns except country and year into a single key-value pair using pivot_longer. This is an intermediate stage of the dataset that is semi-tidy. We then separate the sex and age components of the values into different variables, resulting in a tidy dataset.\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n014\n0\n\n\nAD\n2000\nm\n1524\n0\n\n\nAD\n2000\nm\n2534\n1\n\n\nAD\n2000\nm\n3544\n0\n\n\nAD\n2000\nm\n4554\n0\n\n\nAD\n2000\nm\n5564\n0\n\n\nAD\n2000\nm\n65\n0\n\n\nAD\n2000\nm\nu\nNA\n\n\n\n\n\nThis is pretty close. The age_range variable is still a little clunky because it isn’t easy to read. We could modify this further using mutate and recode from dplyr, but that’s not the immediate emphasis here.\n\n\nCode\ntidy3 &lt;- tidy3 %&gt;% mutate(age_range=recode(age_range,\n                                           \"014\"=\"0-14\",\n                                           \"1524\"=\"15-24\",\n                                           \"2534\"=\"25-34\",\n                                           \"3544\"=\"35-44\",\n                                           \"4554\"=\"45-54\",\n                                           \"5564\"=\"55-64\",\n                                           \"65\"=\"65+\",\n                                           \"u\"=\"unknown\", .default=NA_character_\n))\n\n\n\n\nModern separate_* alternatives\nFor simple string splitting, tidyr now provides separate_wider_delim() (split on a delimiter) and separate_wider_regex() (split using a regex). These are often clearer than separate() because they create named columns directly.\n\n\nCode\ntoy_people &lt;- tibble::tibble(\n  person = c(\"Ada-Lovelace\", \"Grace-Hopper\", \"Katherine-Johnson\")\n)\n\ntoy_people %&gt;% separate_wider_delim(person, delim = \"-\", names = c(\"first\", \"last\"))\n\n\n\n\n\n\nfirst\nlast\n\n\n\n\nAda\nLovelace\n\n\nGrace\nHopper\n\n\nKatherine\nJohnson\n\n\n\n\n\n\nCode\ntoy_people %&gt;% separate_wider_regex(person, patterns = c(first = \"^[^-]+\", \"-\",\n                                                         last = \"[^-]+$\"\n))\n\n\n\n\n\n\nfirst\nlast\n\n\n\n\nAda\nLovelace\n\n\nGrace\nHopper\n\n\nKatherine\nJohnson\n\n\n\n\n\n\n\n\nTidying solution using pivot_longer alone\nI am ambivalent about whether it is useful to combine separable objectives into a single data wrangling verb. Nevertheless, I want to note that the pivot_longer function provides added functionality for both combining columns into a key-value pair format and splitting the key into multiple variables if the key variable is an amalgamation of discrete variables. This can allow us to skip the separate step:\n\n\nCode\ntidy3 &lt;- mess3 %&gt;% \n  pivot_longer(\n    cols = c(-country, -year), \n    names_to = c(\"sex\", \"age_range\"),\n    names_pattern = \"(.)(.*)\", #first character versus the rest\n    values_to = \"freq\",\n    names_ptypes = list(\n      age_range=factor(\n        levels=c(\"u\", \"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"),\n        ordered=TRUE\n      )\n    )\n  )\n\n#Note: we can't adjust the labels in the names_ptype above using labels=c(...).\n#Thus, we'd need to use recode_factor, similar to the above\ntidy3 &lt;- tidy3 %&gt;% mutate(\n  age_range=recode_factor(age_range,\n                          \"014\"=\"0-14\",\n                          \"1524\"=\"15-24\",\n                          \"2534\"=\"25-34\",\n                          \"3544\"=\"35-44\",\n                          \"4554\"=\"45-54\",\n                          \"5564\"=\"55-64\",\n                          \"65\"=\"65+\",\n                          \"u\"=\"unknown\", .default=NA_character_\n  ))\n\ntidy3 %&gt;% kable_table(n=8)\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n0-14\n0\n\n\nAD\n2000\nm\n15-24\n0\n\n\nAD\n2000\nm\n25-34\n1\n\n\nAD\n2000\nm\n35-44\n0\n\n\nAD\n2000\nm\n45-54\n0\n\n\nAD\n2000\nm\n55-64\n0\n\n\nAD\n2000\nm\n65+\n0\n\n\nAD\n2000\nm\nunknown\nNA"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#unite-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#unite-example",
    "title": "Tidy data overview",
    "section": "unite example",
    "text": "unite example\nAlthough the least common of the tidying verbs (in my experience), unite is the complement to separate and can be used to bring together multiple variables that we wish to store as a single variable. For example, we may have first name and last name stored in separate variables, but wish to put them together for display or exporting purposes. Sometimes, we also use unite as an intermediate stage in tidying, bringing together variables, reshaping the data, then re-separating them.\n\n\n\n\n\nfirst_name\nlast_name\nage\nfavorite_color\n\n\n\n\nGraham\nDoe\n11\nPurple\n\n\nKieran\nHelali\n9\nBlue\n\n\nCharlotte\nStafford\n11\nPink\n\n\n\n\n\n\nTidying solution\nIf we wanted to have a full_name, we could use unite to combine first_name and last_name and then get rid of those individual columns.\n\n\nCode\ndf4_united &lt;- df4 %&gt;% unite(col = \"full_name\", first_name, last_name, sep=\" \")\n\n\n\n\n\n\n\nfull_name\nage\nfavorite_color\n\n\n\n\nGraham Doe\n11\nPurple\n\n\nKieran Helali\n9\nBlue\n\n\nCharlotte Stafford\n11\nPink"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#data.table-meltdcast",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#data.table-meltdcast",
    "title": "Tidy data overview",
    "section": "data.table melt/dcast",
    "text": "data.table melt/dcast\nThe data.table package provides melt() and dcast() for fast reshaping. The formula interface is compact for multi-way summary tables, and dcast() can aggregate on the fly while casting.\n\n\nCode\ndt &lt;- data.table::data.table(\n  id = 1:4,\n  group = c(\"A\", \"A\", \"B\", \"B\"),\n  y2022 = c(10, 12, 9, 11),\n  y2023 = c(13, 14, 10, 12),\n  z2022 = c(100, 120, 90, 110),\n  z2023 = c(130, 140, 95, 115)\n)\n\nlong_dt &lt;- data.table::melt(\n  dt,\n  id.vars = c(\"id\", \"group\"),\n  measure.vars = patterns(\"^y\", \"^z\"),\n  variable.name = \"year\",\n  value.name = c(\"y\", \"z\")\n)\n\n# Map pattern indices to actual year labels.\nlong_dt$year &lt;- c(\"2022\", \"2023\")[as.integer(long_dt$year)]\nlong_dt\n\n\n\n\n\n\nid\ngroup\nyear\ny\nz\n\n\n\n\n1\nA\n2022\n10\n100\n\n\n2\nA\n2022\n12\n120\n\n\n3\nB\n2022\n9\n90\n\n\n4\nB\n2022\n11\n110\n\n\n1\nA\n2023\n13\n130\n\n\n2\nA\n2023\n14\n140\n\n\n3\nB\n2023\n10\n95\n\n\n4\nB\n2023\n12\n115\n\n\n\n\n\n\nCode\nmean_y &lt;- data.table::dcast(\n  long_dt,\n  group ~ year,\n  value.var = \"y\",\n  fun.aggregate = mean\n)\n\nmean_y\n\n\n\n\n\n\ngroup\n2022\n2023\n\n\n\n\nA\n11\n13.5\n\n\nB\n10\n11.0\n\n\n\n\n\n\nCode\nmean_multi &lt;- data.table::dcast(\n  long_dt,\n  group ~ year,\n  value.var = c(\"y\", \"z\"),\n  fun.aggregate = mean\n)\n\nmean_multi\n\n\n\n\n\n\ngroup\ny_2022\ny_2023\nz_2022\nz_2023\n\n\n\n\nA\n11\n13.5\n110\n135\n\n\nB\n10\n11.0\n100\n105\n\n\n\n\n\n\nCompared with tidyr, data.table::dcast() makes aggregation part of the casting step and can cast multiple value columns in one pass. It is also a common choice for very large tables where performance matters.\nFurther reshaping extensions using data.table package: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html",
    "href": "materials/w02_wrangling/joins_tutorial.html",
    "title": "Joins Tutorial",
    "section": "",
    "text": "This document has been adapted and extended by Michael Hallquist and Benjamin Johnson from Jenny Bryan’s dplyr joins tutorial (http://stat545.com/bit001_dplyr-cheatsheet.html). Animations were developed by Garrick Aden-Buie. The goal is to develop an intuition of the four major types of two-table join operations: inner, left, right, and full. We’ll also get into using joins to identify areas of match or mismatch between two datasets (using semi- and anti-joins).\n\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n\n\nStorm\ngood\nfemale\nMarvel\n\n\nMystique\nbad\nfemale\nMarvel\n\n\nBatman\ngood\nmale\nDC\n\n\nJoker\nbad\nmale\nDC\n\n\nCatwoman\nbad\nfemale\nDC\n\n\nHellboy\ngood\nmale\nDark Horse Comics\n\n\n\n\n\n\n\n\n\n\n\n\n\npublisher\nyr_founded\n\n\n\n\nDC\n1934\n\n\nMarvel\n1939\n\n\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#superheroes-table",
    "href": "materials/w02_wrangling/joins_tutorial.html#superheroes-table",
    "title": "Joins Tutorial",
    "section": "",
    "text": "name\nalignment\ngender\npublisher\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n\n\nStorm\ngood\nfemale\nMarvel\n\n\nMystique\nbad\nfemale\nMarvel\n\n\nBatman\ngood\nmale\nDC\n\n\nJoker\nbad\nmale\nDC\n\n\nCatwoman\nbad\nfemale\nDC\n\n\nHellboy\ngood\nmale\nDark Horse Comics"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#publishers-table",
    "href": "materials/w02_wrangling/joins_tutorial.html#publishers-table",
    "title": "Joins Tutorial",
    "section": "",
    "text": "publisher\nyr_founded\n\n\n\n\nDC\n1934\n\n\nMarvel\n1939\n\n\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#inner-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#inner-join",
    "title": "Joins Tutorial",
    "section": "inner join",
    "text": "inner join\nRequire match in both datasets (non-matching rows are dropped)\nFor those of you who are visual learners, conceptually, imagine the following two simple datasets:\n\nAn inner join combines the two datasets and drops the non-matching rows like so:\n\nLet’s try it with our superhero data.\n\n\nCode\n#*NB*: Here, we retain the joined dataset as ijsp\nijsp &lt;- inner_join(x=superheroes, y=publishers)\nprint(ijsp)\n\n\n# A tibble: 6 × 5\n  name     alignment gender publisher yr_founded\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n1 Magneto  bad       male   Marvel          1939\n2 Storm    good      female Marvel          1939\n3 Mystique bad       female Marvel          1939\n4 Batman   good      male   DC              1934\n5 Joker    bad       male   DC              1934\n6 Catwoman bad       female DC              1934\n\n\nSame idea, just explicit declaration of key (i.e., “publisher”)\n\n\nCode\n#note that we've cut the x=, y= as this optional\ninner_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\n\n\n\n\nNotice both Hellboy (from the superheroes dataset) and Image comics (from the publishers dataset) were dropped."
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#left-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#left-join",
    "title": "Joins Tutorial",
    "section": "left join",
    "text": "left join\nKeep all rows in left-hand ‘x’ dataset (i.e., superheroes). Add columns from publishers where there is a match. Fill in NA for non-matching observations.\n\n\n\nCode\nleft_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\nHellboy\ngood\nmale\nDark Horse Comics\nNA"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#right-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#right-join",
    "title": "Joins Tutorial",
    "section": "right join",
    "text": "right join\nKeep all rows in right-hand ‘y’ dataset (i.e., publishers). Add columns from superheroes where there is a match. Fill in NA for non-matching observations.\n\n\n\nCode\n# Note the shift to using dplyr piping\n# This achieves the same purpose, but may be preferred by those who love pipes\nsuperheroes %&gt;% right_join(publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\nNA\nNA\nNA\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#full-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#full-join",
    "title": "Joins Tutorial",
    "section": "full join",
    "text": "full join\nKeep all rows in left-hand ‘x’ (superheroes) and right-hand ‘y’ (publishers) datasets.\nResulting dataset will have all columns of both datasets, but filling in NA for any non-matches on either side (denoted as blank spaces below).\n\n\n\nCode\nsuperheroes %&gt;% full_join(publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\nHellboy\ngood\nmale\nDark Horse Comics\nNA\n\n\nNA\nNA\nNA\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#semi_join",
    "href": "materials/w02_wrangling/joins_tutorial.html#semi_join",
    "title": "Joins Tutorial",
    "section": "semi_join",
    "text": "semi_join\nretain observations (rows) in x that match in y\n\n\nObservations in superheroes that match in publishers\nNotice that this is different from the left_join shown above as the data from y is not kept. That is the fundamental difference between ‘mutating joins’ (e.g., left_join) and ‘filtering joins’ (e.g., semi_join).\n\n\nCode\nsemi_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n\n\nStorm\ngood\nfemale\nMarvel\n\n\nMystique\nbad\nfemale\nMarvel\n\n\nBatman\ngood\nmale\nDC\n\n\nJoker\nbad\nmale\nDC\n\n\nCatwoman\nbad\nfemale\nDC\n\n\n\n\n\n\n\n\nObservations in publishers that match in superheroes\n\n\nCode\nsemi_join(publishers, superheroes, by=\"publisher\")\n\n\n\n\n\n\npublisher\nyr_founded\n\n\n\n\nDC\n1934\n\n\nMarvel\n1939\n\n\n\n\n\n\nThis can be useful if you have a dataset of your data of interest and another dataset that indicates which of your participants/observations you want to remove or filter out."
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#anti_join",
    "href": "materials/w02_wrangling/joins_tutorial.html#anti_join",
    "title": "Joins Tutorial",
    "section": "anti_join",
    "text": "anti_join\nobservations in x that are not matched in y Note that this is similar to setdiff in base R\n\n\nObservations in superheroes that don’t match in publishers\n\n\nCode\nanti_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\n\n\n\n\nHellboy\ngood\nmale\nDark Horse Comics\n\n\n\n\n\n\n\n\nObservations in publishers that don’t match in superheroes\n\n\nCode\npublishers %&gt;% anti_join(superheroes, by=\"publisher\")\n\n\n\n\n\n\npublisher\nyr_founded\n\n\n\n\nImage\n1992\n\n\n\n\n\n\nThis can be useful if you are trying to identify extra participants/observations that may have snuck into one dataset (x) or been deleted in another (y)."
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html",
    "href": "materials/w03_automation/validate_bfi.html",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "",
    "text": "We validate a psychology survey dataset using validate and demonstrate detection, selection/isolation, and correction for:\n\nrationally invalid values (out-of-range)\nstatistically invalid values (odd response patterns)\nduplicated observations\nunexplained missing data"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#goal",
    "href": "materials/w03_automation/validate_bfi.html#goal",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "",
    "text": "We validate a psychology survey dataset using validate and demonstrate detection, selection/isolation, and correction for:\n\nrationally invalid values (out-of-range)\nstatistically invalid values (odd response patterns)\nduplicated observations\nunexplained missing data"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#setup",
    "href": "materials/w03_automation/validate_bfi.html#setup",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "Setup",
    "text": "Setup\n\n\nCode\n# Install if needed\npkgs &lt;- c(\"validate\", \"psych\", \"dplyr\")\nmissing &lt;- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\nif (length(missing) &gt; 0) install.packages(missing)\n\nlibrary(validate)\nlibrary(psych)\nlibrary(dplyr)\n\nrows_violating &lt;- function(cn) {\n  v &lt;- values(cn)\n  which(apply(v, 1, function(x) any(x == FALSE, na.rm = TRUE)))\n}"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#data",
    "href": "materials/w03_automation/validate_bfi.html#data",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "Data",
    "text": "Data\npsych::bfi contains Big Five Inventory items (1-6) plus age, gender, education.\n\n\nCode\nitems &lt;- c(paste0(\"A\", 1:5), paste0(\"C\", 1:5), paste0(\"E\", 1:5),\n           paste0(\"N\", 1:5), paste0(\"O\", 1:5))\n\nbfi_raw &lt;- psych::bfi\nbfi_dirty &lt;- bfi_raw\n\n# Inject a few issues so the checks are demonstrable\nbfi_dirty[1, \"A1\"] &lt;- 9\nbfi_dirty[2, \"C3\"] &lt;- 0\nbfi_dirty[3, \"age\"] &lt;- -5\nbfi_dirty[4, \"gender\"] &lt;- 3\nbfi_dirty[5, items] &lt;- 1\nbfi_dirty[6, items[1:10]] &lt;- NA\nbfi_dirty &lt;- rbind(bfi_dirty, bfi_dirty[1, ])"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#rationally-invalid-values",
    "href": "materials/w03_automation/validate_bfi.html#rationally-invalid-values",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "1) Rationally invalid values",
    "text": "1) Rationally invalid values\nDetection with range rules.\n\n\nCode\n# Row-level indicator: all items are within 1..6 or missing\nbfi_dirty$items_in_range &lt;- apply(bfi_dirty[items], 1, function(x) {\n  all(x %in% 1:6 | is.na(x))\n})\n\nrules_range &lt;- validator(\n  items_in_range == TRUE,\n  age &gt;= 16,\n  age &lt;= 100,\n  gender %in% c(1, 2) | is.na(gender),\n  education %in% 1:5 | is.na(education)\n)\n\nrange_cn &lt;- confront(bfi_dirty, rules_range)\nsummary(range_cn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2801\n2798\n3\n0\nFALSE\nFALSE\nitems_in_range == TRUE\n\n\nV2\n2801\n2713\n88\n0\nFALSE\nFALSE\nage - 16 &gt;= -1e-08\n\n\nV3\n2801\n2801\n0\n0\nFALSE\nFALSE\nage - 100 &lt;= 1e-08\n\n\nV4\n2801\n2800\n1\n0\nFALSE\nFALSE\ngender %vin% c(1, 2) | is.na(gender)\n\n\nV5\n2801\n2801\n0\n0\nFALSE\nFALSE\neducation %vin% 1:5 | is.na(education)\n\n\n\n\n\n\nCode\nbad_range &lt;- rows_violating(range_cn)\nhead(bfi_dirty[bad_range, c(\"age\", \"gender\", \"education\", \"A1\", \"C3\")])\n\n\n\n\n\n\n\nage\ngender\neducation\nA1\nC3\n\n\n\n\n61617\n16\n1\nNA\n9\n3\n\n\n61618\n18\n2\nNA\n2\n0\n\n\n61620\n-5\n2\nNA\n5\n4\n\n\n61621\n17\n3\nNA\n4\n3\n\n\n61670\n14\n2\nNA\n2\n4\n\n\n61780\n14\n2\nNA\n5\n6\n\n\n\n\n\n\nCorrection: set invalid values to NA.\n\n\nCode\nclip_to_na &lt;- function(x, lo, hi) {\n  x[x &lt; lo | x &gt; hi] &lt;- NA\n  x\n}\n\nbfi_fixed &lt;- bfi_dirty %&gt;%\n  mutate(\n    across(all_of(items), ~ clip_to_na(.x, 1, 6)),\n    age = clip_to_na(age, 16, 100),\n    gender = if_else(gender %in% c(1, 2), gender, NA_real_),\n    education = if_else(education %in% 1:5, education, NA_real_)\n  )"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#statistically-invalid-values-odd-response-patterns",
    "href": "materials/w03_automation/validate_bfi.html#statistically-invalid-values-odd-response-patterns",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "2) Statistically invalid values (odd response patterns)",
    "text": "2) Statistically invalid values (odd response patterns)\nLook for rows with no variation in items, suggesting flat response pattern\n\n\nCode\nbfi_fixed$row_sd &lt;- apply(bfi_fixed[items], 1, sd, na.rm = TRUE)\nbfi_fixed$n_missing &lt;- rowSums(is.na(bfi_fixed[items]))\n\nrules_stat &lt;- validator(\n  row_sd &gt; 0,\n  n_missing &lt;= 5\n)\n\nstat_cn &lt;- confront(bfi_fixed, rules_stat)\nsummary(stat_cn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2801\n2795\n6\n0\nFALSE\nFALSE\nrow_sd &gt; 0\n\n\nV2\n2801\n2794\n7\n0\nFALSE\nFALSE\nn_missing - 5 &lt;= 1e-08\n\n\n\n\n\n\nCode\nbad_stat &lt;- rows_violating(stat_cn)\nhead(bfi_fixed[bad_stat, c(\"row_sd\", \"n_missing\")])\n\n\n\n\n\n\n\nrow_sd\nn_missing\n\n\n\n\n61622\n0.000000\n0\n\n\n61623\n1.804756\n10\n\n\n62783\n0.000000\n0\n\n\n63030\n1.779513\n15\n\n\n63991\n0.000000\n15\n\n\n64642\n0.000000\n0\n\n\n\n\n\n\nCorrection: remove cases with implausible patterns.\n\n\nCode\nbfi_fixed &lt;- bfi_fixed %&gt;%\n  filter(row_sd &gt; 0, n_missing &lt;= 5)"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#duplicated-observations",
    "href": "materials/w03_automation/validate_bfi.html#duplicated-observations",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "3) Duplicated observations",
    "text": "3) Duplicated observations\nDetection using exact duplicate rows on items + demographics.\n\n\nCode\ndup_key &lt;- c(items, \"age\", \"gender\", \"education\")\nbfi_fixed$dup_row &lt;- duplicated(bfi_fixed[dup_key])\n\nrules_dup &lt;- validator(!dup_row)\n\ndup_cn &lt;- confront(bfi_fixed, rules_dup)\nsummary(dup_cn)\n\n\n\n\n\n\nerror\nwarning\n\n\n\n\n\n\n\n\nCode\nbfi_fixed &lt;- bfi_fixed %&gt;%\n  filter(!dup_row)"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#unexplained-missing-data",
    "href": "materials/w03_automation/validate_bfi.html#unexplained-missing-data",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "4) Unexplained missing data",
    "text": "4) Unexplained missing data\nDetection of missing demographics.\n\n\nCode\nmissing_rules &lt;- validator(\n  !is.na(age),\n  !is.na(gender)\n)\n\nmissing_cn &lt;- confront(bfi_fixed, missing_rules)\nsummary(missing_cn)\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2788\n2702\n86\n0\nFALSE\nFALSE\n!is.na(age)\n\n\nV2\n2788\n2787\n1\n0\nFALSE\nFALSE\n!is.na(gender)\n\n\n\n\n\n\nCode\nbad_missing &lt;- rows_violating(missing_cn)\nhead(bfi_fixed[bad_missing, c(\"age\", \"gender\")])\n\n\n\n\n\n\n\nage\ngender\n\n\n\n\n61620\nNA\n2\n\n\n61621\n17\nNA\n\n\n61670\nNA\n2\n\n\n61780\nNA\n2\n\n\n62151\nNA\n2\n\n\n62360\nNA\n2\n\n\n\n\n\n\nCorrection: remove cases with missing key demographics and impute remaining item missings.\n\n\nCode\nbfi_fixed &lt;- bfi_fixed %&gt;%\n  filter(!is.na(age), !is.na(gender)) %&gt;%\n  mutate(across(all_of(items), ~ replace(.x, is.na(.x), median(.x, na.rm = TRUE))))"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#final-consistency-check",
    "href": "materials/w03_automation/validate_bfi.html#final-consistency-check",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "Final consistency check",
    "text": "Final consistency check\n\n\nCode\n# Recompute indicators after corrections\nbfi_fixed$items_in_range &lt;- apply(bfi_fixed[items], 1, function(x) {\n  all(x %in% 1:6 | is.na(x))\n})\nbfi_fixed$row_sd &lt;- apply(bfi_fixed[items], 1, sd, na.rm = TRUE)\nbfi_fixed$n_missing &lt;- rowSums(is.na(bfi_fixed[items]))\nbfi_fixed$dup_row &lt;- duplicated(bfi_fixed[dup_key])\n\nfinal_rules &lt;- validator(\n  items_in_range == TRUE,\n  age &gt;= 16, age &lt;= 100,\n  gender %in% c(1, 2),\n  education %in% 1:5 | is.na(education),\n  row_sd &gt; 0,\n  n_missing &lt;= 5,\n  !dup_row\n)\n\nfinal_cn &lt;- confront(bfi_fixed, final_rules)\nsummary(final_cn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2701\n2701\n0\n0\nFALSE\nFALSE\nitems_in_range == TRUE\n\n\nV2\n2701\n2701\n0\n0\nFALSE\nFALSE\nage - 16 &gt;= -1e-08\n\n\nV3\n2701\n2701\n0\n0\nFALSE\nFALSE\nage - 100 &lt;= 1e-08\n\n\nV4\n2701\n2701\n0\n0\nFALSE\nFALSE\ngender %vin% c(1, 2)\n\n\nV5\n2701\n2701\n0\n0\nFALSE\nFALSE\neducation %vin% 1:5 | is.na(education)\n\n\nV6\n2701\n2701\n0\n0\nFALSE\nFALSE\nrow_sd &gt; 0\n\n\nV7\n2701\n2701\n0\n0\nFALSE\nFALSE\nn_missing - 5 &lt;= 1e-08\n\n\n\n\n\n\nThis compact workflow uses detection (rules + confront), isolation (violating), and correction (recoding to NA, removal of implausible cases, and simple imputation) to achieve consistent data."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "PSYC 859 Syllabus",
    "section": "",
    "text": "This graduate course is intended to provide an applied introduction to data management and data visualization in the social sciences. In order to take full advantage of modern statistical methods (e.g., structural equation models), competency in data management, semi-automated processing, and data wrangling is prerequisite. Likewise, prior to employing inferential statistics, exploratory visualization and analysis is essential to facilitate data cleaning and to form an initial understanding of patterns in the data. This course will cover both the principles and practice of data management, visualization, and exploratory analysis for summarizing quantitative data. In addition, students will learn data science skills to manage and visualize “big data,” where the size or complexity of the dataset defies traditional techniques.\nApplications of data management, visualization, and analysis will use the R statistical programming language. R is quickly becoming the lingua franca in data science across disciplines and offers unparalleled tools for data analysis and visualization."
  },
  {
    "objectID": "syllabus.html#week-1",
    "href": "syllabus.html#week-1",
    "title": "PSYC 859 Syllabus",
    "section": "Week 1 (1/8): Introduction to data management and tidy data",
    "text": "Week 1 (1/8): Introduction to data management and tidy data\n\nConceptual readings\n\nBriney, K., Coates, H., & Goben, A. (2020). Foundational Practices of Research Data Management. Research Ideas and Outcomes, 6, e56508. https://doi.org/10.3897/rio.6.e56508\nBorer, E. T., Seabloom, E. W., Jones, M. B., & Schildhauer, M. (2009). Some Simple Guidelines for Effective Data Management. The Bulletin of the Ecological Society of America, 90, 205-214. https://doi.org/10.1890/0012-9623-90.2.205\n(Optional) Borghi, J. A., & Gulick, A. E. V. (2021). Data management and sharing: Practices and perceptions of psychology researchers. PLOS ONE, 16, e0252047. https://doi.org/10.1371/journal.pone.0252047\n\n\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 2: Workflow: basics. https://r4ds.hadley.nz/workflow-basics.html\nCh. 4: Workflow: code style. https://r4ds.hadley.nz/workflow-style.html\nCh. 5: Data tidying. https://r4ds.hadley.nz/data-tidy.html\nCh. 6: Workflow: scripts and projects. https://r4ds.hadley.nz/workflow-scripts.html\nCh. 7: Data import. https://r4ds.hadley.nz/data-import.html\n\n(Supplementary) Tidyr pivoting vignette: https://tidyr.tidyverse.org/articles/pivot.html\nTidyr cheatsheet: https://rstudio.github.io/cheatsheets/tidyr.pdf\nRStudio Data Import cheat sheet: https://rstudio.github.io/cheatsheets/data-import.pdf"
  },
  {
    "objectID": "syllabus.html#week-2",
    "href": "syllabus.html#week-2",
    "title": "PSYC 859 Syllabus",
    "section": "Week 2 (1/15): Data aggregation, manipulation, joins",
    "text": "Week 2 (1/15): Data aggregation, manipulation, joins\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 3: Data transformation: https://r4ds.hadley.nz/data-transform.html\nCh. 12: Logical vectors: https://r4ds.hadley.nz/logicals.html\nCh. 13: Numbers: https://r4ds.hadley.nz/numbers.html\nCh. 14: Strings: https://r4ds.hadley.nz/strings.html\nCh. 16: Factors: https://r4ds.hadley.nz/factors.html\nCh. 19: Joins: https://r4ds.hadley.nz/joins.html\n(Optional) Ch. 15: Regular expressions. https://r4ds.hadley.nz/regexps.html\n(Optional) Ch. 17: Dates and times. https://r4ds.hadley.nz/datetimes.html\n(Optional) Ch. 20: Import: spreadsheets. https://r4ds.hadley.nz/spreadsheets.html"
  },
  {
    "objectID": "syllabus.html#week-3",
    "href": "syllabus.html#week-3",
    "title": "PSYC 859 Syllabus",
    "section": "Week 3 (1/22): Data processing and quality assurance, custom functions, basics of automation",
    "text": "Week 3 (1/22): Data processing and quality assurance, custom functions, basics of automation\n\nConceptual readings\n\nVan den Broeck, J., Argeseanu Cunningham, S., Eeckels, R., & Herbst, K. (2005). Data Cleaning: Detecting, diagnosing, and editing data abnormalities. PLoS Medicine, 2(10), e267.\n(Optional) Broman, K. W., & Woo, K. H. (2018). Data Organization in Spreadsheets. The American Statistician, 72, 2-10.\n\n\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 25: Functions. https://r4ds.hadley.nz/functions.html\nCh. 26: Iteration. https://r4ds.hadley.nz/iteration.html\n\nR validate package. Review https://cran.r-project.org/web/packages/validate/vignettes/cookbook.html\nIntroduction to pointblank package: https://bookdown.org/pdr_higgins/rmrwr/data-exploration-and-validation-with-the-pointblank-package.html"
  },
  {
    "objectID": "syllabus.html#week-4",
    "href": "syllabus.html#week-4",
    "title": "PSYC 859 Syllabus",
    "section": "Week 4 (1/29): Advanced data manipulation and management, tracking work in R markdown",
    "text": "Week 4 (1/29): Advanced data manipulation and management, tracking work in R markdown\n\nConceptual readings\n\nWilson, G., Aruliah, D. A., Brown, C. T., Chue Hong, N. P., Davis, M., Guy, R. T., Haddock, S. H. D., Huff, K. D., Mitchell, I. M., Plumbley, M. D., Waugh, B., White, E. P., & Wilson, P. (2014). Best Practices for Scientific Computing. PLoS Biology, 12(1), e1001745. https://doi.org/10.1371/journal.pbio.1001745\nKarl Broman steps to reproducible research: http://kbroman.org/steps2rr/\n\n\n\nPractical readings\n\nGandrud (2015). Chs. 2 and 4 in Reproducible Research with R and RStudio.\nGetting started with Quarto: https://quarto.org/docs/get-started/hello/rstudio.html\nIntroduction to targets package in R: https://books.ropensci.org/targets/walkthrough.html"
  },
  {
    "objectID": "syllabus.html#week-5",
    "href": "syllabus.html#week-5",
    "title": "PSYC 859 Syllabus",
    "section": "Week 5 (2/5): Principles of data visualization and graphical grammar",
    "text": "Week 5 (2/5): Principles of data visualization and graphical grammar"
  },
  {
    "objectID": "syllabus.html#week-6",
    "href": "syllabus.html#week-6",
    "title": "PSYC 859 Syllabus",
    "section": "Week 6 (2/12): Visual and graphical perception",
    "text": "Week 6 (2/12): Visual and graphical perception"
  },
  {
    "objectID": "syllabus.html#week-7",
    "href": "syllabus.html#week-7",
    "title": "PSYC 859 Syllabus",
    "section": "Week 7 (2/19): Graphic design, layout, style, use of color",
    "text": "Week 7 (2/19): Graphic design, layout, style, use of color"
  },
  {
    "objectID": "syllabus.html#week-8",
    "href": "syllabus.html#week-8",
    "title": "PSYC 859 Syllabus",
    "section": "Week 8 (2/26): A tour of quantitative visualization",
    "text": "Week 8 (2/26): A tour of quantitative visualization"
  },
  {
    "objectID": "syllabus.html#week-9",
    "href": "syllabus.html#week-9",
    "title": "PSYC 859 Syllabus",
    "section": "Week 9 (3/5): Visualizing continuous data (in ggplot2)",
    "text": "Week 9 (3/5): Visualizing continuous data (in ggplot2)"
  },
  {
    "objectID": "syllabus.html#week-10",
    "href": "syllabus.html#week-10",
    "title": "PSYC 859 Syllabus",
    "section": "Week 10 (3/12): Visualizing count and categorical data (in ggplot2)",
    "text": "Week 10 (3/12): Visualizing count and categorical data (in ggplot2)"
  },
  {
    "objectID": "syllabus.html#no-class-spring-break",
    "href": "syllabus.html#no-class-spring-break",
    "title": "PSYC 859 Syllabus",
    "section": "3/19: No class (Spring break)",
    "text": "3/19: No class (Spring break)"
  },
  {
    "objectID": "syllabus.html#week-11",
    "href": "syllabus.html#week-11",
    "title": "PSYC 859 Syllabus",
    "section": "Week 11 (3/26): Maximizing clarity: preparing graphics for presentation and publication",
    "text": "Week 11 (3/26): Maximizing clarity: preparing graphics for presentation and publication"
  },
  {
    "objectID": "syllabus.html#no-class-well-being-day",
    "href": "syllabus.html#no-class-well-being-day",
    "title": "PSYC 859 Syllabus",
    "section": "4/2: No class (Well-being day)",
    "text": "4/2: No class (Well-being day)"
  },
  {
    "objectID": "syllabus.html#week-12",
    "href": "syllabus.html#week-12",
    "title": "PSYC 859 Syllabus",
    "section": "Week 12 (4/9): Visualizing and understanding fit (and misfit) of statistical models",
    "text": "Week 12 (4/9): Visualizing and understanding fit (and misfit) of statistical models"
  },
  {
    "objectID": "syllabus.html#week-13",
    "href": "syllabus.html#week-13",
    "title": "PSYC 859 Syllabus",
    "section": "Week 13 (4/16): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction",
    "text": "Week 13 (4/16): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction"
  },
  {
    "objectID": "syllabus.html#week-14",
    "href": "syllabus.html#week-14",
    "title": "PSYC 859 Syllabus",
    "section": "Week 14 (4/23): Final presentations of data projects",
    "text": "Week 14 (4/23): Final presentations of data projects"
  },
  {
    "objectID": "syllabus.html#permitted-uses",
    "href": "syllabus.html#permitted-uses",
    "title": "PSYC 859 Syllabus",
    "section": "Permitted uses",
    "text": "Permitted uses\nStudents may use AI tools for the following purposes:\n\nDebugging code, including identifying syntax errors, logical errors, or unexpected behavior.\nRequesting explanations of how a piece of code works, why it produces a particular result, or why it fails.\nRequesting suggestions for code improvement, refactoring, or alternative approaches to a task.\n\nThese uses are consistent with how AI tools are used responsibly in real research workflows."
  },
  {
    "objectID": "syllabus.html#expectations-and-responsibilities",
    "href": "syllabus.html#expectations-and-responsibilities",
    "title": "PSYC 859 Syllabus",
    "section": "Expectations and responsibilities",
    "text": "Expectations and responsibilities\nWhen using AI tools, students are expected to:\n\nActively evaluate and understand any code they submit, regardless of its source.\nEnsure they can explain what the code does and why it works, including key functions, assumptions, and consequences.\nMake independent decisions about whether to adopt AI-suggested code, rather than copying it uncritically.\nRemain responsible for correctness, clarity, and reproducibility of all submitted work.\n\nSubmitting code that the student does not understand is inconsistent with the learning objectives of the course."
  },
  {
    "objectID": "syllabus.html#prohibited-uses",
    "href": "syllabus.html#prohibited-uses",
    "title": "PSYC 859 Syllabus",
    "section": "Prohibited uses",
    "text": "Prohibited uses\nThe following are not permitted:\n\nSubmitting AI-generated code or analyses without understanding or review.\nUsing AI tools as a substitute for engaging with core course concepts (e.g., visualization principles, data cleaning logic).\nRepresenting AI-generated work as understanding or reasoning that the student cannot demonstrate if asked."
  },
  {
    "objectID": "syllabus.html#transparency",
    "href": "syllabus.html#transparency",
    "title": "PSYC 859 Syllabus",
    "section": "Transparency",
    "text": "Transparency\nFor major assignments and the final project, students may be asked to include a brief AI use statement (1-3 sentences) describing whether and how AI tools were used (e.g., “used for debugging,” “used to explore alternative ggplot layouts”). This is not punitive; its purpose is to promote transparency and reflective practice."
  }
]