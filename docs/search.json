[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "PSYC 859 Syllabus",
    "section": "",
    "text": "This graduate course is intended to provide an applied introduction to data management and data visualization in the social sciences. In order to take full advantage of modern statistical methods (e.g., structural equation models), competency in data management, semi-automated processing, and data wrangling is prerequisite. Likewise, prior to employing inferential statistics, exploratory visualization and analysis is essential to facilitate data cleaning and to form an initial understanding of patterns in the data. This course will cover both the principles and practice of data management, visualization, and exploratory analysis for summarizing quantitative data. In addition, students will learn data science skills to manage and visualize “big data,” where the size or complexity of the dataset defies traditional techniques.\nApplications of data management, visualization, and analysis will use the R statistical programming language. R is quickly becoming the lingua franca in data science across disciplines and offers unparalleled tools for data analysis and visualization."
  },
  {
    "objectID": "syllabus.html#week-1",
    "href": "syllabus.html#week-1",
    "title": "PSYC 859 Syllabus",
    "section": "Week 1 (1/8): Introduction to data management and tidy data",
    "text": "Week 1 (1/8): Introduction to data management and tidy data\n\nConceptual readings\n\nBriney, K., Coates, H., & Goben, A. (2020). Foundational Practices of Research Data Management. Research Ideas and Outcomes, 6, e56508. https://doi.org/10.3897/rio.6.e56508\nBorer, E. T., Seabloom, E. W., Jones, M. B., & Schildhauer, M. (2009). Some Simple Guidelines for Effective Data Management. The Bulletin of the Ecological Society of America, 90, 205-214. https://doi.org/10.1890/0012-9623-90.2.205\n(Optional) Borghi, J. A., & Gulick, A. E. V. (2021). Data management and sharing: Practices and perceptions of psychology researchers. PLOS ONE, 16, e0252047. https://doi.org/10.1371/journal.pone.0252047\n\n\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 2: Workflow: basics. https://r4ds.hadley.nz/workflow-basics.html\nCh. 4: Workflow: code style. https://r4ds.hadley.nz/workflow-style.html\nCh. 5: Data tidying. https://r4ds.hadley.nz/data-tidy.html\nCh. 6: Workflow: scripts and projects. https://r4ds.hadley.nz/workflow-scripts.html\nCh. 7: Data import. https://r4ds.hadley.nz/data-import.html\n\n(Supplementary) Tidyr pivoting vignette: https://tidyr.tidyverse.org/articles/pivot.html\nTidyr cheatsheet: https://rstudio.github.io/cheatsheets/tidyr.pdf\nRStudio Data Import cheat sheet: https://rstudio.github.io/cheatsheets/data-import.pdf"
  },
  {
    "objectID": "syllabus.html#week-2",
    "href": "syllabus.html#week-2",
    "title": "PSYC 859 Syllabus",
    "section": "Week 2 (1/15): Data aggregation, manipulation, joins",
    "text": "Week 2 (1/15): Data aggregation, manipulation, joins\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 3: Data transformation: https://r4ds.hadley.nz/data-transform.html\nCh. 12: Logical vectors: https://r4ds.hadley.nz/logicals.html\nCh. 13: Numbers: https://r4ds.hadley.nz/numbers.html\nCh. 14: Strings: https://r4ds.hadley.nz/strings.html\nCh. 16: Factors: https://r4ds.hadley.nz/factors.html\nCh. 19: Joins: https://r4ds.hadley.nz/joins.html\n(Optional) Ch. 15: Regular expressions. https://r4ds.hadley.nz/regexps.html\n(Optional) Ch. 17: Dates and times. https://r4ds.hadley.nz/datetimes.html\n(Optional) Ch. 20: Import: spreadsheets. https://r4ds.hadley.nz/spreadsheets.html"
  },
  {
    "objectID": "syllabus.html#permitted-uses",
    "href": "syllabus.html#permitted-uses",
    "title": "PSYC 859 Syllabus",
    "section": "Permitted uses",
    "text": "Permitted uses\nStudents may use AI tools for the following purposes:\n\nDebugging code, including identifying syntax errors, logical errors, or unexpected behavior.\nRequesting explanations of how a piece of code works, why it produces a particular result, or why it fails.\nRequesting suggestions for code improvement, refactoring, or alternative approaches to a task.\n\nThese uses are consistent with how AI tools are used responsibly in real research workflows."
  },
  {
    "objectID": "syllabus.html#expectations-and-responsibilities",
    "href": "syllabus.html#expectations-and-responsibilities",
    "title": "PSYC 859 Syllabus",
    "section": "Expectations and responsibilities",
    "text": "Expectations and responsibilities\nWhen using AI tools, students are expected to:\n\nActively evaluate and understand any code they submit, regardless of its source.\nEnsure they can explain what the code does and why it works, including key functions, assumptions, and consequences.\nMake independent decisions about whether to adopt AI-suggested code, rather than copying it uncritically.\nRemain responsible for correctness, clarity, and reproducibility of all submitted work.\n\nSubmitting code that the student does not understand is inconsistent with the learning objectives of the course."
  },
  {
    "objectID": "syllabus.html#prohibited-uses",
    "href": "syllabus.html#prohibited-uses",
    "title": "PSYC 859 Syllabus",
    "section": "Prohibited uses",
    "text": "Prohibited uses\nThe following are not permitted:\n\nSubmitting AI-generated code or analyses without understanding or review.\nUsing AI tools as a substitute for engaging with core course concepts (e.g., visualization principles, data cleaning logic).\nRepresenting AI-generated work as understanding or reasoning that the student cannot demonstrate if asked."
  },
  {
    "objectID": "syllabus.html#transparency",
    "href": "syllabus.html#transparency",
    "title": "PSYC 859 Syllabus",
    "section": "Transparency",
    "text": "Transparency\nFor major assignments and the final project, students may be asked to include a brief AI use statement (1-3 sentences) describing whether and how AI tools were used (e.g., “used for debugging,” “used to explore alternative ggplot layouts”). This is not punitive; its purpose is to promote transparency and reflective practice."
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "This page contains course materials used during the demonstration and practical exercises.\n\nWeek 1\n\nTidy data conceptual walkthrough (HTML) (QMD)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 859",
    "section": "",
    "text": "Welcome to PSYC 859, Data Management and Visualization, taught at UNC by Michael Hallquist. This website provides access to lectures, labs, and other course materials for the Spring 2026 session.\nUse the Syllabus and Materials tabs to navigate the course schedule and resources.\n\nCourse description\nThis graduate course is intended to provide an applied introduction to data management and data visualization in the social sciences. In order to take full advantage of modern statistical methods (e.g., structural equation models), competency in data management, semi-automated processing, and data wrangling is prerequisite. Likewise, prior to employing inferential statistics, exploratory visualization and analysis is essential to facilitate data cleaning and to form an initial understanding of patterns in the data. This course will cover both the principles and practice of data management, visualization, and exploratory analysis for summarizing quantitative data. In addition, students will learn data science skills to manage and visualize “big data,” where the size or complexity of the dataset defies traditional techniques.\nApplications of data management, visualization, and analysis will use the R statistical programming language. R is quickly becoming the lingua franca in data science across disciplines and offers unparalleled tools for data analysis and visualization.\n\n\nSchedule overview\n\n1/8 (Week 1): Introduction to data management and tidy data\n1/15 (Week 2): Data aggregation, manipulation, joins\n1/22 (Week 3): Data processing and quality assurance, custom functions, basics of automation\n1/29 (Week 4): Advanced data manipulation and management, tracking work in R markdown\n2/5 (Week 5): Principles of data visualization and graphical grammar\n2/12 (Week 6): Visual and graphical perception\n2/19 (Week 7): Graphic design, layout, style, use of color\n2/26 (Week 8): A tour of quantitative visualization\n3/5 (Week 9): Visualizing continuous data (in ggplot2)\n3/12 (Week 10): Visualizing count and categorical data (in ggplot2)\n3/19: No class (Spring break)\n3/26 (Week 11): Maximizing clarity: preparing graphics for presentation and publication\n4/2: No class (Well-being day)\n4/9 (Week 12): Visualizing and understanding fit (and misfit) of statistical models\n4/16 (Week 13): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction\n4/23 (Week 14): Final presentations of data projects\n\n\n\nObtaining course materials\nTo obtain the full materials for this class, use git clone to download the course repository:\ngit clone https://github.com/michaelhallquist/dataviz_spr2026.git\nIf you already cloned a local copy of the repo, you can get the latest updates using git pull. If all of this git stuff is foreign, I would recommend a quick skim of this documentation: https://happygitwithr.com."
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html",
    "title": "Tidy data overview",
    "section": "",
    "text": "Most data wrangling can be accomplished using data.frame object (or tbl objects in dplyr). These objects consist of rows and columns, forming a rectangular structure.\n\n\nCode\ngapminder %&gt;% kable_table(n=6)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\nA variable contains all values measuring an attribute (e.g., neuroticism) across units (e.g., people).\nColumns in data.frames are typically labeled and represent variables.\nMoreover, in a data.frame, all values in a given column should have the same data type, such as character strings. But a data.frame is different from a matrix object because columns can differ in terms of data type. For example, in the gapminder, the country column is a factor, whereas lifeExp is a numeric column.\n“A data frame is a list of vectors that R displays as a table. When your data is tidy, the values of each variable fall in their own column vector.” -Garrett Grolemund Citation\n\n\n\nAn observation contains all values measured on the same unit across attributes.\nRows in a data.frame typically represent observations.\n\n\nCode\ngapminder %&gt;% kable_table(n=1)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#variables",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#variables",
    "title": "Tidy data overview",
    "section": "",
    "text": "A variable contains all values measuring an attribute (e.g., neuroticism) across units (e.g., people).\nColumns in data.frames are typically labeled and represent variables.\nMoreover, in a data.frame, all values in a given column should have the same data type, such as character strings. But a data.frame is different from a matrix object because columns can differ in terms of data type. For example, in the gapminder, the country column is a factor, whereas lifeExp is a numeric column.\n“A data frame is a list of vectors that R displays as a table. When your data is tidy, the values of each variable fall in their own column vector.” -Garrett Grolemund Citation"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#observations",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#observations",
    "title": "Tidy data overview",
    "section": "",
    "text": "An observation contains all values measured on the same unit across attributes.\nRows in a data.frame typically represent observations.\n\n\nCode\ngapminder %&gt;% kable_table(n=1)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#tidying-verbs",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#tidying-verbs",
    "title": "Tidy data overview",
    "section": "Tidying verbs",
    "text": "Tidying verbs\nThe tidyr package provides four core functions to aid in converting messy data into tidy form. We may also need functions from dplyr at times. Each of these verbs is also a function that transforms the dataset with the goal of making it more tidy.\n\nPivot_longer: combine multiple columns into a single column with a key-value pair format\nPivot_wider: divide key-value rows into multiple columns\nSeparate: split a single variable into multiple variables by pulling apart the values into pieces\nUnite: merge two variables (columns) into one, effectively pasting together the values\n\nNote: pivot_longer and pivot_wider are complements. And separate and unite are complements.\nDetails about the pivot functions can be found here: https://tidyr.tidyverse.org/articles/pivot.html.\nLet’s look at a series of datasets (from Wickham 2014) and consider how tidy or messy they are."
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_longer-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_longer-example",
    "title": "Tidy data overview",
    "section": "pivot_longer example",
    "text": "pivot_longer example\nHere is our first mess. Notice that the column headers are values, not variable names. This is untidy and hard to look at. We effectively have the data in a cross-tabulated format, but religion and income are not variables in the dataset.\n\nMessy version\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n\n\n\n\nAgnostic\n27\n34\n60\n81\n76\n137\n\n\nAtheist\n12\n27\n37\n52\n35\n70\n\n\nBuddhist\n27\n21\n30\n34\n33\n58\n\n\nCatholic\n418\n617\n732\n670\n638\n1116\n\n\nDon’t know/refused\n15\n14\n15\n11\n10\n35\n\n\nEvangelical Prot\n575\n869\n1064\n982\n881\n1486\n\n\nHindu\n1\n9\n7\n9\n11\n34\n\n\nHistorically Black Prot\n228\n244\n236\n238\n197\n223\n\n\nJehovah's Witness\n20\n27\n24\n24\n21\n30\n\n\nJewish\n19\n19\n25\n25\n30\n95\n\n\n\n\n\n\n\nTidy version\nIn the tidy version, religion and income become variables, and the number of observations in each religion x income combination is a frequency column. This is now tidy insofar as each value in the frequency column represents a unique combination of the religion and income factors, which are coded as variables.\n\n\n\n\n\nreligion\nincome\nfreq\n\n\n\n\nAgnostic\n$10-20k\n34\n\n\nAgnostic\n$100-150k\n109\n\n\nAgnostic\n$20-30k\n60\n\n\nAgnostic\n$30-40k\n81\n\n\nAgnostic\n$40-50k\n76\n\n\nAgnostic\n$50-75k\n137\n\n\nAgnostic\n$75-100k\n122\n\n\nAgnostic\n&lt;$10k\n27\n\n\nAgnostic\n&gt;150k\n84\n\n\nAgnostic\nDon't know/refused\n96\n\n\n\n\n\n\n\nTidying solution\nTo achieve the above transformation, we want to pivot_longer the many columns of income into a single income column.\ntidy1 &lt;- mess1 %&gt;% pivot_longer(cols=-religion, names_to=\"income\", values_to=\"freq\")\nHere, we tell tidyr that we wish to create a lookup (‘key’) column called income whose correponding values will be called freq (here, representing the frequency of this religion x income combination). Furthermore, as additional arguments to pivot_longer, we provide the columns (cols argument) that should be combined, representing levels of the key variable. By specifying -religion, we are saying ‘all columns except religion.’ The alternative would be to provide a comma-separate list of columns like this mess1 %&gt;% pivot_longer(names_to=\"income\", values_to=\"freq\", cols=c(\"&lt;$10k\", \"$10-20k\", etc.))"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_wider-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_wider-example",
    "title": "Tidy data overview",
    "section": "pivot_wider example",
    "text": "pivot_wider example\nIn our second mess, we have a weather dataset from the Global Historical Climatology Network for one weather station (MX17004) in Mexico. The data represent minimum and maximum temperatures measured across 31 days for five months. The days within each month are on the columns, the months are encoded as a variable month, and the min and max temperatures are separated by row, as identified by the element variable.\n\nMessy version\n\n\nCode\n#show a subset of columns that fit on the page\nmess2 %&gt;% dplyr::select(id:d13) %&gt;% kable_table(n=8)\n\n\n\n\n\nid\nyear\nmonth\nelement\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\nd11\nd12\nd13\n\n\n\n\nMX17004\n2010\n1\ntmax\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n1\ntmin\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n2\ntmax\nNA\n27.3\n24.1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n29.7\nNA\nNA\n\n\nMX17004\n2010\n2\ntmin\nNA\n14.4\n14.4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n13.4\nNA\nNA\n\n\nMX17004\n2010\n3\ntmax\nNA\nNA\nNA\nNA\n32.1\nNA\nNA\nNA\nNA\n34.5\nNA\nNA\nNA\n\n\nMX17004\n2010\n3\ntmin\nNA\nNA\nNA\nNA\n14.2\nNA\nNA\nNA\nNA\n16.8\nNA\nNA\nNA\n\n\nMX17004\n2010\n4\ntmax\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n4\ntmin\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nTidy version\n\n\n\n\n\nid\nyear\nmonth\nday\ntmax\ntmin\n\n\n\n\nMX17004\n2010\n1\n30\n27.8\n14.5\n\n\nMX17004\n2010\n2\n2\n27.3\n14.4\n\n\nMX17004\n2010\n2\n3\n24.1\n14.4\n\n\nMX17004\n2010\n2\n11\n29.7\n13.4\n\n\nMX17004\n2010\n2\n23\n29.9\n10.7\n\n\nMX17004\n2010\n3\n5\n32.1\n14.2\n\n\nMX17004\n2010\n3\n10\n34.5\n16.8\n\n\nMX17004\n2010\n3\n16\n31.1\n17.6\n\n\n\n\n\n\n\nTidying solution\nTo clean this up, we need to bring all of the day columns together using pivot_longer so that we can encode day as a variable and temperature as a variable.\nWe also may want to have max and min temperature as separate columns (i.e., variables), rather than keeping that as a key-value pair. That is, the tmin and tmax values denote the attributes of a single observation, which would usually be represented as separate variables in tidy format. To obtain min and max temperatures as separate columns, we use pivot_wider to move the element values onto separate columns.\nHere is the basic approach:\n#use num_range() to select variables called d1--d31\ntidy2 &lt;- mess2 %&gt;% \n  pivot_longer(\n    cols = num_range(\"d\", 1:31), \n    names_to = \"day\", \n    values_to = \"temperature\",\n    names_prefix = \"d\", #trim off the 'd'\n    names_transform = list(day = as.integer)\n  ) %&gt;%\n  pivot_wider(names_from = \"element\", values_from = \"temperature\") %&gt;%\n  na.omit()\nNotice that pivot_longer has a few built-in arguments for helping us trim off parts of the column names that are not data per se. Here, we have d1–d31, but only the numeric part of that is data. The names_prefix=\"d\" tells pivot_longer to trim the leading ‘d’ from every value in the day column. The names_transform=list(day=as.integer) argument then converts the resulting day values to integers so they behave like a numeric variable rather than text.\nHere, pivot_wider a key – element – that has values 'tmin' or 'tmax' and puts the values of these rows onto columns. This is a kind of ‘long-to-wide’ conversion and we would expect here for the number of rows in the dataset drop two-fold with the pivot_wider compared to the preceding step where we’ve gathered the day columns.\nIt is often useful to check the number of rows after each step in a data transformation pipeline. Here, I just break up the pipeline into the pivot_longer and pivot_wider steps and check the structure in between.\n\n\nCode\ntidy2.1 &lt;- mess2 %&gt;% \n  pivot_longer(\n    cols = num_range(\"d\", 1:31), names_to = \"day\", values_to = \"temperature\",\n    names_prefix = \"d\", names_transform = list(day = as.integer)\n  )\n\nnrow(tidy2.1)\n\n\n[1] 682\n\n\nCode\ntidy2.2 &lt;- tidy2.1 %&gt;%\n  pivot_wider(names_from = \"element\", values_from = \"temperature\")\n\n# with NAs included (since data are sparse), we get the expected 50% reduction in rows\nnrow(tidy2.2)\n\n\n[1] 341\n\n\nCode\n# there are only 33 useful/present observations\nnrow(tidy2.2 %&gt;% na.omit)\n\n\n[1] 33"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#missingness-implicit-vs-explicit",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#missingness-implicit-vs-explicit",
    "title": "Tidy data overview",
    "section": "Missingness: implicit vs explicit",
    "text": "Missingness: implicit vs explicit\nTidy data often needs missing values made explicit (or dropped) so analyses behave as intended.\n\n\nCode\ntidy_missing &lt;- tibble::tribble(\n  ~id, ~day, ~score,\n  1, \"Mon\", 10,\n  1, \"Wed\", 8,\n  2, \"Mon\", 9,\n  3, \"Mon\", NA_real_\n)\n\n# Drop rows where all pivoted values are NA\ntidy_missing %&gt;%\n  tidyr::pivot_wider(names_from = \"day\", values_from = \"score\") %&gt;%\n  dplyr::filter(!dplyr::if_all(-id, is.na))\n\n\n\n\n\n\nid\nMon\nWed\n\n\n\n\n1\n10\n8\n\n\n2\n9\nNA\n\n\n\n\n\n\nCode\n# Make implicit missing combinations explicit\ntidy_missing %&gt;%\n  tidyr::complete(id, day)\n\n\n\n\n\n\nid\nday\nscore\n\n\n\n\n1\nMon\n10\n\n\n1\nWed\n8\n\n\n2\nMon\n9\n\n\n2\nWed\nNA\n\n\n3\nMon\nNA\n\n\n3\nWed\nNA\n\n\n\n\n\n\nCode\n# Fill down within a group (e.g., carry forward metadata). Grouping ensures we\n# only fill within each id; ungroup to avoid carrying this into later steps.\ntidy_missing %&gt;%\n  tidyr::complete(id, day) %&gt;%\n  dplyr::group_by(id) %&gt;%\n  tidyr::fill(score, .direction = \"down\") %&gt;%\n  dplyr::ungroup()\n\n\n\n\n\n\nid\nday\nscore\n\n\n\n\n1\nMon\n10\n\n\n1\nWed\n8\n\n\n2\nMon\n9\n\n\n2\nWed\n9\n\n\n3\nMon\nNA\n\n\n3\nWed\nNA"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#separate-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#separate-example",
    "title": "Tidy data overview",
    "section": "separate example",
    "text": "separate example\nIn our third mess, we have multiple variables stored in one column. More specifically, in these data, the ‘m014’ etc. columns represent a combination of sex (m/f) and age range (e.g., 014 is 0–14). The country and year columns are ‘tidy’ because they represent variables, but the sex + age columns are not.\n\nMessy version\n\n\nCode\n#use select to select a few columns that can fit on the page\nmess3 %&gt;% dplyr::select(country:f1524) %&gt;% kable_table(n=5)\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\nf1524\n\n\n\n\nAD\n2000\n0\n0\n1\n0\n0\n0\n0\nNA\nNA\nNA\n\n\nAE\n2000\n2\n4\n4\n6\n5\n12\n10\nNA\n3\n16\n\n\nAF\n2000\n52\n228\n183\n149\n129\n94\n80\nNA\n93\n414\n\n\nAG\n2000\n0\n0\n0\n0\n0\n0\n1\nNA\n1\n1\n\n\nAL\n2000\n2\n19\n21\n14\n24\n19\n16\nNA\n3\n11\n\n\n\n\n\n\n\nTidy version\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n0-14\n0\n\n\nAE\n2000\nm\n0-14\n2\n\n\nAF\n2000\nm\n0-14\n52\n\n\nAG\n2000\nm\n0-14\n0\n\n\nAL\n2000\nm\n0-14\n2\n\n\nAM\n2000\nm\n0-14\n2\n\n\nAN\n2000\nm\n0-14\n0\n\n\nAO\n2000\nm\n0-14\n186\n\n\n\n\n\n\n\nTidying solution\nWe essentially need to parse apart the ‘m’ from the ‘014’ components of each value, which is a job for separate. Note that we also need to pivot_longer the wacky sex + age columns first to make this easier. Here I use cols=c(-country, -year) to say, ‘all columns except these.’\nThe sep argument of separate tells R how to split the values into multiple variables. Here, by using the number 1, we ask for the first character to become sex and the rest to become age_range.\n\n\nCode\ntidy3 &lt;- mess3 %&gt;% \n  pivot_longer(names_to=\"sex_age\", values_to=\"freq\", cols=c(-country, -year)) %&gt;%\n  separate(sex_age, into=c(\"sex\", \"age_range\"), sep=1)\n\n\nHere, we gathered all columns except country and year into a single key-value pair using pivot_longer. This is an intermediate stage of the dataset that is semi-tidy. We then separate the sex and age components of the values into different variables, resulting in a tidy dataset.\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n014\n0\n\n\nAD\n2000\nm\n1524\n0\n\n\nAD\n2000\nm\n2534\n1\n\n\nAD\n2000\nm\n3544\n0\n\n\nAD\n2000\nm\n4554\n0\n\n\nAD\n2000\nm\n5564\n0\n\n\nAD\n2000\nm\n65\n0\n\n\nAD\n2000\nm\nu\nNA\n\n\n\n\n\nThis is pretty close. The age_range variable is still a little clunky because it isn’t easy to read. We could modify this further using mutate and recode from dplyr, but that’s not the immediate emphasis here.\n\n\nCode\ntidy3 &lt;- tidy3 %&gt;% mutate(age_range=recode(age_range,\n                                           \"014\"=\"0-14\",\n                                           \"1524\"=\"15-24\",\n                                           \"2534\"=\"25-34\",\n                                           \"3544\"=\"35-44\",\n                                           \"4554\"=\"45-54\",\n                                           \"5564\"=\"55-64\",\n                                           \"65\"=\"65+\",\n                                           \"u\"=\"unknown\", .default=NA_character_\n))\n\n\n\n\nModern separate_* alternatives\nFor simple string splitting, tidyr now provides separate_wider_delim() (split on a delimiter) and separate_wider_regex() (split using a regex). These are often clearer than separate() because they create named columns directly.\n\n\nCode\ntoy_people &lt;- tibble::tibble(\n  person = c(\"Ada-Lovelace\", \"Grace-Hopper\", \"Katherine-Johnson\")\n)\n\ntoy_people %&gt;% separate_wider_delim(person, delim = \"-\", names = c(\"first\", \"last\"))\n\n\n\n\n\n\nfirst\nlast\n\n\n\n\nAda\nLovelace\n\n\nGrace\nHopper\n\n\nKatherine\nJohnson\n\n\n\n\n\n\nCode\ntoy_people %&gt;% separate_wider_regex(person, patterns = c(first = \"^[^-]+\", \"-\",\n                                                         last = \"[^-]+$\"\n))\n\n\n\n\n\n\nfirst\nlast\n\n\n\n\nAda\nLovelace\n\n\nGrace\nHopper\n\n\nKatherine\nJohnson\n\n\n\n\n\n\n\n\nTidying solution using pivot_longer alone\nI am ambivalent about whether it is useful to combine separable objectives into a single data wrangling verb. Nevertheless, I want to note that the pivot_longer function provides added functionality for both combining columns into a key-value pair format and splitting the key into multiple variables if the key variable is an amalgamation of discrete variables. This can allow us to skip the separate step:\n\n\nCode\ntidy3 &lt;- mess3 %&gt;% \n  pivot_longer(\n    cols = c(-country, -year), \n    names_to = c(\"sex\", \"age_range\"),\n    names_pattern = \"(.)(.*)\", #first character versus the rest\n    values_to = \"freq\",\n    names_ptypes = list(\n      age_range=factor(\n        levels=c(\"u\", \"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"),\n        ordered=TRUE\n      )\n    )\n  )\n\n#Note: we can't adjust the labels in the names_ptype above using labels=c(...).\n#Thus, we'd need to use recode_factor, similar to the above\ntidy3 &lt;- tidy3 %&gt;% mutate(\n  age_range=recode_factor(age_range,\n                          \"014\"=\"0-14\",\n                          \"1524\"=\"15-24\",\n                          \"2534\"=\"25-34\",\n                          \"3544\"=\"35-44\",\n                          \"4554\"=\"45-54\",\n                          \"5564\"=\"55-64\",\n                          \"65\"=\"65+\",\n                          \"u\"=\"unknown\", .default=NA_character_\n  ))\n\ntidy3 %&gt;% kable_table(n=8)\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n0-14\n0\n\n\nAD\n2000\nm\n15-24\n0\n\n\nAD\n2000\nm\n25-34\n1\n\n\nAD\n2000\nm\n35-44\n0\n\n\nAD\n2000\nm\n45-54\n0\n\n\nAD\n2000\nm\n55-64\n0\n\n\nAD\n2000\nm\n65+\n0\n\n\nAD\n2000\nm\nunknown\nNA"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#unite-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#unite-example",
    "title": "Tidy data overview",
    "section": "unite example",
    "text": "unite example\nAlthough the least common of the tidying verbs (in my experience), unite is the complement to separate and can be used to bring together multiple variables that we wish to store as a single variable. For example, we may have first name and last name stored in separate variables, but wish to put them together for display or exporting purposes. Sometimes, we also use unite as an intermediate stage in tidying, bringing together variables, reshaping the data, then re-separating them.\n\n\n\n\n\nfirst_name\nlast_name\nage\nfavorite_color\n\n\n\n\nGraham\nDoe\n11\nPurple\n\n\nKieran\nHelali\n9\nBlue\n\n\nCharlotte\nStafford\n11\nPink\n\n\n\n\n\n\nTidying solution\nIf we wanted to have a full_name, we could use unite to combine first_name and last_name and then get rid of those individual columns.\n\n\nCode\ndf4_united &lt;- df4 %&gt;% unite(col = \"full_name\", first_name, last_name, sep=\" \")\n\n\n\n\n\n\n\nfull_name\nage\nfavorite_color\n\n\n\n\nGraham Doe\n11\nPurple\n\n\nKieran Helali\n9\nBlue\n\n\nCharlotte Stafford\n11\nPink"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#data.table-meltdcast",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#data.table-meltdcast",
    "title": "Tidy data overview",
    "section": "data.table melt/dcast",
    "text": "data.table melt/dcast\nThe data.table package provides melt() and dcast() for fast reshaping. The formula interface is compact for multi-way summary tables, and dcast() can aggregate on the fly while casting.\n\n\nCode\ndt &lt;- data.table::data.table(\n  id = 1:4,\n  group = c(\"A\", \"A\", \"B\", \"B\"),\n  y2022 = c(10, 12, 9, 11),\n  y2023 = c(13, 14, 10, 12),\n  z2022 = c(100, 120, 90, 110),\n  z2023 = c(130, 140, 95, 115)\n)\n\nlong_dt &lt;- data.table::melt(\n  dt,\n  id.vars = c(\"id\", \"group\"),\n  measure.vars = patterns(\"^y\", \"^z\"),\n  variable.name = \"year\",\n  value.name = c(\"y\", \"z\")\n)\n\n# Map pattern indices to actual year labels.\nlong_dt$year &lt;- c(\"2022\", \"2023\")[as.integer(long_dt$year)]\nlong_dt\n\n\n\n\n\n\nid\ngroup\nyear\ny\nz\n\n\n\n\n1\nA\n2022\n10\n100\n\n\n2\nA\n2022\n12\n120\n\n\n3\nB\n2022\n9\n90\n\n\n4\nB\n2022\n11\n110\n\n\n1\nA\n2023\n13\n130\n\n\n2\nA\n2023\n14\n140\n\n\n3\nB\n2023\n10\n95\n\n\n4\nB\n2023\n12\n115\n\n\n\n\n\n\nCode\nmean_y &lt;- data.table::dcast(\n  long_dt,\n  group ~ year,\n  value.var = \"y\",\n  fun.aggregate = mean\n)\n\nmean_y\n\n\n\n\n\n\ngroup\n2022\n2023\n\n\n\n\nA\n11\n13.5\n\n\nB\n10\n11.0\n\n\n\n\n\n\nCode\nmean_multi &lt;- data.table::dcast(\n  long_dt,\n  group ~ year,\n  value.var = c(\"y\", \"z\"),\n  fun.aggregate = mean\n)\n\nmean_multi\n\n\n\n\n\n\ngroup\ny_2022\ny_2023\nz_2022\nz_2023\n\n\n\n\nA\n11\n13.5\n110\n135\n\n\nB\n10\n11.0\n100\n105\n\n\n\n\n\n\nCompared with tidyr, data.table::dcast() makes aggregation part of the casting step and can cast multiple value columns in one pass. It is also a common choice for very large tables where performance matters.\nFurther reshaping extensions using data.table package: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html"
  }
]