[
  {
    "objectID": "assignments/data_quality_assurance_processing_project_proposal_2026.html",
    "href": "assignments/data_quality_assurance_processing_project_proposal_2026.html",
    "title": "Data Quality Assurance and Processing Project Proposal",
    "section": "",
    "text": "PSYC 859 Spring 2026\n\nDue dates\n\nProposal: 1/29/2026\nProject submission: 2/12/2026\n\n\n\nProposal guidelines\nPlease submit a brief proposal (6-8 pages) describing the data import, processing, and quality assurance steps to prepare datasets under a conceptual umbrella (e.g., a research project) for data visualization and analysis. Ideally, please use a dataset that will help you in current and/or future projects. Use multiple files from the same project that can be merged or aligned (shared IDs, time points, or measures) to support a coherent visualization/analysis goal.\nFor the proposal document, feel free to be concise, make use of lists, etc. For example, under section 6 (processing scripts proposal), this may look more like a numbered outline than prose. Note that when you submit the project, data (if provided) should be deidentified.\n\n\nKey ingredients of the proposal\n\nBackground and rationale for project (1-2 paragraphs)\nDescription of data structure:\n\nTypes of data/data source. Please provide enough detail about the measures and collection procedures that a scientific reader understands the basics. For example: ECG data collected at 1000Hz using BIOPAC equipment over the course of a 5-minute Treer social stress test.\nNames and brief descriptions of key variables\nOriginal data storage format: file type, number of files, storage on file system (i.e., subdirectory structure)\n\nProposed file system structure for project\n\nPlan for maintaining unadulterated original data source\nPlan for organizing code, output, figures, and data on file system\nFor processed data, how will they be stored? Why is this optimal?\nRationale for how this plan will support understanding and accessibility for collaborators (or other researchers in the future)\nHow will you document decisions about the data processing and analysis for the project? How will you document the basic file system structure of the project?\n\nDescription of any data preprocessing steps that need to be performed outside of R (e.g., large existing codebase for denoising EEG data etc.)\nData quality assurance plan\n\nWhat are the potential sources of artifacts or errors in the data?\nWhat checks can be conducted to mitigate these problems using algorithmic methods (i.e., automated code that walks through a dataset diagnosing and reporting problems)?\nHow will missing data be identified and resolved? Resolution of missing values includes\n\nWhen possible, finding the missing data and entering it.\nDocumenting the missingness and, when possible, having a mechanism for quieting your QA script so that it does not perpetually yell at you.\n\nAre there QA steps that can only be conducted by a trained human (e.g., ECG artifact correction)? If so, briefly document why these cannot be automated or codified.\nRelatedly, can simple algorithms be included to enhance error detection for human follow-up? For example, in ECG data, consecutive changes of &gt; 300ms in the inter-beat interval may be indicative of an artifact to be checked by a human. Or in fMRI, a shift in the global mean of the signal 3SD+ above/below the temporal mean may be indicative of an artifact.\n\nData QA and processing scripts proposal: Building on the QA plan above, write out a simple algorithm (cookbook-style, roughly like a simple outline) for the order of steps in your data processing pipeline. Note that the number and organization of scripts will depend on your project. In general, try to write reasonably modular (functionally specialized) scripts. Core steps include:\n\nSetting the working directory (or how you will use an Rstudio project)\nDefining any variables that will be used throughout the script (e.g., absolute paths to output data)\nImport of original data\nBasic tidying of data, if necessary\nBasic QA checks on each dataset. What are the expected outputs that will document data quality?\nDataset merging, including fidelity checks on merge\nData manipulation/wrangling of merged data for analysis and visualization\n\nBriefly document functions that you anticipate writing to automate repetitive steps in the QA process (e.g., a function that checks for invalid data by looking for values outside of a given range).\nBriefly document functions that exist in publicly available R packages (e.g., dplyr, tidyr, purrr, editrules, validate) that you anticipate using to accomplish your project."
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html",
    "href": "materials/w03_automation/validate_bfi.html",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "",
    "text": "We validate a psychology survey dataset using validate and demonstrate detection, selection/isolation, and correction for:\n\nrationally invalid values (out-of-range)\nstatistically invalid values (odd response patterns)\nduplicated observations\nunexplained missing data"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#goal",
    "href": "materials/w03_automation/validate_bfi.html#goal",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "",
    "text": "We validate a psychology survey dataset using validate and demonstrate detection, selection/isolation, and correction for:\n\nrationally invalid values (out-of-range)\nstatistically invalid values (odd response patterns)\nduplicated observations\nunexplained missing data"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#setup",
    "href": "materials/w03_automation/validate_bfi.html#setup",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "Setup",
    "text": "Setup\n\n\nCode\n# Install if needed\npkgs &lt;- c(\"validate\", \"psych\", \"dplyr\")\nmissing &lt;- pkgs[!vapply(pkgs, requireNamespace, logical(1), quietly = TRUE)]\nif (length(missing) &gt; 0) install.packages(missing)\n\nlibrary(validate)\nlibrary(psych)\nlibrary(dplyr)\n\nrows_violating &lt;- function(cn) {\n  v &lt;- values(cn)\n  which(apply(v, 1, function(x) any(x == FALSE, na.rm = TRUE)))\n}"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#data",
    "href": "materials/w03_automation/validate_bfi.html#data",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "Data",
    "text": "Data\npsych::bfi contains Big Five Inventory items (1-6) plus age, gender, education.\n\n\nCode\nitems &lt;- c(paste0(\"A\", 1:5), paste0(\"C\", 1:5), paste0(\"E\", 1:5),\n           paste0(\"N\", 1:5), paste0(\"O\", 1:5))\n\nbfi_raw &lt;- psych::bfi\nbfi_dirty &lt;- bfi_raw\n\n# Inject a few issues so the checks are demonstrable\nbfi_dirty[1, \"A1\"] &lt;- 9\nbfi_dirty[2, \"C3\"] &lt;- 0\nbfi_dirty[3, \"age\"] &lt;- -5\nbfi_dirty[4, \"gender\"] &lt;- 3\nbfi_dirty[5, items] &lt;- 1\nbfi_dirty[6, items[1:10]] &lt;- NA\nbfi_dirty &lt;- rbind(bfi_dirty, bfi_dirty[1, ])"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#rationally-invalid-values",
    "href": "materials/w03_automation/validate_bfi.html#rationally-invalid-values",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "1) Rationally invalid values",
    "text": "1) Rationally invalid values\nDetection with range rules.\n\n\nCode\n# Row-level indicator: all items are within 1..6 or missing\nbfi_dirty$items_in_range &lt;- apply(bfi_dirty[items], 1, function(x) {\n  all(x %in% 1:6 | is.na(x))\n})\n\nrules_range &lt;- validator(\n  items_in_range == TRUE,\n  age &gt;= 16,\n  age &lt;= 100,\n  gender %in% c(1, 2) | is.na(gender),\n  education %in% 1:5 | is.na(education)\n)\n\nrange_cn &lt;- confront(bfi_dirty, rules_range)\nsummary(range_cn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2801\n2798\n3\n0\nFALSE\nFALSE\nitems_in_range == TRUE\n\n\nV2\n2801\n2713\n88\n0\nFALSE\nFALSE\nage - 16 &gt;= -1e-08\n\n\nV3\n2801\n2801\n0\n0\nFALSE\nFALSE\nage - 100 &lt;= 1e-08\n\n\nV4\n2801\n2800\n1\n0\nFALSE\nFALSE\ngender %vin% c(1, 2) | is.na(gender)\n\n\nV5\n2801\n2801\n0\n0\nFALSE\nFALSE\neducation %vin% 1:5 | is.na(education)\n\n\n\n\n\n\nCode\nbad_range &lt;- rows_violating(range_cn)\nhead(bfi_dirty[bad_range, c(\"age\", \"gender\", \"education\", \"A1\", \"C3\")])\n\n\n\n\n\n\n\nage\ngender\neducation\nA1\nC3\n\n\n\n\n61617\n16\n1\nNA\n9\n3\n\n\n61618\n18\n2\nNA\n2\n0\n\n\n61620\n-5\n2\nNA\n5\n4\n\n\n61621\n17\n3\nNA\n4\n3\n\n\n61670\n14\n2\nNA\n2\n4\n\n\n61780\n14\n2\nNA\n5\n6\n\n\n\n\n\n\nCorrection: set invalid values to NA.\n\n\nCode\nclip_to_na &lt;- function(x, lo, hi) {\n  x[x &lt; lo | x &gt; hi] &lt;- NA\n  x\n}\n\nbfi_fixed &lt;- bfi_dirty %&gt;%\n  mutate(\n    across(all_of(items), ~ clip_to_na(.x, 1, 6)),\n    age = clip_to_na(age, 16, 100),\n    gender = if_else(gender %in% c(1, 2), gender, NA_real_),\n    education = if_else(education %in% 1:5, education, NA_real_)\n  )"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#statistically-invalid-values-odd-response-patterns",
    "href": "materials/w03_automation/validate_bfi.html#statistically-invalid-values-odd-response-patterns",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "2) Statistically invalid values (odd response patterns)",
    "text": "2) Statistically invalid values (odd response patterns)\nLook for rows with no variation in items, suggesting flat response pattern\n\n\nCode\nbfi_fixed$row_sd &lt;- apply(bfi_fixed[items], 1, sd, na.rm = TRUE)\nbfi_fixed$n_missing &lt;- rowSums(is.na(bfi_fixed[items]))\n\nrules_stat &lt;- validator(\n  row_sd &gt; 0,\n  n_missing &lt;= 5\n)\n\nstat_cn &lt;- confront(bfi_fixed, rules_stat)\nsummary(stat_cn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2801\n2795\n6\n0\nFALSE\nFALSE\nrow_sd &gt; 0\n\n\nV2\n2801\n2794\n7\n0\nFALSE\nFALSE\nn_missing - 5 &lt;= 1e-08\n\n\n\n\n\n\nCode\nbad_stat &lt;- rows_violating(stat_cn)\nhead(bfi_fixed[bad_stat, c(\"row_sd\", \"n_missing\")])\n\n\n\n\n\n\n\nrow_sd\nn_missing\n\n\n\n\n61622\n0.000000\n0\n\n\n61623\n1.804756\n10\n\n\n62783\n0.000000\n0\n\n\n63030\n1.779513\n15\n\n\n63991\n0.000000\n15\n\n\n64642\n0.000000\n0\n\n\n\n\n\n\nCorrection: remove cases with implausible patterns.\n\n\nCode\nbfi_fixed &lt;- bfi_fixed %&gt;%\n  filter(row_sd &gt; 0, n_missing &lt;= 5)"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#duplicated-observations",
    "href": "materials/w03_automation/validate_bfi.html#duplicated-observations",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "3) Duplicated observations",
    "text": "3) Duplicated observations\nDetection using exact duplicate rows on items + demographics.\n\n\nCode\ndup_key &lt;- c(items, \"age\", \"gender\", \"education\")\nbfi_fixed$dup_row &lt;- duplicated(bfi_fixed[dup_key])\n\nrules_dup &lt;- validator(!dup_row)\n\ndup_cn &lt;- confront(bfi_fixed, rules_dup)\nsummary(dup_cn)\n\n\n\n\n\n\nerror\nwarning\n\n\n\n\n\n\n\n\nCode\nbfi_fixed &lt;- bfi_fixed %&gt;%\n  filter(!dup_row)"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#unexplained-missing-data",
    "href": "materials/w03_automation/validate_bfi.html#unexplained-missing-data",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "4) Unexplained missing data",
    "text": "4) Unexplained missing data\nDetection of missing demographics.\n\n\nCode\nmissing_rules &lt;- validator(\n  !is.na(age),\n  !is.na(gender)\n)\n\nmissing_cn &lt;- confront(bfi_fixed, missing_rules)\nsummary(missing_cn)\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2788\n2702\n86\n0\nFALSE\nFALSE\n!is.na(age)\n\n\nV2\n2788\n2787\n1\n0\nFALSE\nFALSE\n!is.na(gender)\n\n\n\n\n\n\nCode\nbad_missing &lt;- rows_violating(missing_cn)\nhead(bfi_fixed[bad_missing, c(\"age\", \"gender\")])\n\n\n\n\n\n\n\nage\ngender\n\n\n\n\n61620\nNA\n2\n\n\n61621\n17\nNA\n\n\n61670\nNA\n2\n\n\n61780\nNA\n2\n\n\n62151\nNA\n2\n\n\n62360\nNA\n2\n\n\n\n\n\n\nCorrection: remove cases with missing key demographics and impute remaining item missings.\n\n\nCode\nbfi_fixed &lt;- bfi_fixed %&gt;%\n  filter(!is.na(age), !is.na(gender)) %&gt;%\n  mutate(across(all_of(items), ~ replace(.x, is.na(.x), median(.x, na.rm = TRUE))))"
  },
  {
    "objectID": "materials/w03_automation/validate_bfi.html#final-consistency-check",
    "href": "materials/w03_automation/validate_bfi.html#final-consistency-check",
    "title": "Compact Data Validation with validate (Psychology Survey Example)",
    "section": "Final consistency check",
    "text": "Final consistency check\n\n\nCode\n# Recompute indicators after corrections\nbfi_fixed$items_in_range &lt;- apply(bfi_fixed[items], 1, function(x) {\n  all(x %in% 1:6 | is.na(x))\n})\nbfi_fixed$row_sd &lt;- apply(bfi_fixed[items], 1, sd, na.rm = TRUE)\nbfi_fixed$n_missing &lt;- rowSums(is.na(bfi_fixed[items]))\nbfi_fixed$dup_row &lt;- duplicated(bfi_fixed[dup_key])\n\nfinal_rules &lt;- validator(\n  items_in_range == TRUE,\n  age &gt;= 16, age &lt;= 100,\n  gender %in% c(1, 2),\n  education %in% 1:5 | is.na(education),\n  row_sd &gt; 0,\n  n_missing &lt;= 5,\n  !dup_row\n)\n\nfinal_cn &lt;- confront(bfi_fixed, final_rules)\nsummary(final_cn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nitems\npasses\nfails\nnNA\nerror\nwarning\nexpression\n\n\n\n\nV1\n2701\n2701\n0\n0\nFALSE\nFALSE\nitems_in_range == TRUE\n\n\nV2\n2701\n2701\n0\n0\nFALSE\nFALSE\nage - 16 &gt;= -1e-08\n\n\nV3\n2701\n2701\n0\n0\nFALSE\nFALSE\nage - 100 &lt;= 1e-08\n\n\nV4\n2701\n2701\n0\n0\nFALSE\nFALSE\ngender %vin% c(1, 2)\n\n\nV5\n2701\n2701\n0\n0\nFALSE\nFALSE\neducation %vin% 1:5 | is.na(education)\n\n\nV6\n2701\n2701\n0\n0\nFALSE\nFALSE\nrow_sd &gt; 0\n\n\nV7\n2701\n2701\n0\n0\nFALSE\nFALSE\nn_missing - 5 &lt;= 1e-08\n\n\n\n\n\n\nThis compact workflow uses detection (rules + confront), isolation (violating), and correction (recoding to NA, removal of implausible cases, and simple imputation) to achieve consistent data."
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html",
    "href": "materials/w02_wrangling/joins_tutorial.html",
    "title": "Joins Tutorial",
    "section": "",
    "text": "This document has been adapted and extended by Michael Hallquist and Benjamin Johnson from Jenny Bryan’s dplyr joins tutorial (http://stat545.com/bit001_dplyr-cheatsheet.html). Animations were developed by Garrick Aden-Buie. The goal is to develop an intuition of the four major types of two-table join operations: inner, left, right, and full. We’ll also get into using joins to identify areas of match or mismatch between two datasets (using semi- and anti-joins).\n\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n\n\nStorm\ngood\nfemale\nMarvel\n\n\nMystique\nbad\nfemale\nMarvel\n\n\nBatman\ngood\nmale\nDC\n\n\nJoker\nbad\nmale\nDC\n\n\nCatwoman\nbad\nfemale\nDC\n\n\nHellboy\ngood\nmale\nDark Horse Comics\n\n\n\n\n\n\n\n\n\n\n\n\n\npublisher\nyr_founded\n\n\n\n\nDC\n1934\n\n\nMarvel\n1939\n\n\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#superheroes-table",
    "href": "materials/w02_wrangling/joins_tutorial.html#superheroes-table",
    "title": "Joins Tutorial",
    "section": "",
    "text": "name\nalignment\ngender\npublisher\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n\n\nStorm\ngood\nfemale\nMarvel\n\n\nMystique\nbad\nfemale\nMarvel\n\n\nBatman\ngood\nmale\nDC\n\n\nJoker\nbad\nmale\nDC\n\n\nCatwoman\nbad\nfemale\nDC\n\n\nHellboy\ngood\nmale\nDark Horse Comics"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#publishers-table",
    "href": "materials/w02_wrangling/joins_tutorial.html#publishers-table",
    "title": "Joins Tutorial",
    "section": "",
    "text": "publisher\nyr_founded\n\n\n\n\nDC\n1934\n\n\nMarvel\n1939\n\n\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#inner-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#inner-join",
    "title": "Joins Tutorial",
    "section": "inner join",
    "text": "inner join\nRequire match in both datasets (non-matching rows are dropped)\nFor those of you who are visual learners, conceptually, imagine the following two simple datasets:\n\nAn inner join combines the two datasets and drops the non-matching rows like so:\n\nLet’s try it with our superhero data.\n\n\nCode\n#*NB*: Here, we retain the joined dataset as ijsp\nijsp &lt;- inner_join(x=superheroes, y=publishers)\nprint(ijsp)\n\n\n# A tibble: 6 × 5\n  name     alignment gender publisher yr_founded\n  &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;\n1 Magneto  bad       male   Marvel          1939\n2 Storm    good      female Marvel          1939\n3 Mystique bad       female Marvel          1939\n4 Batman   good      male   DC              1934\n5 Joker    bad       male   DC              1934\n6 Catwoman bad       female DC              1934\n\n\nSame idea, just explicit declaration of key (i.e., “publisher”)\n\n\nCode\n#note that we've cut the x=, y= as this optional\ninner_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\n\n\n\n\nNotice both Hellboy (from the superheroes dataset) and Image comics (from the publishers dataset) were dropped."
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#left-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#left-join",
    "title": "Joins Tutorial",
    "section": "left join",
    "text": "left join\nKeep all rows in left-hand ‘x’ dataset (i.e., superheroes). Add columns from publishers where there is a match. Fill in NA for non-matching observations.\n\n\n\nCode\nleft_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\nHellboy\ngood\nmale\nDark Horse Comics\nNA"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#right-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#right-join",
    "title": "Joins Tutorial",
    "section": "right join",
    "text": "right join\nKeep all rows in right-hand ‘y’ dataset (i.e., publishers). Add columns from superheroes where there is a match. Fill in NA for non-matching observations.\n\n\n\nCode\n# Note the shift to using dplyr piping\n# This achieves the same purpose, but may be preferred by those who love pipes\nsuperheroes %&gt;% right_join(publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\nNA\nNA\nNA\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#full-join",
    "href": "materials/w02_wrangling/joins_tutorial.html#full-join",
    "title": "Joins Tutorial",
    "section": "full join",
    "text": "full join\nKeep all rows in left-hand ‘x’ (superheroes) and right-hand ‘y’ (publishers) datasets.\nResulting dataset will have all columns of both datasets, but filling in NA for any non-matches on either side (denoted as blank spaces below).\n\n\n\nCode\nsuperheroes %&gt;% full_join(publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\nyr_founded\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n1939\n\n\nStorm\ngood\nfemale\nMarvel\n1939\n\n\nMystique\nbad\nfemale\nMarvel\n1939\n\n\nBatman\ngood\nmale\nDC\n1934\n\n\nJoker\nbad\nmale\nDC\n1934\n\n\nCatwoman\nbad\nfemale\nDC\n1934\n\n\nHellboy\ngood\nmale\nDark Horse Comics\nNA\n\n\nNA\nNA\nNA\nImage\n1992"
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#semi_join",
    "href": "materials/w02_wrangling/joins_tutorial.html#semi_join",
    "title": "Joins Tutorial",
    "section": "semi_join",
    "text": "semi_join\nretain observations (rows) in x that match in y\n\n\nObservations in superheroes that match in publishers\nNotice that this is different from the left_join shown above as the data from y is not kept. That is the fundamental difference between ‘mutating joins’ (e.g., left_join) and ‘filtering joins’ (e.g., semi_join).\n\n\nCode\nsemi_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\n\n\n\n\nMagneto\nbad\nmale\nMarvel\n\n\nStorm\ngood\nfemale\nMarvel\n\n\nMystique\nbad\nfemale\nMarvel\n\n\nBatman\ngood\nmale\nDC\n\n\nJoker\nbad\nmale\nDC\n\n\nCatwoman\nbad\nfemale\nDC\n\n\n\n\n\n\n\n\nObservations in publishers that match in superheroes\n\n\nCode\nsemi_join(publishers, superheroes, by=\"publisher\")\n\n\n\n\n\n\npublisher\nyr_founded\n\n\n\n\nDC\n1934\n\n\nMarvel\n1939\n\n\n\n\n\n\nThis can be useful if you have a dataset of your data of interest and another dataset that indicates which of your participants/observations you want to remove or filter out."
  },
  {
    "objectID": "materials/w02_wrangling/joins_tutorial.html#anti_join",
    "href": "materials/w02_wrangling/joins_tutorial.html#anti_join",
    "title": "Joins Tutorial",
    "section": "anti_join",
    "text": "anti_join\nobservations in x that are not matched in y Note that this is similar to setdiff in base R\n\n\nObservations in superheroes that don’t match in publishers\n\n\nCode\nanti_join(superheroes, publishers, by=\"publisher\")\n\n\n\n\n\n\nname\nalignment\ngender\npublisher\n\n\n\n\nHellboy\ngood\nmale\nDark Horse Comics\n\n\n\n\n\n\n\n\nObservations in publishers that don’t match in superheroes\n\n\nCode\npublishers %&gt;% anti_join(superheroes, by=\"publisher\")\n\n\n\n\n\n\npublisher\nyr_founded\n\n\n\n\nImage\n1992\n\n\n\n\n\n\nThis can be useful if you are trying to identify extra participants/observations that may have snuck into one dataset (x) or been deleted in another (y)."
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html",
    "title": "Tidy data overview",
    "section": "",
    "text": "Most data wrangling can be accomplished using data.frame object (or tbl objects in dplyr). These objects consist of rows and columns, forming a rectangular structure.\n\n\nCode\ngapminder %&gt;% kable_table(n=6)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453\n\n\nAfghanistan\nAsia\n1957\n30.332\n9240934\n820.8530\n\n\nAfghanistan\nAsia\n1962\n31.997\n10267083\n853.1007\n\n\nAfghanistan\nAsia\n1967\n34.020\n11537966\n836.1971\n\n\nAfghanistan\nAsia\n1972\n36.088\n13079460\n739.9811\n\n\nAfghanistan\nAsia\n1977\n38.438\n14880372\n786.1134\n\n\n\n\n\n\n\nA variable contains all values measuring an attribute (e.g., neuroticism) across units (e.g., people).\nColumns in data.frames are typically labeled and represent variables.\nMoreover, in a data.frame, all values in a given column should have the same data type, such as character strings. But a data.frame is different from a matrix object because columns can differ in terms of data type. For example, in the gapminder, the country column is a factor, whereas lifeExp is a numeric column.\n“A data frame is a list of vectors that R displays as a table. When your data is tidy, the values of each variable fall in their own column vector.” -Garrett Grolemund Citation\n\n\n\nAn observation contains all values measured on the same unit across attributes.\nRows in a data.frame typically represent observations.\n\n\nCode\ngapminder %&gt;% kable_table(n=1)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#variables",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#variables",
    "title": "Tidy data overview",
    "section": "",
    "text": "A variable contains all values measuring an attribute (e.g., neuroticism) across units (e.g., people).\nColumns in data.frames are typically labeled and represent variables.\nMoreover, in a data.frame, all values in a given column should have the same data type, such as character strings. But a data.frame is different from a matrix object because columns can differ in terms of data type. For example, in the gapminder, the country column is a factor, whereas lifeExp is a numeric column.\n“A data frame is a list of vectors that R displays as a table. When your data is tidy, the values of each variable fall in their own column vector.” -Garrett Grolemund Citation"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#observations",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#observations",
    "title": "Tidy data overview",
    "section": "",
    "text": "An observation contains all values measured on the same unit across attributes.\nRows in a data.frame typically represent observations.\n\n\nCode\ngapminder %&gt;% kable_table(n=1)\n\n\n\n\n\ncountry\ncontinent\nyear\nlifeExp\npop\ngdpPercap\n\n\n\n\nAfghanistan\nAsia\n1952\n28.801\n8425333\n779.4453"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#tidying-verbs",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#tidying-verbs",
    "title": "Tidy data overview",
    "section": "Tidying verbs",
    "text": "Tidying verbs\nThe tidyr package provides four core functions to aid in converting messy data into tidy form. We may also need functions from dplyr at times. Each of these verbs is also a function that transforms the dataset with the goal of making it more tidy.\n\nPivot_longer: combine multiple columns into a single column with a key-value pair format\nPivot_wider: divide key-value rows into multiple columns\nSeparate: split a single variable into multiple variables by pulling apart the values into pieces\nUnite: merge two variables (columns) into one, effectively pasting together the values\n\nNote: pivot_longer and pivot_wider are complements. And separate and unite are complements.\nDetails about the pivot functions can be found here: https://tidyr.tidyverse.org/articles/pivot.html.\nLet’s look at a series of datasets (from Wickham 2014) and consider how tidy or messy they are."
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_longer-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_longer-example",
    "title": "Tidy data overview",
    "section": "pivot_longer example",
    "text": "pivot_longer example\nHere is our first mess. Notice that the column headers are values, not variable names. This is untidy and hard to look at. We effectively have the data in a cross-tabulated format, but religion and income are not variables in the dataset.\n\nMessy version\n\n\n\n\n\nreligion\n&lt;$10k\n$10-20k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n\n\n\n\nAgnostic\n27\n34\n60\n81\n76\n137\n\n\nAtheist\n12\n27\n37\n52\n35\n70\n\n\nBuddhist\n27\n21\n30\n34\n33\n58\n\n\nCatholic\n418\n617\n732\n670\n638\n1116\n\n\nDon’t know/refused\n15\n14\n15\n11\n10\n35\n\n\nEvangelical Prot\n575\n869\n1064\n982\n881\n1486\n\n\nHindu\n1\n9\n7\n9\n11\n34\n\n\nHistorically Black Prot\n228\n244\n236\n238\n197\n223\n\n\nJehovah's Witness\n20\n27\n24\n24\n21\n30\n\n\nJewish\n19\n19\n25\n25\n30\n95\n\n\n\n\n\n\n\nTidy version\nIn the tidy version, religion and income become variables, and the number of observations in each religion x income combination is a frequency column. This is now tidy insofar as each value in the frequency column represents a unique combination of the religion and income factors, which are coded as variables.\n\n\n\n\n\nreligion\nincome\nfreq\n\n\n\n\nAgnostic\n$10-20k\n34\n\n\nAgnostic\n$100-150k\n109\n\n\nAgnostic\n$20-30k\n60\n\n\nAgnostic\n$30-40k\n81\n\n\nAgnostic\n$40-50k\n76\n\n\nAgnostic\n$50-75k\n137\n\n\nAgnostic\n$75-100k\n122\n\n\nAgnostic\n&lt;$10k\n27\n\n\nAgnostic\n&gt;150k\n84\n\n\nAgnostic\nDon't know/refused\n96\n\n\n\n\n\n\n\nTidying solution\nTo achieve the above transformation, we want to pivot_longer the many columns of income into a single income column.\ntidy1 &lt;- mess1 %&gt;% pivot_longer(cols=-religion, names_to=\"income\", values_to=\"freq\")\nHere, we tell tidyr that we wish to create a lookup (‘key’) column called income whose correponding values will be called freq (here, representing the frequency of this religion x income combination). Furthermore, as additional arguments to pivot_longer, we provide the columns (cols argument) that should be combined, representing levels of the key variable. By specifying -religion, we are saying ‘all columns except religion.’ The alternative would be to provide a comma-separate list of columns like this mess1 %&gt;% pivot_longer(names_to=\"income\", values_to=\"freq\", cols=c(\"&lt;$10k\", \"$10-20k\", etc.))"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_wider-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#pivot_wider-example",
    "title": "Tidy data overview",
    "section": "pivot_wider example",
    "text": "pivot_wider example\nIn our second mess, we have a weather dataset from the Global Historical Climatology Network for one weather station (MX17004) in Mexico. The data represent minimum and maximum temperatures measured across 31 days for five months. The days within each month are on the columns, the months are encoded as a variable month, and the min and max temperatures are separated by row, as identified by the element variable.\n\nMessy version\n\n\nCode\n#show a subset of columns that fit on the page\nmess2 %&gt;% dplyr::select(id:d13) %&gt;% kable_table(n=8)\n\n\n\n\n\nid\nyear\nmonth\nelement\nd1\nd2\nd3\nd4\nd5\nd6\nd7\nd8\nd9\nd10\nd11\nd12\nd13\n\n\n\n\nMX17004\n2010\n1\ntmax\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n1\ntmin\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n2\ntmax\nNA\n27.3\n24.1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n29.7\nNA\nNA\n\n\nMX17004\n2010\n2\ntmin\nNA\n14.4\n14.4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n13.4\nNA\nNA\n\n\nMX17004\n2010\n3\ntmax\nNA\nNA\nNA\nNA\n32.1\nNA\nNA\nNA\nNA\n34.5\nNA\nNA\nNA\n\n\nMX17004\n2010\n3\ntmin\nNA\nNA\nNA\nNA\n14.2\nNA\nNA\nNA\nNA\n16.8\nNA\nNA\nNA\n\n\nMX17004\n2010\n4\ntmax\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMX17004\n2010\n4\ntmin\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nTidy version\n\n\n\n\n\nid\nyear\nmonth\nday\ntmax\ntmin\n\n\n\n\nMX17004\n2010\n1\n30\n27.8\n14.5\n\n\nMX17004\n2010\n2\n2\n27.3\n14.4\n\n\nMX17004\n2010\n2\n3\n24.1\n14.4\n\n\nMX17004\n2010\n2\n11\n29.7\n13.4\n\n\nMX17004\n2010\n2\n23\n29.9\n10.7\n\n\nMX17004\n2010\n3\n5\n32.1\n14.2\n\n\nMX17004\n2010\n3\n10\n34.5\n16.8\n\n\nMX17004\n2010\n3\n16\n31.1\n17.6\n\n\n\n\n\n\n\nTidying solution\nTo clean this up, we need to bring all of the day columns together using pivot_longer so that we can encode day as a variable and temperature as a variable.\nWe also may want to have max and min temperature as separate columns (i.e., variables), rather than keeping that as a key-value pair. That is, the tmin and tmax values denote the attributes of a single observation, which would usually be represented as separate variables in tidy format. To obtain min and max temperatures as separate columns, we use pivot_wider to move the element values onto separate columns.\nHere is the basic approach:\n#use num_range() to select variables called d1--d31\ntidy2 &lt;- mess2 %&gt;% \n  pivot_longer(\n    cols = num_range(\"d\", 1:31), \n    names_to = \"day\", \n    values_to = \"temperature\",\n    names_prefix = \"d\", #trim off the 'd'\n    names_transform = list(day = as.integer)\n  ) %&gt;%\n  pivot_wider(names_from = \"element\", values_from = \"temperature\") %&gt;%\n  na.omit()\nNotice that pivot_longer has a few built-in arguments for helping us trim off parts of the column names that are not data per se. Here, we have d1–d31, but only the numeric part of that is data. The names_prefix=\"d\" tells pivot_longer to trim the leading ‘d’ from every value in the day column. The names_transform=list(day=as.integer) argument then converts the resulting day values to integers so they behave like a numeric variable rather than text.\nHere, pivot_wider a key – element – that has values 'tmin' or 'tmax' and puts the values of these rows onto columns. This is a kind of ‘long-to-wide’ conversion and we would expect here for the number of rows in the dataset drop two-fold with the pivot_wider compared to the preceding step where we’ve gathered the day columns.\nIt is often useful to check the number of rows after each step in a data transformation pipeline. Here, I just break up the pipeline into the pivot_longer and pivot_wider steps and check the structure in between.\n\n\nCode\ntidy2.1 &lt;- mess2 %&gt;% \n  pivot_longer(\n    cols = num_range(\"d\", 1:31), names_to = \"day\", values_to = \"temperature\",\n    names_prefix = \"d\", names_transform = list(day = as.integer)\n  )\n\nnrow(tidy2.1)\n\n\n[1] 682\n\n\nCode\ntidy2.2 &lt;- tidy2.1 %&gt;%\n  pivot_wider(names_from = \"element\", values_from = \"temperature\")\n\n# with NAs included (since data are sparse), we get the expected 50% reduction in rows\nnrow(tidy2.2)\n\n\n[1] 341\n\n\nCode\n# there are only 33 useful/present observations\nnrow(tidy2.2 %&gt;% na.omit)\n\n\n[1] 33"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#missingness-implicit-vs-explicit",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#missingness-implicit-vs-explicit",
    "title": "Tidy data overview",
    "section": "Missingness: implicit vs explicit",
    "text": "Missingness: implicit vs explicit\nTidy data often needs missing values made explicit (or dropped) so analyses behave as intended.\n\n\nCode\ntidy_missing &lt;- tibble::tribble(\n  ~id, ~day, ~score,\n  1, \"Mon\", 10,\n  1, \"Wed\", 8,\n  2, \"Mon\", 9,\n  3, \"Mon\", NA_real_\n)\n\n# Drop rows where all pivoted values are NA\ntidy_missing %&gt;%\n  tidyr::pivot_wider(names_from = \"day\", values_from = \"score\") %&gt;%\n  dplyr::filter(!dplyr::if_all(-id, is.na))\n\n\n\n\n\n\nid\nMon\nWed\n\n\n\n\n1\n10\n8\n\n\n2\n9\nNA\n\n\n\n\n\n\nCode\n# Make implicit missing combinations explicit\ntidy_missing %&gt;%\n  tidyr::complete(id, day)\n\n\n\n\n\n\nid\nday\nscore\n\n\n\n\n1\nMon\n10\n\n\n1\nWed\n8\n\n\n2\nMon\n9\n\n\n2\nWed\nNA\n\n\n3\nMon\nNA\n\n\n3\nWed\nNA\n\n\n\n\n\n\nCode\n# Fill down within a group (e.g., carry forward metadata). Grouping ensures we\n# only fill within each id; ungroup to avoid carrying this into later steps.\ntidy_missing %&gt;%\n  tidyr::complete(id, day) %&gt;%\n  dplyr::group_by(id) %&gt;%\n  tidyr::fill(score, .direction = \"down\") %&gt;%\n  dplyr::ungroup()\n\n\n\n\n\n\nid\nday\nscore\n\n\n\n\n1\nMon\n10\n\n\n1\nWed\n8\n\n\n2\nMon\n9\n\n\n2\nWed\n9\n\n\n3\nMon\nNA\n\n\n3\nWed\nNA"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#separate-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#separate-example",
    "title": "Tidy data overview",
    "section": "separate example",
    "text": "separate example\nIn our third mess, we have multiple variables stored in one column. More specifically, in these data, the ‘m014’ etc. columns represent a combination of sex (m/f) and age range (e.g., 014 is 0–14). The country and year columns are ‘tidy’ because they represent variables, but the sex + age columns are not.\n\nMessy version\n\n\nCode\n#use select to select a few columns that can fit on the page\nmess3 %&gt;% dplyr::select(country:f1524) %&gt;% kable_table(n=5)\n\n\n\n\n\ncountry\nyear\nm014\nm1524\nm2534\nm3544\nm4554\nm5564\nm65\nmu\nf014\nf1524\n\n\n\n\nAD\n2000\n0\n0\n1\n0\n0\n0\n0\nNA\nNA\nNA\n\n\nAE\n2000\n2\n4\n4\n6\n5\n12\n10\nNA\n3\n16\n\n\nAF\n2000\n52\n228\n183\n149\n129\n94\n80\nNA\n93\n414\n\n\nAG\n2000\n0\n0\n0\n0\n0\n0\n1\nNA\n1\n1\n\n\nAL\n2000\n2\n19\n21\n14\n24\n19\n16\nNA\n3\n11\n\n\n\n\n\n\n\nTidy version\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n0-14\n0\n\n\nAE\n2000\nm\n0-14\n2\n\n\nAF\n2000\nm\n0-14\n52\n\n\nAG\n2000\nm\n0-14\n0\n\n\nAL\n2000\nm\n0-14\n2\n\n\nAM\n2000\nm\n0-14\n2\n\n\nAN\n2000\nm\n0-14\n0\n\n\nAO\n2000\nm\n0-14\n186\n\n\n\n\n\n\n\nTidying solution\nWe essentially need to parse apart the ‘m’ from the ‘014’ components of each value, which is a job for separate. Note that we also need to pivot_longer the wacky sex + age columns first to make this easier. Here I use cols=c(-country, -year) to say, ‘all columns except these.’\nThe sep argument of separate tells R how to split the values into multiple variables. Here, by using the number 1, we ask for the first character to become sex and the rest to become age_range.\n\n\nCode\ntidy3 &lt;- mess3 %&gt;% \n  pivot_longer(names_to=\"sex_age\", values_to=\"freq\", cols=c(-country, -year)) %&gt;%\n  separate(sex_age, into=c(\"sex\", \"age_range\"), sep=1)\n\n\nHere, we gathered all columns except country and year into a single key-value pair using pivot_longer. This is an intermediate stage of the dataset that is semi-tidy. We then separate the sex and age components of the values into different variables, resulting in a tidy dataset.\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n014\n0\n\n\nAD\n2000\nm\n1524\n0\n\n\nAD\n2000\nm\n2534\n1\n\n\nAD\n2000\nm\n3544\n0\n\n\nAD\n2000\nm\n4554\n0\n\n\nAD\n2000\nm\n5564\n0\n\n\nAD\n2000\nm\n65\n0\n\n\nAD\n2000\nm\nu\nNA\n\n\n\n\n\nThis is pretty close. The age_range variable is still a little clunky because it isn’t easy to read. We could modify this further using mutate and recode from dplyr, but that’s not the immediate emphasis here.\n\n\nCode\ntidy3 &lt;- tidy3 %&gt;% mutate(age_range=recode(age_range,\n                                           \"014\"=\"0-14\",\n                                           \"1524\"=\"15-24\",\n                                           \"2534\"=\"25-34\",\n                                           \"3544\"=\"35-44\",\n                                           \"4554\"=\"45-54\",\n                                           \"5564\"=\"55-64\",\n                                           \"65\"=\"65+\",\n                                           \"u\"=\"unknown\", .default=NA_character_\n))\n\n\n\n\nModern separate_* alternatives\nFor simple string splitting, tidyr now provides separate_wider_delim() (split on a delimiter) and separate_wider_regex() (split using a regex). These are often clearer than separate() because they create named columns directly.\n\n\nCode\ntoy_people &lt;- tibble::tibble(\n  person = c(\"Ada-Lovelace\", \"Grace-Hopper\", \"Katherine-Johnson\")\n)\n\ntoy_people %&gt;% separate_wider_delim(person, delim = \"-\", names = c(\"first\", \"last\"))\n\n\n\n\n\n\nfirst\nlast\n\n\n\n\nAda\nLovelace\n\n\nGrace\nHopper\n\n\nKatherine\nJohnson\n\n\n\n\n\n\nCode\ntoy_people %&gt;% separate_wider_regex(person, patterns = c(first = \"^[^-]+\", \"-\",\n                                                         last = \"[^-]+$\"\n))\n\n\n\n\n\n\nfirst\nlast\n\n\n\n\nAda\nLovelace\n\n\nGrace\nHopper\n\n\nKatherine\nJohnson\n\n\n\n\n\n\n\n\nTidying solution using pivot_longer alone\nI am ambivalent about whether it is useful to combine separable objectives into a single data wrangling verb. Nevertheless, I want to note that the pivot_longer function provides added functionality for both combining columns into a key-value pair format and splitting the key into multiple variables if the key variable is an amalgamation of discrete variables. This can allow us to skip the separate step:\n\n\nCode\ntidy3 &lt;- mess3 %&gt;% \n  pivot_longer(\n    cols = c(-country, -year), \n    names_to = c(\"sex\", \"age_range\"),\n    names_pattern = \"(.)(.*)\", #first character versus the rest\n    values_to = \"freq\",\n    names_ptypes = list(\n      age_range=factor(\n        levels=c(\"u\", \"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"),\n        ordered=TRUE\n      )\n    )\n  )\n\n#Note: we can't adjust the labels in the names_ptype above using labels=c(...).\n#Thus, we'd need to use recode_factor, similar to the above\ntidy3 &lt;- tidy3 %&gt;% mutate(\n  age_range=recode_factor(age_range,\n                          \"014\"=\"0-14\",\n                          \"1524\"=\"15-24\",\n                          \"2534\"=\"25-34\",\n                          \"3544\"=\"35-44\",\n                          \"4554\"=\"45-54\",\n                          \"5564\"=\"55-64\",\n                          \"65\"=\"65+\",\n                          \"u\"=\"unknown\", .default=NA_character_\n  ))\n\ntidy3 %&gt;% kable_table(n=8)\n\n\n\n\n\ncountry\nyear\nsex\nage_range\nfreq\n\n\n\n\nAD\n2000\nm\n0-14\n0\n\n\nAD\n2000\nm\n15-24\n0\n\n\nAD\n2000\nm\n25-34\n1\n\n\nAD\n2000\nm\n35-44\n0\n\n\nAD\n2000\nm\n45-54\n0\n\n\nAD\n2000\nm\n55-64\n0\n\n\nAD\n2000\nm\n65+\n0\n\n\nAD\n2000\nm\nunknown\nNA"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#unite-example",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#unite-example",
    "title": "Tidy data overview",
    "section": "unite example",
    "text": "unite example\nAlthough the least common of the tidying verbs (in my experience), unite is the complement to separate and can be used to bring together multiple variables that we wish to store as a single variable. For example, we may have first name and last name stored in separate variables, but wish to put them together for display or exporting purposes. Sometimes, we also use unite as an intermediate stage in tidying, bringing together variables, reshaping the data, then re-separating them.\n\n\n\n\n\nfirst_name\nlast_name\nage\nfavorite_color\n\n\n\n\nGraham\nDoe\n11\nPurple\n\n\nKieran\nHelali\n9\nBlue\n\n\nCharlotte\nStafford\n11\nPink\n\n\n\n\n\n\nTidying solution\nIf we wanted to have a full_name, we could use unite to combine first_name and last_name and then get rid of those individual columns.\n\n\nCode\ndf4_united &lt;- df4 %&gt;% unite(col = \"full_name\", first_name, last_name, sep=\" \")\n\n\n\n\n\n\n\nfull_name\nage\nfavorite_color\n\n\n\n\nGraham Doe\n11\nPurple\n\n\nKieran Helali\n9\nBlue\n\n\nCharlotte Stafford\n11\nPink"
  },
  {
    "objectID": "materials/w01_tidy_management/tidy_data_conceptual.html#data.table-meltdcast",
    "href": "materials/w01_tidy_management/tidy_data_conceptual.html#data.table-meltdcast",
    "title": "Tidy data overview",
    "section": "data.table melt/dcast",
    "text": "data.table melt/dcast\nThe data.table package provides melt() and dcast() for fast reshaping. The formula interface is compact for multi-way summary tables, and dcast() can aggregate on the fly while casting.\n\n\nCode\ndt &lt;- data.table::data.table(\n  id = 1:4,\n  group = c(\"A\", \"A\", \"B\", \"B\"),\n  y2022 = c(10, 12, 9, 11),\n  y2023 = c(13, 14, 10, 12),\n  z2022 = c(100, 120, 90, 110),\n  z2023 = c(130, 140, 95, 115)\n)\n\nlong_dt &lt;- data.table::melt(\n  dt,\n  id.vars = c(\"id\", \"group\"),\n  measure.vars = patterns(\"^y\", \"^z\"),\n  variable.name = \"year\",\n  value.name = c(\"y\", \"z\")\n)\n\n# Map pattern indices to actual year labels.\nlong_dt$year &lt;- c(\"2022\", \"2023\")[as.integer(long_dt$year)]\nlong_dt\n\n\n\n\n\n\nid\ngroup\nyear\ny\nz\n\n\n\n\n1\nA\n2022\n10\n100\n\n\n2\nA\n2022\n12\n120\n\n\n3\nB\n2022\n9\n90\n\n\n4\nB\n2022\n11\n110\n\n\n1\nA\n2023\n13\n130\n\n\n2\nA\n2023\n14\n140\n\n\n3\nB\n2023\n10\n95\n\n\n4\nB\n2023\n12\n115\n\n\n\n\n\n\nCode\nmean_y &lt;- data.table::dcast(\n  long_dt,\n  group ~ year,\n  value.var = \"y\",\n  fun.aggregate = mean\n)\n\nmean_y\n\n\n\n\n\n\ngroup\n2022\n2023\n\n\n\n\nA\n11\n13.5\n\n\nB\n10\n11.0\n\n\n\n\n\n\nCode\nmean_multi &lt;- data.table::dcast(\n  long_dt,\n  group ~ year,\n  value.var = c(\"y\", \"z\"),\n  fun.aggregate = mean\n)\n\nmean_multi\n\n\n\n\n\n\ngroup\ny_2022\ny_2023\nz_2022\nz_2023\n\n\n\n\nA\n11\n13.5\n110\n135\n\n\nB\n10\n11.0\n100\n105\n\n\n\n\n\n\nCompared with tidyr, data.table::dcast() makes aggregation part of the casting step and can cast multiple value columns in one pass. It is also a common choice for very large tables where performance matters.\nFurther reshaping extensions using data.table package: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-reshape.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSYC 859",
    "section": "",
    "text": "Welcome to PSYC 859, Data Management and Visualization, taught at UNC by Michael Hallquist. This website provides access to lectures, labs, and other course materials for the Spring 2026 session.\nUse the Syllabus and Materials tabs to navigate the course schedule and resources.\n\nCourse description\nThis graduate course is intended to provide an applied introduction to data management and data visualization in the social sciences. In order to take full advantage of modern statistical methods (e.g., structural equation models), competency in data management, semi-automated processing, and data wrangling is prerequisite. Likewise, prior to employing inferential statistics, exploratory visualization and analysis is essential to facilitate data cleaning and to form an initial understanding of patterns in the data. This course will cover both the principles and practice of data management, visualization, and exploratory analysis for summarizing quantitative data. In addition, students will learn data science skills to manage and visualize “big data,” where the size or complexity of the dataset defies traditional techniques.\nApplications of data management, visualization, and analysis will use the R statistical programming language. R is quickly becoming the lingua franca in data science across disciplines and offers unparalleled tools for data analysis and visualization.\n\n\nSchedule overview\n\n1/8 (Week 1): Introduction to data management and tidy data\n1/15 (Week 2): Data aggregation, manipulation, joins\n1/22 (Week 3): Data processing and quality assurance, custom functions, basics of automation\n1/29 (Week 4): Advanced data manipulation and management, tracking work in R markdown\n2/5 (Week 5): Principles of data visualization and graphical grammar\n2/12 (Week 6): Visual and graphical perception\n2/19 (Week 7): Graphic design, layout, style, use of color\n2/26 (Week 8): A tour of quantitative visualization\n3/5 (Week 9): Visualizing continuous data (in ggplot2)\n3/12 (Week 10): Visualizing count and categorical data (in ggplot2)\n3/19: No class (Spring break)\n3/26 (Week 11): Maximizing clarity: preparing graphics for presentation and publication\n4/2: No class (Well-being day)\n4/9 (Week 12): Visualizing and understanding fit (and misfit) of statistical models\n4/16 (Week 13): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction\n4/23 (Week 14): Final presentations of data projects\n\n\n\nObtaining course materials\nTo obtain the full materials for this class, use git clone to download the course repository:\ngit clone https://github.com/michaelhallquist/dataviz_spr2026.git\nIf you already cloned a local copy of the repo, you can get the latest updates using git pull. If all of this git stuff is foreign, I would recommend a quick skim of this documentation: https://happygitwithr.com."
  },
  {
    "objectID": "assignments/week1_tidyData.html",
    "href": "assignments/week1_tidyData.html",
    "title": "Week 1: Data Tidying",
    "section": "",
    "text": "This assignment is due by 1/21/2026 at 8am."
  },
  {
    "objectID": "assignments/week1_tidyData.html#instructions",
    "href": "assignments/week1_tidyData.html#instructions",
    "title": "Week 1: Data Tidying",
    "section": "Instructions",
    "text": "Instructions\nFor each question either write the code you would use or copy and paste it from the RStudio syntax window. Additionally, paste any (reasonable) output generated by the code. If there is a lot of output, paste enough to see that you were able to get the correct answer."
  },
  {
    "objectID": "assignments/week1_tidyData.html#datasets-for-vignettes",
    "href": "assignments/week1_tidyData.html#datasets-for-vignettes",
    "title": "Week 1: Data Tidying",
    "section": "Datasets for Vignettes",
    "text": "Datasets for Vignettes\nLoad the who2 dataset from the tidyr package into your working environment.\n\n\nCode\ndata(\"who2\", package = \"tidyr\")\n\n\nThis dataset records counts of tuberculosis by country and year. The other values correspond to a method of diagnosis, sex, and age group. The method of diagnosis codes are: rel = relapse, sn = negative pulmonary smear, sp = positive pulmonary smear, ep = extrapulmonary. For example, sp_m_014 corresponds to positive pulmonary smear (sp), male sex (m), and ages 0-14 (014).\nYou will see many NA values in who2 because some country-year combinations do not report counts for certain diagnosis/sex/age groups. When you summarize totals, remember to use na.rm = TRUE so missing values do not turn your results into NA.\nAdditionally, load the Pew relig_income dataset from the tidyr package:\n\n\nCode\ndata(\"relig_income\", package = \"tidyr\")\n\n\nThis dataset describes the relationship between income and religion."
  },
  {
    "objectID": "assignments/week1_tidyData.html#basic-pew-dataset-structure",
    "href": "assignments/week1_tidyData.html#basic-pew-dataset-structure",
    "title": "Week 1: Data Tidying",
    "section": "Basic Pew Dataset Structure",
    "text": "Basic Pew Dataset Structure\nUsing the Pew dataset\n\nLook at the first and last five observations of the Pew dataset using head() and tail(). Is the dataset considered tidy? Why or why not?\n\n\nLook at the structure (str()) and class (class()) of the data. If this is not tidy, verbally describe how it would look if it were a tidy dataset. If it is tidy, is it in a format that you would store the data in long-term?"
  },
  {
    "objectID": "assignments/week1_tidyData.html#tidying-pew-data",
    "href": "assignments/week1_tidyData.html#tidying-pew-data",
    "title": "Week 1: Data Tidying",
    "section": "Tidying Pew Data",
    "text": "Tidying Pew Data\n\nUse tidyr verbs to tidy the Pew dataset. Your tidy dataset should have columns named religion, income, and count. Show the first five rows."
  },
  {
    "objectID": "assignments/week1_tidyData.html#tidying-tuberculosis-data",
    "href": "assignments/week1_tidyData.html#tidying-tuberculosis-data",
    "title": "Week 1: Data Tidying",
    "section": "Tidying Tuberculosis Data",
    "text": "Tidying Tuberculosis Data\nUsing the who2 dataset\n\nExplore the who2 dataset using the methods described in the section above.\n\n\nUsing functions from the tidyr package, tidy the who2 dataset so there is a case_group variable and a cases variable. Use those names throughout the rest of the assignment. Show the first five rows of your new dataframe by using head(). Remember, a tidy dataset consists of:\n\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\n\nThe values within the case_group variable are still considered not tidy because they represent three observations in one (case type, sex, and age). Use tidyr verbs to separate this variable into type, sex, and age (hint: the values are separated by _).\n\n\nCheck that each row is a unique observation by counting duplicates of country, year, type, sex, and age. Report whether any combinations appear more than once.\n\n\nRename the values within sex and age to be more descriptive of what they represent (male or female, 0-4, 5-14, etc.). You can use a combination of dplyr::mutate() and dplyr::recode() to recode the values. Use ?recode if you get stuck.\n\n\nTake your new tidy dataframe with the recoded values and variables and demonstrate tidyr::unite() by recombining sex and age into a single variable (e.g., sex_age). This is just to practice unite() before you mess it up again in the next step. Show the first five rows of your new dataframe.\n\n\nGo back to the dataframe you created in question two. Using that tidy dataframe, use tidyr verbs to recreate a ‘messy’ dataframe. It should look exactly the same (or similar) as when you first loaded the who2 dataset into your environment. Show the first five rows of your new dataframe. If the code throws a ‘duplicate identifiers error’ you should use dplyr::distinct() in your pipeline.\n\n\nArrange the dataset to be descending by most cases of tuberculosis to least using dplyr verbs (use your cases variable).\n\n\nSummarize how many cases of tuberculosis there are by sex and age (use your cases variable). Show your output. If you get NA values for everything, remember to remove those empty values somewhere in your pipeline! Hint Look at the default arguments for the functions you use to summarize your data.\n\n\nFinally, summarize the dataset by country and sex, then add a variable that is the relative prevalence by sex in each country."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Week 1: Data Tidying (due 1/21 at 8am) — Download QMD\nWeek 2: Workflowr Project Setup (due 1/28 at 8am) — Download QMD\nData quality assurance and processing project proposal (due 1/29/2026)"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html",
    "href": "assignments/week2_workflowr_setup.html",
    "title": "Week 2: Workflowr Project Setup",
    "section": "",
    "text": "This assignment is due by 1/28/2026 at 8am."
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#overview",
    "href": "assignments/week2_workflowr_setup.html#overview",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Overview",
    "text": "Overview\nThis assignment builds directly on the workflowr vignette: https://workflowr.github.io/workflowr/articles/wflow-01-getting-started.html. Your goal is to create a new workflowr project with a clean structure, publish at least one analysis file, and make it available as a GitHub Pages site.\nI don’t expect that you necessarily use workflowr in the class or more generally, but I think it is useful to see how the package takes a structured approach to:\n\nSetting up a predictable project structure\nUses Git to support version control and reproducibility\nAllows you to publish your reports easily at GitHub Pages accessible to others."
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#learning-goals",
    "href": "assignments/week2_workflowr_setup.html#learning-goals",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Learning goals",
    "text": "Learning goals\n\nInitialize a workflowr project with a reproducible structure\nKnit analysis files with workflowr and track the build status\nPublish a workflowr site to GitHub Pages"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#setup-requirements",
    "href": "assignments/week2_workflowr_setup.html#setup-requirements",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Setup requirements",
    "text": "Setup requirements\nBefore starting, make sure you have:\n\nR and RStudio installed\nGit installed and configured (For help, see: https://happygitwithr.com)\nA GitHub account\nThe workflowr package installed\n\n\n\nCode\n# Install once if needed\ninstall.packages(\"workflowr\")"
  },
  {
    "objectID": "assignments/week2_workflowr_setup.html#tasks",
    "href": "assignments/week2_workflowr_setup.html#tasks",
    "title": "Week 2: Workflowr Project Setup",
    "section": "Tasks",
    "text": "Tasks\n\n1. Start a new workflowr project\nCreate a new project named something like workflowr-&lt;your-name&gt; in a location you can find later. The wflow_start command will setup a new workflowr project in the speecified directory (by default, it does not overwrite existing projects).\n\n\nCode\nlibrary(workflowr) # load library\n# wflow_start(\"~/Downloads/workflowr-michael\")\n\n\nOpen the project in RStudio (File &gt; Open Project) and review the default folders: analysis/, data/, code/, output/, and docs/.\n\n\n2. Create a new analysis file\nCreate a new analysis Rmarkdown file at analysis/01-getting-started.Rmd. You can use wflow_open if you wish, or just File &gt; New File &gt; R Markdown.\nIn the Rmd file, include the following:\n\nA short paragraph describing the project topic you plan to use this semester\nAdd a small public domain data file to the data/ folder (e.g., a CSV from data.gov or another public domain source).\nOne code chunk that reads your data file from data/ and prints a small summary (e.g., str() or summary())\n\n\n\n3. Build and check the site locally\nUse workflowr to build the site and check its status.\n\n\nCode\nwflow_build(\"analysis/01-getting-started.Rmd\") # build only one script\nwflow_build() # build the whole site\nwflow_status() # check the build status\n\n\n\n\n4. Publish and commit with workflowr\nPublish the new analysis page using workflowr so it is tracked by Git.\n\n\nCode\nwflow_publish(\"analysis/01-getting-started.Rmd\", message = \"Add initial analysis page\") # committing your work to the repository\n\n\n\n\n5. Push to GitHub and enable GitHub Pages\nAfter publishing (basically concretizing a change), connect the project to GitHub with your username:\nSetup your Github repository using the wflow_use_github function. I chose the Oauth route when prompted, but either is fine.\n\n\nCode\nwflow_use_github(\"myname\")\n\n\nNow push the code from your computer to Github\n\n\nCode\nwflow_git_push()\n\n\nIn GitHub, enable Pages for the docs/ folder on the main branch.\nGuidance here: https://docs.github.com/en/pages/quickstart\nAfter configured, this should look like the following: \nAfter a few minutes of setting this, the top part of the GitHub Pages for the repo (https://github.com/&lt;username&gt;/&lt;reponame&gt;/settings/pages) should look something like this:\n\n\n\n6. Verify the published site\nOpen your GitHub Pages site and confirm:\n\nThe home page loads\nYour analysis page appears in the navigation\nThe analysis page renders your text and code output\n\n\n\nDeliverables\n\nProvide the URL of your project site.\nShort reflection (2-3 sentences) on what was straightforward vs. confusing, what you think may be helpful about this approach and/or what aspects you may adopt in your own workflows"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "This page contains course materials used during the demonstration and practical exercises.\n\nWeek 1\n\nTidy data conceptual walkthrough (QMD)\n\n\n\nWeek 2\n\nData wrangling in dplyr (QMD)\nJoins tutorial (QMD)\nStrings in R with stringr (QMD)\n\n\n\nWeek 3\n\nCompact data validation with validate (Psychology Survey Example) (QMD)\nECG physio processing script (Data)"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html",
    "title": "Data wrangling in dplyr",
    "section": "",
    "text": "The goal of this document is to provide a basic introduction to data wrangling using functions from the so-called ‘tidyverse’ approach. The tidyverse (https://www.tidyverse.org) is a set of data science packages in R that are intended to provide a consistent paradigm for working with data. This approach unifies a previously inchoate landscape of different functions and packages in R that could be daunting to new users.\nAlthough I do not claim that the tidyverse approach is best according to all possible criteria, I believe that it is the best paradigm for working with data in R for social scientists, many of whom do not have a formal background in computer programming.\nHere, I will draw primarily from the tidyr and dplyr packages in R.\nFor an excellent book-length treatment of the tidyverse approach, see R for Data Science (2nd edition) by Hadley Wickham, Mine Cetinkaya-Rundel, and Garrett Grolemund."
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#addressing-namespace-collisions",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#addressing-namespace-collisions",
    "title": "Data wrangling in dplyr",
    "section": "Addressing namespace collisions",
    "text": "Addressing namespace collisions\n\nWatch out for warnings about objects being ‘masked’ when packages are loaded.\nExplicitly specify the package where your desired function lives using the double colon operator. Example: dplyr::summarize.\nTry to load tidyverse packages using library(tidyverse). This handles collisions within the tidyverse!\n\nFor example, in this document, multilevel pulls in MASS, which masks dplyr::select. If we were to load dplyr first, then multilevel, we would see the following:\n&gt; library(dplyr)\n&gt; library(multilevel)\nLoading required package: nlme\n\nAttaching package: ‘nlme’\n\nThe following object is masked from ‘package:dplyr’:\n\n    collapse\n\nLoading required package: MASS\n\nAttaching package: ‘MASS’\n\nThe following object is masked from ‘package:dplyr’:\n\n    select\nRead in English, this says, “The multilevel package needs to load nlme (a mixed-effects package). Nlme has a function called ‘collapse’ that will now take precedence over ‘collapse’ in dplyr if you run collapse() in your session. Multilevel also needs the MASS package, so I’m loading it. The MASS package has a function called ‘select’ that now takes precedence over dplyr’s select function, so if you call select() in your session, you will be getting the MASS package function, not the dplyr one.”\nTo get around these problems, the easiest route is to load dplyr (and other tidyverse packages) last in the chain, giving them precedence over other packages. As you see above in the p_load call, we load dplyr last in the chain so that its select function is preferred over MASS. This is important because functions of the same name across packages may have completely different purposes and syntax!\nThe slightly harder, but more robust, route is always to use explicit namespace qualifiers with the :: operator. If you use dplyr::select as the function call, you are explicitly telling R that you want dplyr’s select function, not some other package’s.\nThe two most common collisions are with select (MASS) and summarize (Hmisc). If you’re not sure which version of a function has taken precedence you can type the function name without parentheses. This will print the function source (which you can ignore). But take a look at the last line, which reveals where R ‘found’ the function in the search across loaded packages (based on their load order/precedence).\n&gt; select\nfunction (obj) \nUseMethod(\"select\")\n&lt;bytecode: 0x93b18ebd8&gt;\n&lt;environment: namespace:MASS&gt;\nIf you were expecting that last line to read &lt;environment: namespace:dplyr&gt; (i.e. dplyr’s version), then you have a namespace problem to resolve!"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#use-of-this-reference-in-tidyverse",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#use-of-this-reference-in-tidyverse",
    "title": "Data wrangling in dplyr",
    "section": "Use of ‘this’ reference in tidyverse",
    "text": "Use of ‘this’ reference in tidyverse\nSometimes it is useful to refer to the current dataset or variable explicitly in tidyverse data wrangling syntax.\ndplyr/magrittr tends to hide this from us for convenience, but it’s there under the hood.\niris %&gt;% filter(Sepal.Length &gt; 7)\nis the same as\niris %&gt;% filter(., Sepal.Length &gt; 7)\nSo, '.' refers to the current dataset or variable (depending on context) in dplyr operations. And if you don’t specify where the '.' falls in your syntax, it will always be passed as the first argument to the downstream function."
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#overview-a-first-pass-through-an-nhanes-dataset",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#overview-a-first-pass-through-an-nhanes-dataset",
    "title": "Data wrangling in dplyr",
    "section": "Overview: a first pass through an NHANES dataset",
    "text": "Overview: a first pass through an NHANES dataset\nTo learn dplyr, let’s start with a survey from the National Health and Nutrition Examination Survey (NHANES) dataset. These data are provided in the nhanesA package. We’ll start by looking at a couple of basic demographic variables.\nNHANES variables used below (from DEMO_D and BMX_D):\n\nSEQN: respondent ID\nRIAGENDR: gender (coded numeric in raw NHANES)\nRIDAGEYR: age in years\nDMDHREDU: education level\nBMXWT: weight (kg)\nBMXARML: upper arm length (cm)\nBMXTHICR: thigh circumference (cm)\n\n\n\nCode\n# demographics: note that this code relies on functions in the nhanesA package, which\n#   is useful for working with NHANES data, but is not a focus of this workshop per se.\n\ndemo_d &lt;- nhanes('DEMO_D')\ndemo_d_vars  &lt;- nhanesTableVars('DEMOGRAPHICS', 'DEMO_D', namesonly=TRUE)\n\n# translate numeric codes into factors by using the nhanes lookup/codebook functions\ndemo_d &lt;- suppressWarnings(nhanesTranslate('DEMO_D', demo_d_vars, data=demo_d))\n\n\nTranslated columns: AIALANG DMDBORN DMDCITZN DMDEDUC2 DMDEDUC3 DMDHRBRN DMDHREDU DMDHRGND DMDHRMAR DMDHSEDU DMDMARTL DMDSCHOL DMDYRSUS DMQMILIT FIAINTRP FIALANG FIAPROXY INDFMINC INDHHINC MIAINTRP MIALANG MIAPROXY RIAGENDR RIDEXMON RIDEXPRG RIDRETH1 RIDSTATR SDDSRVYR SIAINTRP SIALANG SIAPROXY\n\n\nCode\n# dplyr pipeline\ndemo_d &lt;- demo_d %&gt;% \n  filter(!INDHHINC %in% c(\"Over $20,000\", \"Under $20,000\", \"Refused\", \"Don't know\")) %&gt;% \n  droplevels() %&gt;% # drop unused factor levels from the data.frame\n  mutate(\n    # case_when() is a vectorized if/else ladder for many conditions\n    income_num = case_when( # convert range-based factor labels to midpoint numbers\n    INDHHINC == \"$     0 to $ 4,999\" ~ 2500,\n    INDHHINC == \"$ 5,000 to $ 9,999\" ~ 7500,\n    INDHHINC == \"$10,000 to $14,999\" ~ 12500,\n    INDHHINC == \"$15,000 to $19,999\" ~ 17500,\n    INDHHINC == \"$20,000 to $24,999\" ~ 22500,\n    INDHHINC == \"$25,000 to $34,999\" ~ 30000,\n    INDHHINC == \"$35,000 to $44,999\" ~ 40000,\n    INDHHINC == \"$45,000 to $54,999\" ~ 50000,\n    INDHHINC == \"$55,000 to $64,999\" ~ 60000,\n    INDHHINC == \"$65,000 to $74,999\" ~ 70000,\n    INDHHINC == \"$75,000 and Over\" ~ 80000\n    )\n  )\n \n#load body (biometric) measures\nbmx_d &lt;- nhanes('BMX_D')\nbmx_d_vars  &lt;- nhanesTableVars('EXAM', 'BMX_D', namesonly=TRUE)\nbmx_d &lt;- suppressWarnings(nhanesTranslate('BMX_D', bmx_d_vars, data=bmx_d))\n\n\nTranslated columns: BMDSTATS BMIARMC BMIARML BMICALF BMIHT BMILEG BMIRECUM BMISUB BMITHICR BMITRI BMIWAIST BMIWT\n\n\nCode\n# merge the education demographics variable with the biometric data, joining on the SEQN column (basically an ID)\nbmx_d &lt;- demo_d %&gt;% \n  dplyr::select(SEQN, DMDHREDU) %&gt;% \n  inner_join(bmx_d, by=\"SEQN\")"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#group_by-summarize",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#group_by-summarize",
    "title": "Data wrangling in dplyr",
    "section": "group_by + summarize",
    "text": "group_by + summarize\nLet’s summarize the mean household income (income_num), converted from categories in INDHHINC) by highest level of education completed (DMDHREDU)\n\n\nCode\ndemo_d %&gt;% \n  group_by(DMDHREDU) %&gt;% # divide dataset into separate compartments by education level\n  dplyr::summarize(\n    n=n(), # number of observations\n    m_income=mean(income_num, na.rm=T), \n    sd_income=sd(income_num, na.rm=T)\n  ) %&gt;% \n  kable_table()\n\n\n\n\n\nDMDHREDU\nn\nm_income\nsd_income\n\n\n\n\nLess Than 9th Grade\n1173\n28024\n18489\n\n\n9-11th Grade (Includes 12th grade with no diploma)\n1626\n31318\n21584\n\n\nHigh School Grad/GED or equivalent\n2317\n39400\n23422\n\n\nSome College or AA degree\n2664\n48137\n25326\n\n\nCollege Graduate or above\n1768\n63785\n21816\n\n\nRefused\n6\n25000\n12624\n\n\nDon't know\n26\n20900\n15760\n\n\nNA\n276\n44623\n24849\n\n\n\n\n\nNote that summarize removes a single level of grouping in the group_by process. Here, we only have one grouping variable, DMDHREDU, so the output of summarize will be ‘ungrouped.’"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#grouped-summaries-of-several-variables",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#grouped-summaries-of-several-variables",
    "title": "Data wrangling in dplyr",
    "section": "Grouped summaries of several variables",
    "text": "Grouped summaries of several variables\nWhat if I want to have means and SDs for several continuous variables grouped by highest education? Let’s look specifically at the weight (BMXWT), upper arm length (BMXARML), and thigh circumference (BMXTHICR) measurements. The combination of summarize and across provide functionality to specify several variables using the .cols argument of across and potentially several summary functions by passing them in a named list.\n\n\nCode\nbmx_d %&gt;% \n  group_by(DMDHREDU) %&gt;% \n  dplyr::summarize(\n    across(\n      c(BMXWT, BMXARML, BMXTHICR), \n      list(m=~mean(.x, na.rm=T), sd=~sd(.x, na.rm=T))\n    )\n  ) %&gt;% \n  kable_table()\n\n\n\n\n\nDMDHREDU\nBMXWT_m\nBMXWT_sd\nBMXARML_m\nBMXARML_sd\nBMXTHICR_m\nBMXTHICR_sd\n\n\n\n\nLess Than 9th Grade\n58\n30\n32\n7.8\n50\n7.48\n\n\n9-11th Grade (Includes 12th grade with no diploma)\n56\n33\n32\n8.2\n51\n8.40\n\n\nHigh School Grad/GED or equivalent\n61\n33\n33\n7.9\n52\n8.10\n\n\nSome College or AA degree\n62\n32\n33\n7.6\n52\n8.65\n\n\nCollege Graduate or above\n61\n31\n33\n7.8\n52\n7.99\n\n\nRefused\n70\n25\n38\n2.5\n50\n0.57\n\n\nDon't know\n50\n36\n28\n11.5\n53\n7.74\n\n\nNA\n56\n29\n32\n7.6\n51\n7.73\n\n\n\n\n\nLet’s slow this down:\n\ngroup_by verb\nsurvey %&gt;% group_by(DMDHREDU)\nThis tells dplyr to divide the bmx_d (NHANES biometric measurements) data into a set of smaller data.frame objects, one per level of DMDHREDU. Internally, this looks something like the output below. After this division of the dataset into chunks, summarize will work on each chunk individually.\n\n\n$`Less Than 9th Grade`\n   SEQN            DMDHREDU                    BMDSTATS BMXWT\n1 31137 Less Than 9th Grade          Other partial exam    80\n2 31157 Less Than 9th Grade Complete data for age group    42\n3 31169 Less Than 9th Grade Complete data for age group    22\n4 31175 Less Than 9th Grade Complete data for age group    86\n\n$`9-11th Grade (Includes 12th grade with no diploma)`\n   SEQN                                           DMDHREDU\n1 31128 9-11th Grade (Includes 12th grade with no diploma)\n2 31133 9-11th Grade (Includes 12th grade with no diploma)\n3 31145 9-11th Grade (Includes 12th grade with no diploma)\n4 31148 9-11th Grade (Includes 12th grade with no diploma)\n                     BMDSTATS BMXWT\n1 Complete data for age group    40\n2 Complete data for age group    45\n3 Complete data for age group    40\n4 Complete data for age group    52\n\n$`High School Grad/GED or equivalent`\n   SEQN                           DMDHREDU                    BMDSTATS BMXWT\n1 31127 High School Grad/GED or equivalent Complete data for age group    10\n2 31139 High School Grad/GED or equivalent          Other partial exam    74\n3 31140 High School Grad/GED or equivalent Complete data for age group    42\n4 31143 High School Grad/GED or equivalent Complete data for age group    76\n\n$`Some College or AA degree`\n   SEQN                  DMDHREDU                    BMDSTATS BMXWT\n1 31129 Some College or AA degree Complete data for age group    75\n2 31130 Some College or AA degree  No body measures exam data    NA\n3 31138 Some College or AA degree Complete data for age group    14\n4 31142 Some College or AA degree          Other partial exam    80\n\n$`College Graduate or above`\n   SEQN                  DMDHREDU                    BMDSTATS BMXWT\n1 31131 College Graduate or above          Other partial exam    75\n2 31132 College Graduate or above Complete data for age group    70\n3 31135 College Graduate or above Complete data for age group    10\n4 31141 College Graduate or above Complete data for age group    60\n\n$Refused\n   SEQN DMDHREDU                             BMDSTATS BMXWT\n1 33385  Refused          Complete data for age group    78\n2 35296  Refused Partial:  Height and weight obtained    38\n3 37761  Refused          Complete data for age group    67\n4 40762  Refused Partial:  Height and weight obtained    99\n\n$`Don't know`\n   SEQN   DMDHREDU                    BMDSTATS BMXWT\n1 31546 Don't know          Other partial exam    78\n2 32106 Don't know Complete data for age group    12\n3 32626 Don't know Complete data for age group    64\n4 32648 Don't know Complete data for age group    85\n\n\n\n\nsummarize + across\nThe summarize function transforms a dataset that has many rows to a dataset that has a single row per grouping unit. If you do not use group_by, summarize will yield an overall summary statistic in the entire dataset. For example, to get the mean and SD of household income in NHANES, irrespective of education level or other categorical moderators, we could just use a simple summarize:\n\n\nCode\ndemo_d %&gt;%\n  dplyr::summarize(\n    m_income=mean(income_num, na.rm=T), \n    sd_income=sd(income_num, na.rm=T)\n  ) %&gt;% \n  kable_table()\n\n\n\n\n\nm_income\nsd_income\n\n\n\n\n43590\n25726\n\n\n\n\n\nBut because we used group_by(DMDHREDU) above, we got unique summaries of the variables at each level of education.\nThe across function accepts two primary arguments. First, we specify a set of variables (the .cols argument) that we wish to summarize in the same way (i.e., compute the same summary statistics). Second, we specify which statistics we wish to compute (the .fns argument). In our case, the syntax was:\ndplyr::summarize(\n    across(c(BMXWT, BMXARML, BMXTHICR), \n           list(m=~mean(.x, na.rm=T), sd=~sd(.x, na.rm=T)))\n)\nThe c() function specifies a vector of unquoted variable names in the dataset we wish to summarize, separated by commas. Note that any tidyselect operator for selecting columns will work. This includes starts_with, ends_with, matches, and others. For details, see ?dplyr_tidy_select.\nThe list() here asks dplyr to run each function in the list against each variables in the .cols specification. Here, this means that dplyr will compute the mean and SD of each variable in the .cols argument at each level of education completed (the group_by basis). The names of the list elements (left side) — here, m and sd — become the suffixes added for each variable. The value of the element (right side) — here, mean and sd — are the functions that should be used to compute a summary statistic (they should return one number per grouped variable).\nNote that the column names that result from an across operation can be modified using the .names argument. The default is to suffix the variable name with the names of the functions list, preceded by an underscore (e.g., income_m).\nPassing arguments to summary functions\nNotice how we passed na.rm=TRUE to the mean function within the list. This tells the mean to ignore missing (NA) values when computing the mean (i.e., mean of the non-missing numbers). In general, the dplyr syntax using what they call “lambdas” (starting with ~) is the clearest way to control the arguments passed to each function. Here is a simple definition of a lambda in R:\n~ mean(.x, na.rm=TRUE)\nThe .x refers to the current variable being used within a dplyr data wrangling operation. This is in contrast to ., which generally refers to the current dataset.\nIf you don’t need to pass arguments to the functions in an across() operation, you can just state the function name:\ndplyr::summarize(\n    across(c(BMXWT, BMXARML, BMXTHICR), \n           list(m=mean, sd=sd))\n)"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#making-a-summarize-pipeline-even-more-beautiful",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#making-a-summarize-pipeline-even-more-beautiful",
    "title": "Data wrangling in dplyr",
    "section": "Making a summarize pipeline even more beautiful",
    "text": "Making a summarize pipeline even more beautiful\nWe can also make the output more beautiful using tidying techniques we’ve already seen in the tidyr tutorial. Remember that R is all about programming for data science. In particular, notice that we have some columns that are means and others that are SDs.\nWe can just extend our data pipeline a bit. The extract function from tidyr here is like separate, but with a bit more oomph using regular expressions. This is a more intermediate topic, but there is a useful tutorial here: http://www.regular-expressions.info/tutorial.html.\n\n\nCode\nbmx_d %&gt;% \n  group_by(DMDHREDU) %&gt;% \n  dplyr::summarize(\n    across(\n      c(BMXWT, BMXARML, BMXTHICR), \n      list(m=~mean(.x, na.rm=T), sd=~sd(.x, na.rm=T))\n    )\n  ) %&gt;%\n  \n   # combine m and sd statistics (notice how you can add comments inline within a pipeline?)\n  pivot_longer(cols=-DMDHREDU, names_to = \"Measure\", values_to = \"value\") %&gt;%\n  \n  # divide income_m into income and m\n  #extract(col=Measure, into=c(\"bio_measure\", \"statistic\"), regex=(\"(.*)_(.*)$\")) %&gt;% \n  separate(col=Measure, into=c(\"bio_measure\", \"statistic\"), sep=\"_\") %&gt;% \n  pivot_wider(names_from=statistic, values_from = value) %&gt;% \n  arrange (bio_measure, DMDHREDU) %&gt;%\n  kable_table()\n\n\n\n\n\nDMDHREDU\nbio_measure\nm\nsd\n\n\n\n\nLess Than 9th Grade\nBMXARML\n32\n7.77\n\n\n9-11th Grade (Includes 12th grade with no diploma)\nBMXARML\n32\n8.22\n\n\nHigh School Grad/GED or equivalent\nBMXARML\n33\n7.86\n\n\nSome College or AA degree\nBMXARML\n33\n7.61\n\n\nCollege Graduate or above\nBMXARML\n33\n7.81\n\n\nRefused\nBMXARML\n38\n2.47\n\n\nDon't know\nBMXARML\n28\n11.49\n\n\nNA\nBMXARML\n32\n7.62\n\n\nLess Than 9th Grade\nBMXTHICR\n50\n7.48\n\n\n9-11th Grade (Includes 12th grade with no diploma)\nBMXTHICR\n51\n8.40\n\n\nHigh School Grad/GED or equivalent\nBMXTHICR\n52\n8.10\n\n\nSome College or AA degree\nBMXTHICR\n52\n8.65\n\n\nCollege Graduate or above\nBMXTHICR\n52\n7.99\n\n\nRefused\nBMXTHICR\n50\n0.57\n\n\nDon't know\nBMXTHICR\n53\n7.74\n\n\nNA\nBMXTHICR\n51\n7.73\n\n\nLess Than 9th Grade\nBMXWT\n58\n30.15\n\n\n9-11th Grade (Includes 12th grade with no diploma)\nBMXWT\n56\n33.44\n\n\nHigh School Grad/GED or equivalent\nBMXWT\n61\n32.70\n\n\nSome College or AA degree\nBMXWT\n62\n32.48\n\n\nCollege Graduate or above\nBMXWT\n61\n31.30\n\n\nRefused\nBMXWT\n70\n25.08\n\n\nDon't know\nBMXWT\n50\n36.00\n\n\nNA\nBMXWT\n56\n29.12\n\n\n\n\n\n\narrange: order observations\nToward the end of the pipeline above, we see:\narrange (bio_measure, DMDHREDU) %&gt;%\nThe arrange verb in dplyr requests that observations be sorted according to one or more variables. Here, we ask for the dataset to be sorted by bio_measure (biometric measure, such as weight) first, then by education level within that measure. The arrange verb sorts observations in ascending order (low to high) by default, but data can be sorted in descending order using the desc() function:\narrange(bio_measure, desc(DMDHREDU)) %&gt;%\nThis would sort by highest to lowest education level within each measure (bio_measure), where the bio_measures are still in ascending (alphabetical) order"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#filter-obtaining-observations-rows-based-on-some-criteria",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#filter-obtaining-observations-rows-based-on-some-criteria",
    "title": "Data wrangling in dplyr",
    "section": "filter: obtaining observations (rows) based on some criteria",
    "text": "filter: obtaining observations (rows) based on some criteria\nObjective: Retain only men in company A\n\n\nCode\nCOMPANY=\"A\"\nCOMPANY &lt;- \"A\"\n\ncompany_A_men &lt;- filter(univbct, COMPANY==\"A\" & GENDER==1)\n#print 10 observations at random to check the accuracy of the filter\n#p=11 just shows the first 11 columns to keep it on one page for formatting\ncompany_A_men %&gt;% sample_n(10) %&gt;% kable_table(p=11)\n\n\n\n\n\n\nBTN\nCOMPANY\nMARITAL\nGENDER\nHOWLONG\nRANK\nEDUCATE\nAGE\nJOBSAT1\nCOMMIT1\nREADY1\n\n\n\n\n822\n4042\nA\n2\n1\n2\n15\n2\n27\n4.0\n4.0\n4.5\n\n\n821\n4042\nA\n2\n1\n2\n15\n2\n27\n4.0\n4.0\n4.5\n\n\n624\n299\nA\n2\n1\n2\n15\n2\n26\n1.7\n3.0\n2.8\n\n\n556\n144\nA\n1\n1\n2\n21\n5\n23\n3.3\n3.7\n3.5\n\n\n529\n3066\nA\n1\n1\n2\n12\n2\n20\n2.7\n1.3\n1.8\n\n\n453\n299\nA\n2\n1\n2\n21\n5\n23\n3.3\n3.7\n2.2\n\n\n55\n104\nA\n1\n1\nNA\n15\n2\n23\n3.0\n5.0\n3.5\n\n\n1234\n3066\nA\n2\n1\n2\n16\n2\n34\n3.0\n3.7\n3.0\n\n\n618\n4042\nA\n1\n1\n2\n13\n2\n19\n1.3\n3.3\n2.5\n\n\n592\n1022\nA\n1\n1\n4\n14\n2\n22\n3.3\n3.3\n3.0\n\n\n\n\n\nObjective: Count how many people are in companies A and B\n\n\nCode\nfilter(univbct, COMPANY %in% c(\"A\",\"B\")) %&gt;% nrow()\n\n\n[1] 750\n\n\nObjective: What about counts by company and battalion?\n\n\nCode\nunivbct %&gt;% \n  group_by(BTN, COMPANY) %&gt;% \n  tally() %&gt;%\n  kable_table(n=12)\n\n\n\n\n\nBTN\nCOMPANY\nn\n\n\n\n\n4\nA\n66\n\n\n4\nB\n15\n\n\n4\nC\n12\n\n\n4\nD\n30\n\n\n4\nHHC\n18\n\n\n104\nA\n12\n\n\n104\nHHC\n3\n\n\n124\nA\n42\n\n\n144\nA\n30\n\n\n299\nA\n39\n\n\n299\nB\n30\n\n\n299\nC\n27\n\n\n\n\n\nCode\n# N.B. The same result could be obtained with count(BTN, COMPANY) alone.\n#  This combines the group_by and tally functions"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#select-choose-variables-columns-based-on-some-criteria",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#select-choose-variables-columns-based-on-some-criteria",
    "title": "Data wrangling in dplyr",
    "section": "select: choose variables (columns) based on some criteria",
    "text": "select: choose variables (columns) based on some criteria\nLet’s start by keeping only the three core dependent variables over time: jobsat, commit, ready. Keep SUBNUM as well for unique identification.\n\n\nCode\ndvs_only &lt;- univbct %&gt;% \n  dplyr::select(SUBNUM, JOBSAT1, JOBSAT2, JOBSAT3, \n                COMMIT1, COMMIT2, COMMIT3, \n                READY1, READY2, READY3)\n\n\nIf you have many variables of a similar name, you might try starts_with(). Note in this case that it brings in “READY”, too. Note that you can mix different selection mechanisms within select. Look at the cheatsheet.\n\n\nCode\ndvs_only &lt;- univbct %&gt;% \n  dplyr::select(SUBNUM, starts_with(\"JOBSAT\"), starts_with(\"COMMIT\"), starts_with(\"READY\"))\n\n\nOther selection mechanisms:\n\ncontains: variable name contains a literal string\nstarts_with: variable names start with a string\nends_with: variable names end with a string\nnum_range: variables that have a common prefix (e.g., ‘reasoning’) and a numeric range (e.g., 1-20)\nmatches: variable name matches a regular expression\none_of: variable is one of the elements in a character vector. Example: select(one_of(c(“A”, “B”)))\n\nSee ?select_helpers for more details."
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#select-filter-zooming-in-on-specific-observations-and-variables",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#select-filter-zooming-in-on-specific-observations-and-variables",
    "title": "Data wrangling in dplyr",
    "section": "select + filter: zooming in on specific observations and variables",
    "text": "select + filter: zooming in on specific observations and variables\nNote that select and filter can be combined to subset both observations and variables of interest.\nFor example, look at readiness to deploy in battalion 299 only:\n\n\nCode\nunivbct %&gt;% \n  filter(BTN==299) %&gt;% \n  dplyr::select(SUBNUM, READY1, READY2, READY3) %&gt;% \n  kable_table(n=6)\n\n\n\n\n\n\nSUBNUM\nREADY1\nREADY2\nREADY3\n\n\n\n\n10\n4\n2.5\n3.2\n3.0\n\n\n11\n4\n2.5\n3.2\n3.0\n\n\n12\n4\n2.5\n3.2\n3.0\n\n\n19\n7\n2.0\n1.8\n1.2\n\n\n20\n7\n2.0\n1.8\n1.2\n\n\n21\n7\n2.0\n1.8\n1.2\n\n\n\n\n\nselect is also useful for dropping variables that are not of interest using a kind of subtraction syntax.\n\n\nCode\nnojobsat &lt;- univbct %&gt;% \n  dplyr::select(-starts_with(\"JOBSAT\"))\nnames(nojobsat)\n\n\n [1] \"BTN\"     \"COMPANY\" \"MARITAL\" \"GENDER\"  \"HOWLONG\" \"RANK\"    \"EDUCATE\"\n [8] \"AGE\"     \"COMMIT1\" \"READY1\"  \"COMMIT2\" \"READY2\"  \"COMMIT3\" \"READY3\" \n[15] \"TIME\"    \"JSAT\"    \"COMMIT\"  \"READY\"   \"SUBNUM\""
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#mutate-add-one-or-more-variables-that-are-a-function-of-other-variables",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#mutate-add-one-or-more-variables-that-are-a-function-of-other-variables",
    "title": "Data wrangling in dplyr",
    "section": "mutate: add one or more variables that are a function of other variables",
    "text": "mutate: add one or more variables that are a function of other variables\n(Row-wise) mean of commit scores over waves. Note how you can used select() within a mutate to run a function on a subset of the data.\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  mutate(commitmean=rowMeans(dplyr::select(., COMMIT1, COMMIT2, COMMIT3)))\n\n\nMutate can manipulate several variables in one call. Here, mean center any variable that starts with COMMIT and add the suffix _cm for clarity. Also compute the percentile rank for each of these columns, with _pct as suffix. Note the use of the starts_with function here within the across(). This operates identically to select, but in the context of a summary or mutation operation on specific variables. See ?select_helpers for details.\n\n\nCode\nmeancent &lt;- function(x) { x - mean(x, na.rm=TRUE) } #simple worker function to mean center a variable\n\nunivbct &lt;- univbct %&gt;% \n  mutate(across(starts_with(\"COMMIT\", ignore.case = FALSE), list(cm=meancent, pct=percent_rank)))\n\nunivbct %&gt;%\n  dplyr::select(starts_with(\"COMMIT\", ignore.case = FALSE)) %&gt;%\n  kable_table(n=8) %&gt;% kable_styling(font_size = 12)\n\n\n\n\n\nCOMMIT1\nCOMMIT2\nCOMMIT3\nCOMMIT\nCOMMIT1_cm\nCOMMIT1_pct\nCOMMIT2_cm\nCOMMIT2_pct\nCOMMIT3_cm\nCOMMIT3_pct\nCOMMIT_cm\nCOMMIT_pct\n\n\n\n\n1.7\n1.7\n3.0\n1.7\n-1.95\n0.01\n-1.80\n0.04\n-0.54\n0.12\n-1.87\n0.02\n\n\n1.7\n1.7\n3.0\n1.7\n-1.95\n0.01\n-1.80\n0.04\n-0.54\n0.12\n-1.87\n0.02\n\n\n1.7\n1.7\n3.0\n3.0\n-1.95\n0.01\n-1.80\n0.04\n-0.54\n0.12\n-0.54\n0.15\n\n\n1.7\n1.3\n1.3\n1.7\n-1.95\n0.01\n-2.13\n0.03\n-2.20\n0.01\n-1.87\n0.02\n\n\n1.7\n1.3\n1.3\n1.3\n-1.95\n0.01\n-2.13\n0.03\n-2.20\n0.01\n-2.21\n0.01\n\n\n1.7\n1.3\n1.3\n1.3\n-1.95\n0.01\n-2.13\n0.03\n-2.20\n0.01\n-2.21\n0.01\n\n\n3.3\n3.3\n3.7\n3.3\n-0.28\n0.27\n-0.13\n0.33\n0.13\n0.45\n-0.21\n0.30\n\n\n3.3\n3.3\n3.7\n3.3\n-0.28\n0.27\n-0.13\n0.33\n0.13\n0.45\n-0.21\n0.30"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#arrange-reorder-observations-in-specific-order",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#arrange-reorder-observations-in-specific-order",
    "title": "Data wrangling in dplyr",
    "section": "arrange: reorder observations in specific order",
    "text": "arrange: reorder observations in specific order\nOrder data by ascending battalion, company, then subnum\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  arrange(BTN, COMPANY, SUBNUM)\n\n\nDescending sort: descending battalion, ascending company, ascending subnum\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  arrange(desc(BTN), COMPANY, SUBNUM)"
  },
  {
    "objectID": "materials/w02_wrangling/dplyr_walkthrough.html#a-more-realistic-example-preparation-for-multilevel-analysis",
    "href": "materials/w02_wrangling/dplyr_walkthrough.html#a-more-realistic-example-preparation-for-multilevel-analysis",
    "title": "Data wrangling in dplyr",
    "section": "A more realistic example: preparation for multilevel analysis",
    "text": "A more realistic example: preparation for multilevel analysis\nIn MLM, one strategy for disentangling within- versus between-person effects is to include both within-person-centered variables and person means in the model (Curran & Bauer, 2011).\nWe can achieve this easily for our three DVs here using a single pipeline that combines tidying and mutation. Using -1 as the sep argument to separate splits the string at the second-to-last position (i.e., starting at the right).\nFor reshaping to work smoothly, we need a unique identifier for each row. Also, univbct is stored in a dangerously untidy format in which variables with suffix 1-3 indicate a ‘wide format’, but the data is also in long format under variables such as ‘JSAT’ and ‘COMMIT.’ In other words, there is a peculiar redundancy in the data that is altogether confusing.\nTake a look:\n\n\nCode\nunivbct %&gt;%\n  dplyr::select(SUBNUM, starts_with(\"JOBSAT\"), JSAT) %&gt;% \n  kable_table(n=12)\n\n\n\n\n\n\nSUBNUM\nJOBSAT1\nJOBSAT2\nJOBSAT3\nJSAT\n\n\n\n\n319\n103\n2.0\n2.3\n3.3\n2.0\n\n\n320\n103\n2.0\n2.3\n3.3\n2.3\n\n\n321\n103\n2.0\n2.3\n3.3\n3.3\n\n\n397\n129\n3.7\n4.3\n4.7\n3.7\n\n\n398\n129\n3.7\n4.3\n4.7\n4.3\n\n\n399\n129\n3.7\n4.3\n4.7\n4.7\n\n\n523\n171\n3.7\n4.0\nNA\n3.7\n\n\n524\n171\n3.7\n4.0\nNA\n4.0\n\n\n525\n171\n3.7\n4.0\nNA\nNA\n\n\n616\n202\n1.3\n2.0\n4.3\n1.3\n\n\n617\n202\n1.3\n2.0\n4.3\n2.0\n\n\n618\n202\n1.3\n2.0\n4.3\n4.3\n\n\n\n\n\nWe first need to eliminate this insanity. Group by subject number and retain only the first row (i.e., keep the wide version).\n\n\nCode\nunivbct &lt;- univbct %&gt;% \n  group_by(SUBNUM) %&gt;% # split into separate groups for each subject\n  filter(row_number() == 1) %&gt;% # only retain the first row of each subject\n  dplyr::select(-JSAT, -COMMIT, -READY) %&gt;% # drop redundant columns\n  ungroup() # remove grouping from data structure (we are done with group-based wrangling)\n\n\nFirst, let’s get the data into a conventional format (long) for MLM (e.g., using lmer)\n\n\nCode\nforMLM &lt;- univbct %&gt;% \n  dplyr::select(SUBNUM, JOBSAT1, JOBSAT2, JOBSAT3, \n                COMMIT1, COMMIT2, COMMIT3, \n                READY1, READY2, READY3) %&gt;% \n  \n  # pivot everything but SUBNUM\n  pivot_longer(names_to = \"key\", values_to = \"value\", cols=-SUBNUM) %&gt;%\n  \n  # -1 splits at the last character of the variable name\n  separate(col=\"key\", into=c(\"variable\", \"occasion\"), -1) %&gt;%\n  pivot_wider(names_from = variable, values_from = value) %&gt;% \n  mutate(occasion=as.numeric(occasion))\n\n\nNow, let’s perform the centering described above. You could do this in one pipeline – I just separated things here for conceptual clarity.\n\n\nCode\nforMLM &lt;- forMLM %&gt;% group_by(SUBNUM) %&gt;% \n  mutate(across(c(COMMIT, JOBSAT, READY), list(wic=meancent, pm=mean))) %&gt;%\n  ungroup()\n\nforMLM %&gt;% kable_table(n=10) %&gt;% kable_styling(font_size = 14)\n\n\n\n\n\nSUBNUM\noccasion\nJOBSAT\nCOMMIT\nREADY\nCOMMIT_wic\nCOMMIT_pm\nJOBSAT_wic\nJOBSAT_pm\nREADY_wic\nREADY_pm\n\n\n\n\n103\n1\n2.0\n3.7\n4.0\n0.00\n3.7\n-0.56\n2.6\n1.25\n2.8\n\n\n103\n2\n2.3\n3.7\n2.0\n0.00\n3.7\n-0.22\n2.6\n-0.75\n2.8\n\n\n103\n3\n3.3\n3.7\n2.2\n0.00\n3.7\n0.78\n2.6\n-0.50\n2.8\n\n\n129\n1\n3.7\n5.0\n2.5\n0.44\n4.6\n-0.56\n4.2\n-0.33\n2.8\n\n\n129\n2\n4.3\n4.3\n2.8\n-0.22\n4.6\n0.11\n4.2\n-0.08\n2.8\n\n\n129\n3\n4.7\n4.3\n3.2\n-0.22\n4.6\n0.44\n4.2\n0.42\n2.8\n\n\n171\n1\n3.7\n4.0\n3.2\n-0.17\nNA\n-0.17\nNA\n-0.12\nNA\n\n\n171\n2\n4.0\n4.3\n3.5\n0.17\nNA\n0.17\nNA\n0.12\nNA\n\n\n171\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n202\n1\n1.3\n3.3\n2.5\n0.44\n2.9\n-1.22\n2.6\n-0.75\n3.2"
  },
  {
    "objectID": "materials/w02_wrangling/strings_stringr.html",
    "href": "materials/w02_wrangling/strings_stringr.html",
    "title": "Strings in R with stringr",
    "section": "",
    "text": "The stringr package provides a consistent set of functions for working with strings. All functions start with str_ and are vectorized, so they work naturally with columns in a data.frame.\nWe will use a small example dataset to demonstrate the core verbs.\nCode\npeople &lt;- tibble::tibble(\n  id = 1:5,\n  name = c(\"Ada Lovelace\", \"Grace Hopper\", \"Margaret Hamilton\",\n           \"Katherine Johnson\", \"Mary Jackson\"),\n  email = c(\"ada@navy.mil\", \"grace@navy.mil\", \"margaret@mit.edu\",\n            \"katherine@nasa.gov\", NA),\n  dept = c(\"CompSci\", \"CompSci\", \"Engineering\", \"Research\", \"Research\")\n)\n\npeople %&gt;% kable_table()\n\n\n\n\n\nid\nname\nemail\ndept\n\n\n\n\n1\nAda Lovelace\nada@navy.mil\nCompSci\n\n\n2\nGrace Hopper\ngrace@navy.mil\nCompSci\n\n\n3\nMargaret Hamilton\nmargaret@mit.edu\nEngineering\n\n\n4\nKatherine Johnson\nkatherine@nasa.gov\nResearch\n\n\n5\nMary Jackson\nNA\nResearch"
  },
  {
    "objectID": "materials/w02_wrangling/strings_stringr.html#combine-strings",
    "href": "materials/w02_wrangling/strings_stringr.html#combine-strings",
    "title": "Strings in R with stringr",
    "section": "Combine strings",
    "text": "Combine strings\n\n\nCode\npeople %&gt;%\n  mutate(\n    label = str_c(name, \" (\", dept, \")\", sep = \"\")\n  ) %&gt;%\n  kable_table()\n\n\n\n\n\nid\nname\nemail\ndept\nlabel\n\n\n\n\n1\nAda Lovelace\nada@navy.mil\nCompSci\nAda Lovelace (CompSci)\n\n\n2\nGrace Hopper\ngrace@navy.mil\nCompSci\nGrace Hopper (CompSci)\n\n\n3\nMargaret Hamilton\nmargaret@mit.edu\nEngineering\nMargaret Hamilton (Engineering)\n\n\n4\nKatherine Johnson\nkatherine@nasa.gov\nResearch\nKatherine Johnson (Research)\n\n\n5\nMary Jackson\nNA\nResearch\nMary Jackson (Research)\n\n\n\n\n\nstr_glue() is convenient for inline formatting:\n\n\nCode\npeople %&gt;%\n  mutate(label = str_glue(\"{name} [{dept}]\")) %&gt;%\n  kable_table()\n\n\n\n\n\nid\nname\nemail\ndept\nlabel\n\n\n\n\n1\nAda Lovelace\nada@navy.mil\nCompSci\nAda Lovelace [CompSci]\n\n\n2\nGrace Hopper\ngrace@navy.mil\nCompSci\nGrace Hopper [CompSci]\n\n\n3\nMargaret Hamilton\nmargaret@mit.edu\nEngineering\nMargaret Hamilton [Engineering]\n\n\n4\nKatherine Johnson\nkatherine@nasa.gov\nResearch\nKatherine Johnson [Research]\n\n\n5\nMary Jackson\nNA\nResearch\nMary Jackson [Research]\n\n\n\n\n\nIf you want to collapse a vector into one string, use str_flatten():\n\n\nCode\nstr_flatten(people$dept, collapse = \", \")\n\n\n[1] \"CompSci, CompSci, Engineering, Research, Research\""
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "PSYC 859 Syllabus",
    "section": "",
    "text": "This graduate course is intended to provide an applied introduction to data management and data visualization in the social sciences. In order to take full advantage of modern statistical methods (e.g., structural equation models), competency in data management, semi-automated processing, and data wrangling is prerequisite. Likewise, prior to employing inferential statistics, exploratory visualization and analysis is essential to facilitate data cleaning and to form an initial understanding of patterns in the data. This course will cover both the principles and practice of data management, visualization, and exploratory analysis for summarizing quantitative data. In addition, students will learn data science skills to manage and visualize “big data,” where the size or complexity of the dataset defies traditional techniques.\nApplications of data management, visualization, and analysis will use the R statistical programming language. R is quickly becoming the lingua franca in data science across disciplines and offers unparalleled tools for data analysis and visualization."
  },
  {
    "objectID": "syllabus.html#week-1",
    "href": "syllabus.html#week-1",
    "title": "PSYC 859 Syllabus",
    "section": "Week 1 (1/8): Introduction to data management and tidy data",
    "text": "Week 1 (1/8): Introduction to data management and tidy data\n\nConceptual readings\n\nBriney, K., Coates, H., & Goben, A. (2020). Foundational Practices of Research Data Management. Research Ideas and Outcomes, 6, e56508. https://doi.org/10.3897/rio.6.e56508\nBorer, E. T., Seabloom, E. W., Jones, M. B., & Schildhauer, M. (2009). Some Simple Guidelines for Effective Data Management. The Bulletin of the Ecological Society of America, 90, 205-214. https://doi.org/10.1890/0012-9623-90.2.205\n(Optional) Borghi, J. A., & Gulick, A. E. V. (2021). Data management and sharing: Practices and perceptions of psychology researchers. PLOS ONE, 16, e0252047. https://doi.org/10.1371/journal.pone.0252047\n\n\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 2: Workflow: basics. https://r4ds.hadley.nz/workflow-basics.html\nCh. 4: Workflow: code style. https://r4ds.hadley.nz/workflow-style.html\nCh. 5: Data tidying. https://r4ds.hadley.nz/data-tidy.html\nCh. 6: Workflow: scripts and projects. https://r4ds.hadley.nz/workflow-scripts.html\nCh. 7: Data import. https://r4ds.hadley.nz/data-import.html\n\n(Supplementary) Tidyr pivoting vignette: https://tidyr.tidyverse.org/articles/pivot.html\nTidyr cheatsheet: https://rstudio.github.io/cheatsheets/tidyr.pdf\nRStudio Data Import cheat sheet: https://rstudio.github.io/cheatsheets/data-import.pdf"
  },
  {
    "objectID": "syllabus.html#week-2",
    "href": "syllabus.html#week-2",
    "title": "PSYC 859 Syllabus",
    "section": "Week 2 (1/15): Data aggregation, manipulation, joins",
    "text": "Week 2 (1/15): Data aggregation, manipulation, joins\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 3: Data transformation: https://r4ds.hadley.nz/data-transform.html\nCh. 12: Logical vectors: https://r4ds.hadley.nz/logicals.html\nCh. 13: Numbers: https://r4ds.hadley.nz/numbers.html\nCh. 14: Strings: https://r4ds.hadley.nz/strings.html\nCh. 16: Factors: https://r4ds.hadley.nz/factors.html\nCh. 19: Joins: https://r4ds.hadley.nz/joins.html\n(Optional) Ch. 15: Regular expressions. https://r4ds.hadley.nz/regexps.html\n(Optional) Ch. 17: Dates and times. https://r4ds.hadley.nz/datetimes.html\n(Optional) Ch. 20: Import: spreadsheets. https://r4ds.hadley.nz/spreadsheets.html"
  },
  {
    "objectID": "syllabus.html#week-3",
    "href": "syllabus.html#week-3",
    "title": "PSYC 859 Syllabus",
    "section": "Week 3 (1/22): Data processing and quality assurance, custom functions, basics of automation",
    "text": "Week 3 (1/22): Data processing and quality assurance, custom functions, basics of automation\n\nConceptual readings\n\nVan den Broeck, J., Argeseanu Cunningham, S., Eeckels, R., & Herbst, K. (2005). Data Cleaning: Detecting, diagnosing, and editing data abnormalities. PLoS Medicine, 2(10), e267.\n(Optional) Broman, K. W., & Woo, K. H. (2018). Data Organization in Spreadsheets. The American Statistician, 72, 2-10.\n\n\n\nPractical readings\n\nWickham, H., Cetinka-Rundel, M., & Grolemund, G. (2023). R for Data Science (2nd ed.).\n\nCh. 25: Functions. https://r4ds.hadley.nz/functions.html\nCh. 26: Iteration. https://r4ds.hadley.nz/iteration.html\n\nR validate package. Review https://cran.r-project.org/web/packages/validate/vignettes/cookbook.html\nIntroduction to pointblank package: https://bookdown.org/pdr_higgins/rmrwr/data-exploration-and-validation-with-the-pointblank-package.html"
  },
  {
    "objectID": "syllabus.html#week-4",
    "href": "syllabus.html#week-4",
    "title": "PSYC 859 Syllabus",
    "section": "Week 4 (1/29): Advanced data manipulation and management, tracking work in R markdown",
    "text": "Week 4 (1/29): Advanced data manipulation and management, tracking work in R markdown"
  },
  {
    "objectID": "syllabus.html#week-5",
    "href": "syllabus.html#week-5",
    "title": "PSYC 859 Syllabus",
    "section": "Week 5 (2/5): Principles of data visualization and graphical grammar",
    "text": "Week 5 (2/5): Principles of data visualization and graphical grammar"
  },
  {
    "objectID": "syllabus.html#week-6",
    "href": "syllabus.html#week-6",
    "title": "PSYC 859 Syllabus",
    "section": "Week 6 (2/12): Visual and graphical perception",
    "text": "Week 6 (2/12): Visual and graphical perception"
  },
  {
    "objectID": "syllabus.html#week-7",
    "href": "syllabus.html#week-7",
    "title": "PSYC 859 Syllabus",
    "section": "Week 7 (2/19): Graphic design, layout, style, use of color",
    "text": "Week 7 (2/19): Graphic design, layout, style, use of color"
  },
  {
    "objectID": "syllabus.html#week-8",
    "href": "syllabus.html#week-8",
    "title": "PSYC 859 Syllabus",
    "section": "Week 8 (2/26): A tour of quantitative visualization",
    "text": "Week 8 (2/26): A tour of quantitative visualization"
  },
  {
    "objectID": "syllabus.html#week-9",
    "href": "syllabus.html#week-9",
    "title": "PSYC 859 Syllabus",
    "section": "Week 9 (3/5): Visualizing continuous data (in ggplot2)",
    "text": "Week 9 (3/5): Visualizing continuous data (in ggplot2)"
  },
  {
    "objectID": "syllabus.html#week-10",
    "href": "syllabus.html#week-10",
    "title": "PSYC 859 Syllabus",
    "section": "Week 10 (3/12): Visualizing count and categorical data (in ggplot2)",
    "text": "Week 10 (3/12): Visualizing count and categorical data (in ggplot2)"
  },
  {
    "objectID": "syllabus.html#no-class-spring-break",
    "href": "syllabus.html#no-class-spring-break",
    "title": "PSYC 859 Syllabus",
    "section": "3/19: No class (Spring break)",
    "text": "3/19: No class (Spring break)"
  },
  {
    "objectID": "syllabus.html#week-11",
    "href": "syllabus.html#week-11",
    "title": "PSYC 859 Syllabus",
    "section": "Week 11 (3/26): Maximizing clarity: preparing graphics for presentation and publication",
    "text": "Week 11 (3/26): Maximizing clarity: preparing graphics for presentation and publication"
  },
  {
    "objectID": "syllabus.html#no-class-well-being-day",
    "href": "syllabus.html#no-class-well-being-day",
    "title": "PSYC 859 Syllabus",
    "section": "4/2: No class (Well-being day)",
    "text": "4/2: No class (Well-being day)"
  },
  {
    "objectID": "syllabus.html#week-12",
    "href": "syllabus.html#week-12",
    "title": "PSYC 859 Syllabus",
    "section": "Week 12 (4/9): Visualizing and understanding fit (and misfit) of statistical models",
    "text": "Week 12 (4/9): Visualizing and understanding fit (and misfit) of statistical models"
  },
  {
    "objectID": "syllabus.html#week-13",
    "href": "syllabus.html#week-13",
    "title": "PSYC 859 Syllabus",
    "section": "Week 13 (4/16): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction",
    "text": "Week 13 (4/16): Exploratory statistics for understanding data: clustering, multidimensional scaling, dimension reduction"
  },
  {
    "objectID": "syllabus.html#week-14",
    "href": "syllabus.html#week-14",
    "title": "PSYC 859 Syllabus",
    "section": "Week 14 (4/23): Final presentations of data projects",
    "text": "Week 14 (4/23): Final presentations of data projects"
  },
  {
    "objectID": "syllabus.html#permitted-uses",
    "href": "syllabus.html#permitted-uses",
    "title": "PSYC 859 Syllabus",
    "section": "Permitted uses",
    "text": "Permitted uses\nStudents may use AI tools for the following purposes:\n\nDebugging code, including identifying syntax errors, logical errors, or unexpected behavior.\nRequesting explanations of how a piece of code works, why it produces a particular result, or why it fails.\nRequesting suggestions for code improvement, refactoring, or alternative approaches to a task.\n\nThese uses are consistent with how AI tools are used responsibly in real research workflows."
  },
  {
    "objectID": "syllabus.html#expectations-and-responsibilities",
    "href": "syllabus.html#expectations-and-responsibilities",
    "title": "PSYC 859 Syllabus",
    "section": "Expectations and responsibilities",
    "text": "Expectations and responsibilities\nWhen using AI tools, students are expected to:\n\nActively evaluate and understand any code they submit, regardless of its source.\nEnsure they can explain what the code does and why it works, including key functions, assumptions, and consequences.\nMake independent decisions about whether to adopt AI-suggested code, rather than copying it uncritically.\nRemain responsible for correctness, clarity, and reproducibility of all submitted work.\n\nSubmitting code that the student does not understand is inconsistent with the learning objectives of the course."
  },
  {
    "objectID": "syllabus.html#prohibited-uses",
    "href": "syllabus.html#prohibited-uses",
    "title": "PSYC 859 Syllabus",
    "section": "Prohibited uses",
    "text": "Prohibited uses\nThe following are not permitted:\n\nSubmitting AI-generated code or analyses without understanding or review.\nUsing AI tools as a substitute for engaging with core course concepts (e.g., visualization principles, data cleaning logic).\nRepresenting AI-generated work as understanding or reasoning that the student cannot demonstrate if asked."
  },
  {
    "objectID": "syllabus.html#transparency",
    "href": "syllabus.html#transparency",
    "title": "PSYC 859 Syllabus",
    "section": "Transparency",
    "text": "Transparency\nFor major assignments and the final project, students may be asked to include a brief AI use statement (1-3 sentences) describing whether and how AI tools were used (e.g., “used for debugging,” “used to explore alternative ggplot layouts”). This is not punitive; its purpose is to promote transparency and reflective practice."
  }
]